{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/F25_Class_06_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 6: Reinforcement Learning**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* Part 6.1: Introduction to Introduction to Gymnasium and Q-Learning\n",
        "* Part 6.2: Stable Baselines Q-Learning\n",
        "* Part 6.3: Atari Games with Stable Baselines Neural Networks\n",
        "* **Part 6.4: Future of Reinforcement Learning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERfMETL_-NoS",
        "outputId": "db45ae1a-4127-4229-8c52-bbdf2146ed11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is included as the last line in the output above."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Gymnasium\n",
        "\n",
        "Before we can beging, we need to install Hugging Face datasets by running the code in the next cell."
      ],
      "metadata": {
        "id": "dxOKy1Sat7rz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IcTY1gkD-X5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f72a3b9-726b-46c3-b946-d54c542e2030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Unable to locate package python-opengl\n"
          ]
        }
      ],
      "source": [
        "# Install gymnasium\n",
        "\n",
        "# Install stable-baselines\n",
        "!pip install stable-baselines3[extra] gymnasium > /dev/null\n",
        "\n",
        "# Install gymnasium[atari] package\n",
        "!pip install gymnasium[atari] > /dev/null\n",
        "\n",
        "# Install pyvirtualdisplay\n",
        "!pip install pyvirtualdisplay > /dev/null\n",
        "\n",
        "# Install opengl\n",
        "!sudo apt-get install -y python3-opengl ffmpeg > /dev/null\n",
        "\n",
        "# Install OpenGL and ffmpeg\n",
        "!pip install PyOpenGL > /dev/null\n",
        "\n",
        "# Set a non-interactive frontend for debconf and install xvfb and ffmpeg\n",
        "!sudo DEBIAN_FRONTEND=noninteractive apt-get install -y xvfb ffmpeg > /dev/null\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Custom Functions\n",
        "\n",
        "Run the cell below to load custom functions used in this lesson."
      ],
      "metadata": {
        "id": "dfExM5hY3fAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "37ShL3h23sK5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Future Directions of Reinforcement Learning**\n",
        "\n",
        "Reinforcement Learning (RL) initially showed great promise as it defeated humans in games such as chess and Go. The 1990s heralded the birth of RL, a time when the field was a conclave of a few passionate souls. In 1995, the first community began to form at the inaugural National Science Foundation workshop on RL.\n",
        "\n",
        "As the new millennium dawned, RL's presence grew silently yet steadfastly, never quite breaking into mainstream machine learning research within machine learning until the advent of DeepMind. Their innovative synthesis of deep learning with RL demonstrated with Atari gaming, was a revelation. It sparked a renaissance, suddenly making RL the object of desire for tech conglomerates and startups, evidenced by Google's princely acquisition of DeepMind.\n",
        "\n",
        "To many, RL appears triumphant, destined to continue its march forward. Sessions dedicated to RL are overflowing at AI and ML conferences, and the influx of papers is ongoing, reinforcing a narrative of success.\n",
        "\n",
        "However, some researchers believe that RL may be at a dead end. When we impose on RL the stringent criteria of learning in real-time, common in the real world, the picture shifts. The current methods of deep RL, which shine in the simulated realities where failure is inconsequential, and repetition is infinite, falter against the complex learning that real life demands. Consider how humans learn to master the art of driving. It's not merely a function of operating the vehicle but a culmination of years of passive observation, an intricate dance of the senses and cognition that no current RL algorithm can claim to replicate. OpenAI, once the leading proponent of RL, has divested itself of the OpenAI Gym.\n",
        "\n",
        "The foray into teaching machines through RL has often ignored this fundamental aspect of human learningâ€”the vast repository of implicit knowledge we bring to each new learning experience. The stark contrast in learning efficiencies between humans and RL algorithms becomes apparent. Where deep RL takes days and millions of iterations to grasp a game like Frostbite, a human requires only a minute and a few hundred trials. This disparity cannot be understated.\n",
        "\n",
        "So, where does this leave RL? Can it be rescued from the potential impasse where current trends lead? Some believe the answer lies not in the abandonment of RL but in its evolution. The future of RL should embrace a model that integrates observation, mimicking, transfer learning, and the scaffolding provided by prior knowledge. This paradigm resonates with how humans acquire complex skills. The musings of Richard Feynman on education resonate deeply with this issue. Despite his unparalleled eloquence in teaching physics, Feynman acknowledged the instruction limitations. This sentiment refers to an older, perhaps more universal understanding of learning: We absorb best what we are already primed to receive.\n",
        "\n",
        "This recognition brings us to a pivotal juncture for RL. The way forward is not to mimic the human learning process but to create a symbiotic framework where machines can benefit from the richness of the human experience. We stand on the brink of a transformative evolution in RL or its decline into obsolescence. The choice is ours to make, the direction ours to steer. The future of RL will depend on whether we choose to learn from the depth of human experience or continue down a path of isolated, knowledge-free computation.\n",
        "\n",
        "For a more indepth analysis of the future of reinforcement learning, consider the Quora post by [Sridhar Mahadevan](https://www.quora.com/Is-reinforcement-learning-a-dead-end).\n"
      ],
      "metadata": {
        "id": "WZFcOAcddRGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_06_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5rrxX_hachd6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vilb8qMcN2mj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9 (torch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}