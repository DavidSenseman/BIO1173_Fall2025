{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/Class_01_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDKB7COOe0fg"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkgEwkrMe0fg"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class_01_6:  Pandas and File Handling**"
      ],
      "metadata": {
        "id": "ekH-C9L6BUjK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYwb3wGce0fg"
      },
      "source": [
        "##### **Module I: Getting Started with Python**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module I Material\n",
        "\n",
        "* Part 1.1: Introduction to Google CoLab\n",
        "* Part 1.2: Python Basics 1 -- Strings, Variables, Functions\n",
        "* Part 1.3: Python Basics 3 -- Lists, Dictionaries, Sets and JSON\n",
        "* Part 1.4: Python Basics 4 -- Conditionals and Loops\n",
        "* Part 1.5: Python Basics 5 -- Packages, NumPy arrays and Matplotlib\n",
        "* **Part 1.6: Python Basics 6 -- Pandas and File Handling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_Y1eMUe0fg"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to ```/content/drive``` and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output, but your GMAIL address will be printed out.\n",
        "\n",
        "~~~Text\n",
        "Mounted at /content/drive\n",
        "Note: Using Google CoLab\n",
        "david.senseman@gmail.com\n",
        "~~~\n",
        "\n",
        "If your GMAIL address is not visible, your submission will not be graded."
      ],
      "metadata": {
        "id": "RJCA3G9DBvmC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQVGxd2le0fh"
      },
      "source": [
        "# **Pandas and File Handling**\n",
        "\n",
        "In this lesson we will focus on the software package **Pandas** and on file handling. These two topics naturally go together since the Pandas package includes a number of file handling methods that are frequently used in Python programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqkCSMODe0fh"
      },
      "source": [
        "## **Pandas**\n",
        "\n",
        "**_Pandas_** (pronounced as \"PAN-daz\") is a Python package designed for data manipulation and analysis. It provides data structures and operations for manipulating numerical tables and time series.\n",
        "\n",
        "Pandas is built on top of the Numpy package and provides a high-level interface for working with data including data selection, cleaning, filtering, aggregation, and visualization.\n",
        "\n",
        "A central concept in Pandas is the **_DataFrame_**. A Pandas DataFrame is generally the most commonly used Pandas object.\n",
        "\n",
        "A _DataFrame_ is a two-dimensional labeled data structure with columns of potentially different data types (e.g. integers, floats, and strings). They are very similar to an Excel spreadsheet in which each **_row_** represents a single experimental subject or clinical patient and each **_column_** contains a different experimental or clinical measurement from the subject.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL-Djrnde0fh"
      },
      "source": [
        "### The Pandas Package\n",
        "\n",
        "Like other Python packages, Pandas has to be _imported_ into a Python program with the following command before it can be used.\n",
        "\n",
        "`import pandas as pd`\n",
        "\n",
        "The normal _alias_ ('nickname') for Pandas is `pd`. When using a method that is part of a Pandas package, the alias `pd` will be used instead of the package name. For example, to use the Pandas `read_csv()` method, the command would be:\n",
        "\n",
        "`pd.read_csv(filename)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjuK0r9xe0fh"
      },
      "source": [
        "## **File handling**\n",
        "\n",
        "**_File handling_** in Python is the process of manipulating files and data stored in a file system. This includes reading and writing files, creating and deleting files, accessing metadata about files, and more.\n",
        "\n",
        "Python has a variety of built-in functions to help with file handling, such as the `open()` and `close()` functions for opening and closing files, and the `os module` for interacting with the file system. Additionally, there are several third-party libraries that can be used to simplify file handling, such as the Pandas library for working with tabular data.\n",
        "\n",
        "Files often contain the data that you use to train your AI programs. Once trained, your models may use real-time data to form predictions. These predictions might be made on files too. Regardless of predicting or training, file processing is a vital skill.\n",
        "\n",
        "There are many different types of files that you must be able to process. The most important file types are listed here:\n",
        "\n",
        "* **CSV files:** (generally have the .csv extension) hold tabular data that resembles spreadsheet data.\n",
        "* **Image files:** (generally with the .png or .jpg extension) hold images for computer vision.\n",
        "* **Text files:** (often have the .txt extension) hold unstructured text and are essential for natural language processing.\n",
        "* **JSONL** (often have the .json extension) contain semi-structured textual data in a human-readable text-based format.\n",
        "* **H5:** (can have a wide array of extensions) contain semi-structured textual data in a human-readable text-based format. Keras and TensorFlow store neural networks as H5 files.\n",
        "* **Audio Files:** (often have an extension such as .au or .wav) contain recorded sound.\n",
        "\n",
        "Data can come from a variety of sources. In this class, you will obtain data from three primary locations:\n",
        "\n",
        "* **Your Hard Drive -** This type of data is stored locally, and Python accesses it from a path that looks something like: c:\\data\\myfile.csv. You will download these files from Canvas as part of the lesson in a compressed (Zip) file.\n",
        "* **The Internet -** This type of data resides in the cloud and Python accesses it from a URL that looks something like: https://images.pexels.com/photos/9487467/pexels-photo-9487467.jpeg.\n",
        "* **Google Drive (cloud) -** If your code in Google CoLab, you use GoogleDrive to save and load some data files. CoLab mounts your GoogleDrive into a path similar to the following: /content/drive/My Drive/myfile.csv.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKtP_gkoe0fh"
      },
      "source": [
        "### **File format of Data Files**\n",
        "\n",
        "Data files can either be **_formatted_** and **_unformatted_**. For example, a Microsoft Word file (.doc or .docx) is a _formatted file_. Microsoft uses a proprietary document format to store MS Word files. If you try to \"read\" a formated file with a simple text editor like Notepad, you would see something unintelligiblelike this:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/MSWord.png)\n",
        "\n",
        "Most data files used for Machine Learning, including Neural Networks, are _unformatted_ text files. They can be read with any word processor program or even a simple text editor.\n",
        "\n",
        "For example, the file `iris.txt`, that was included with this lesson, looks like this if you read it with a simple text editor:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppZf4br1e0fh"
      },
      "source": [
        "~~~text\n",
        "sepal_l\tsepal_w\tpetal_l\tpetal_w\t species\n",
        "5.1     3.5\t    1.4\t    0.2\t     Iris-setosa\n",
        "4.9\t    3.0\t    1.4\t    0.2\t     Iris-setosa\n",
        "4.7\t    3.2\t    1.3\t    0.2\t     Iris-setosa\n",
        "4.6\t    3.1\t    1.5\t    0.2\t     Iris-setosa\n",
        "5.0\t    3.6\t    1.4\t    0.2\t     Iris-setosa\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX-3eCb9e0fh"
      },
      "source": [
        "### **File handling with Pandas**\n",
        "\n",
        "**_Pandas_** is frequently used in Python programs to read unformated text file. File handling using Pandas typically involves reading in data from a file into a Pandas **_DataFrame_** using the `pd.read_csv(filename)` function. This function can be used to read in data from a variety of sources including CSV files, Excel files, HTML tables, and other formats.\n",
        "\n",
        "The function's name refers to a particularily common file type called a CSV (Comma Separated Values) file. In this file type, a comma **`,`** is used as the **_delimiter_** value, to **separate** one data value from another.\n",
        "\n",
        "Here is what you would see if you read the datafile `iris.csv` with a text editor. In this file, each data value is separated by a comma **`,`** from the next value.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT24xSDZe0fh"
      },
      "source": [
        "~~~text\n",
        "sepal_l,sepal_w,petal_l,petal_w,species\n",
        "5.1,3.5,1.4,0.2,Iris-setosa\n",
        "4.9,3.0,1.4,0.2,Iris-setosa\n",
        "4.7,3.2,1.3,0.2,Iris-setosa\n",
        "4.6,3.1,1.5,0.2,Iris-setosa\n",
        "5.0,3.6,1.4,0.2,Iris-setosa\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvsK_tpfe0fh"
      },
      "source": [
        "While a comma **`,`** is often used a the delimited in data text files, it is certainly not the only character used. Other delimiters include a space ` ` or a tab `\\t`.\n",
        "\n",
        "In order to handle txt files with different delimiters, `pd.read_csv(filename)` can take a second argument called `sep` that defines the separator character to use when reading and processing the text file.\n",
        "\n",
        "For example, the following command would be used to read a text file that used a space as a delimiter:\n",
        "\n",
        "`pd.read_csv(filename, sep=' ')`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWZu9LJGe0fh"
      },
      "source": [
        "### Example 1: Read a data file stored on the course HTTPS server\n",
        "\n",
        "The code in the cell below uses the function `pd.read_csv(filename, sep)` to read the data file `iris.txt` stored on the course HTTPS server [https://biologicslab.co](https://biologicslab.co). As the file is read, it is stored in a Pandas DataFrame called `df1`. In the file `iris.txt` a **_tab_** is used as the delimiter. To specify a tab, you use the following: `\\t`.\n",
        "\n",
        "When reading text files with `pd.read_csv()`, it is _always_ a good idea to print out the first 5 rows of data to make sure the read was successful. One way to do this is with the Pandas method `head()` as demonstrated in the example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Pq5Ux0Ihe0fh"
      },
      "outputs": [],
      "source": [
        "# Example 1: Use pd.read_csv to read a file on the hard drive\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read local data file using Pandas read_csv() function\n",
        "df1 = pd.read_csv(\"https://biologicslab.co/BIO1173/data/iris.txt\", sep='\\t')  # define the separator as a tab\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_rows', 6)\n",
        "pd.set_option('display.max_columns', 9)\n",
        "\n",
        "# Print out the first 5 records using the head() method\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeJEy2j1e0fh"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![____](https://biologicslab.co/BIO1173/images/class_01/class_01_9_iris.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUp5tIdOe0fi"
      },
      "source": [
        "### **Exercise 1: Read a data file stored on the course HTTPS server**\n",
        "\n",
        "In the cell below, use the Pandas `pd.read_csv(filename, sep)` function to read the data file `Pima.txt` on your hard drive and store the data in a new data frame called `df2`. Use the `head()` method to print out the first 5 records in `df2`. The deliminter in this file a comma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPHdtt6ue0fi"
      },
      "outputs": [],
      "source": [
        "### Insert your code for Exercise 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8Vh5Mpe0fi"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/Pima.png)\n",
        "\n",
        "However, if you used the **wrong** delimiter, you will see this instead.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/PimaWrong.png)\n",
        "\n",
        "If you see the second image, you need to change the value of the delimiter to a comma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1chfJ6KOe0fi"
      },
      "source": [
        "### Example 2: Use the Pandas `display()` function.\n",
        "\n",
        "Similar to the `head()` function, the Pandas `display()` function is another convenient way to view data in a DataFrame. It is used to quickly display the contents of a DataFrame especially in an interactive environments such as JupyterLab. It is quite useful for quickly analyzing data and it allows the user to have more control over the way the data is displayed.\n",
        "\n",
        "The code in the cell below shows how to use this method with the Iris data stores in `df1`. Since the `display()` function allows you to control the maximum rows and columns to show, it can provide a cleaner display than merely printing a DataFrame with many columns and/or rows that using the `head()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uewfbYLle0fi"
      },
      "outputs": [],
      "source": [
        "# Example 2: Use display method\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Set max columns and max rows\n",
        "pd.set_option('display.max_columns', 4)\n",
        "pd.set_option('display.max_rows', 6)\n",
        "\n",
        "# Display 4 columns and 6 rows\n",
        "display(df1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgB1U-RNe0fi"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![____](https://biologicslab.co/BIO1173/images/class_01/class_01_9_display.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aIL55_Re0fi"
      },
      "source": [
        "### **Exercise 2: Use the Pandas display() function.**\n",
        "\n",
        "In the cell below, use the Pandas `display()` function to print a maximum of 8 columns and 8 rows of the DataFrame `df2`.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blAMjH9le0fi"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "llVRqdQIe0fi"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/PDdisplay.png)\n",
        "\n",
        "However, if you used the **wrong** delimiter in **Exercise 2**, you will see this instead.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/PDdisplayWrong.png)\n",
        "\n",
        "### **If you see the wrong display --STOP**\n",
        "\n",
        "The reason that you are seeing the wrong display is that you still haven't changed the delimiter value in **Exercise 2**. Go back to **Exercise 2** and change the delimiter to be a comma `sep=','` and then **_re-run_** the Exercise 2 code cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwAjzm-9e0fi"
      },
      "source": [
        "### Example 3: Use the Pandas `describe()` method.\n",
        "\n",
        "With any new dataset, it is generally useful to get a quick, overall view of dataset's contents using the Pandas `describe()` method.  \n",
        "\n",
        "The `descibe()` method returns an variety of summary statistics about the data, including the count, mean, standard deviation, minimum, maximum, and first and third quartiles. It also includes a count of the number of non-null values, and the percent of the data that is missing. This information can be used to get a better understanding of the data and its distributions.\n",
        "\n",
        "The code in the cell below shows how to use this method with the Iris data stores in `df1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-mFTr1MQe0fi"
      },
      "outputs": [],
      "source": [
        "# Example 3: Use describe() method to print summary statistics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Set max columns and max rows\n",
        "pd.set_option('display.max_columns', 8)\n",
        "pd.set_option('display.max_rows', 8)\n",
        "\n",
        "# Describe() method with df1\n",
        "df1.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16tKI4WTe0fi"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![____](https://biologicslab.co/BIO1173/images/class_01/class_01_9_describe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1viP50Ne0fi"
      },
      "source": [
        "One thing to notice is that the **count** values are not the same for all the columns. In particular, the count for the column `petal_w`is `149`, while the count for all the other columns is 150.\n",
        "\n",
        "The `describe()` method excludes any NA or missing values which suggests that there is a missing value in the `petal_w` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrLStvp0e0fi"
      },
      "source": [
        "### **Exercise 3: Use the Pandas `describe()` method.**\n",
        "\n",
        "In the cell below, use the Pandas `describe()` method to print the summary statistics of the data in `df2`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quLSvVT7e0fi"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt-IVzoZe0fj"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_01/Describe.png)\n",
        "\n",
        "Look at the **count** values to see if there are any missing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSAzwH8Pe0fj"
      },
      "source": [
        "--------------------\n",
        "\n",
        "## **Missing Values**\n",
        "\n",
        "Missing values are a reality of machine learning.  Ideally, every row of data will have values for all columns.  However, this is rarely the case. Replacing missing values is important in machine learning they can cause problems with the model's accuracy and can lead to incorrect predictions. It is up to you to **_clean your data_**  \n",
        "\n",
        "There are a few different ways to deal with missing data. You could simply delete any record (i.e. the entire row) if it contained one (or more) missing value(s). This may or may not be reasonable depending up the data set and the number of missing values in it.\n",
        "\n",
        "An alternative approach is to replace missing value(s) with the **_median value_** for that column. Here is the Wiki page for the program that calculates the [median](https://en.wikipedia.org/wiki/Median).  \n",
        "\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UvRHc6le0fj"
      },
      "source": [
        "### Example 4: Use the Pandas `isnull()` method to find missing values.\n",
        "\n",
        "When working with a new dataset you should perform a quick check for any missing values. The example below, the Pandas method `isnull()` is used to locate any missing values in the Iris data stored in `df1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNanAHBoe0fl"
      },
      "outputs": [],
      "source": [
        "# Example 4: Use isnull() method to find missing data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Find the locations of missing data\n",
        "missing_locations = df1.isnull().any()\n",
        "\n",
        "# Display the locations of missing data\n",
        "print(missing_locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9ZCYHzce0fl"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "sepal_l    False\n",
        "sepal_w    False\n",
        "petal_l    False\n",
        "petal_w     True\n",
        "species    False\n",
        "dtype: bool\n",
        "~~~\n",
        "The output says there is at least one (and possibly more) missing values in the column `petal_w`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fw8pE5te0fl"
      },
      "source": [
        "### **Exercise 4: Use the Pandas `isnull()` method to find missing values.**\n",
        "\n",
        "In the cell below, use the Pandas `isnull()` method to locate any missing values in `df2`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an4-LUjte0fl"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3mrYN-ce0fm"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J27vtb00e0fm"
      },
      "source": [
        "~~~text\n",
        "npreg    False\n",
        "glu      False\n",
        "bp       False\n",
        "skin     False\n",
        "bmi       True\n",
        "ped      False\n",
        "age      False\n",
        "type     False\n",
        "dtype: bool\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyQkH_Hue0fm"
      },
      "source": [
        "The output reports that there is at least one and possibly more missing values in the column `bmi`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag2Ldyy8e0fm"
      },
      "source": [
        "### Example 5: Use the Pandas `fillna()` method to replace missing values.\n",
        "\n",
        "Now that the location of the missing value(s) has been determined, the Pandas method `fillna()` can now be use to replace the missing values.  \n",
        "\n",
        "The first step is to compute the `median` value for the column using the Pandas method `median()`. The next step is to use the Pandas method `fillna()` to replace any missing values in the `petal_w` column with the median value. Finally, we check once more for any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIptr_a2e0fm"
      },
      "outputs": [],
      "source": [
        "# Example 5: Use isnull() method to find missing data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Find the median of the column petal_w\n",
        "PWidth_med = df1['petal_w'].median()\n",
        "\n",
        "# Print out the median value\n",
        "print(f\"The median value = {PWidth_med} for petal width.\")\n",
        "print(f\"Replacing missing values with {PWidth_med}.\")\n",
        "\n",
        "# Use fillna method\n",
        "df1['petal_w'] = df1['petal_w'].fillna(PWidth_med)\n",
        "\n",
        "# Find the locations of missing data\n",
        "print(\"\\nLooking for missing values...\")  # The \\n means print a newline\n",
        "missing_locations = df1.isnull().any()\n",
        "\n",
        "# Display the locations of missing data\n",
        "print(missing_locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWddn9sEe0fm"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "The median value = 1.3 for petal width.\n",
        "Replacing missing values with 1.3.\n",
        "\n",
        "Looking for missing values...\n",
        "sepal_l    False\n",
        "sepal_w    False\n",
        "petal_l    False\n",
        "petal_w    False\n",
        "species    False\n",
        "dtype: bool\n",
        "~~~\n",
        "The output says there are no longer any missing values in any column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4sMo1Ffe0fm"
      },
      "source": [
        "### **Exercise 5: Use the Pandas `fillna()` method to replace missing values**\n",
        "\n",
        "In the cell below, use the Pandas method fillna() to replace the missing values in the column `bmi`.\n",
        "\n",
        "Make sure to compute the median value for the `bmi` column using the Pandas method median(). Then use the Pandas method fillna() to replace any missing values in the `bmi` column with the median value. Finally, check once more for any missing values. values in `df2`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXnn0_Hce0fm"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PymPW41Ke0fm"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGhJQfI6e0fm"
      },
      "source": [
        "~~~text\n",
        "The median value = 32.8 for bmi.\n",
        "Replacing missing values with 32.8.\n",
        "\n",
        "Looking for missing values...\n",
        "npreg    False\n",
        "glu      False\n",
        "bp       False\n",
        "skin     False\n",
        "bmi      False\n",
        "ped      False\n",
        "age      False\n",
        "type     False\n",
        "dtype: bool\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iS6Vgjle0fm"
      },
      "source": [
        "The output above reports that there are no more missing values in any column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVuxDMRe0fm"
      },
      "source": [
        "### Example 6: Use the Pandas `copy()` method to create a copy of a data frame.\n",
        "\n",
        "The Pandas `copy()` method is used to create a **_shallow copy_** of a DataFrame. A shallow copy of a DataFrame is a copy that does _not_ contain a deep copy of the objects or data within the DataFrame. Instead, it creates a new object with references to the objects or data in the original DataFrame. In other words, if the original DataFrame is changed, the shallow copy will **_not_** be affected.  \n",
        "\n",
        "You might want to use a shallow copy of a DataFrame when you need to make changes to the DataFrame without affecting the original data. Shallow copies are often used when you need to make temporary changes to DataFrame and then revert back to the original.\n",
        "\n",
        "The code in the cell below creates a shallow copy of the DataFrame `df1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "flz8uaQPe0fm"
      },
      "outputs": [],
      "source": [
        "# Example 6: Use the `copy()` method to make a shallow copy\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Make shallow copy\n",
        "df1_copy = df1.copy()\n",
        "\n",
        "# Show the first records in the copy\n",
        "df1_copy.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHjTsAEFe0fm"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![____](https://biologicslab.co/BIO1173/images/class_01/class_01_9_Exm6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdQEaKFle0fm"
      },
      "source": [
        "### **Exercise 6: Use the Pandas `copy()` method to create a copy of a data frame**.\n",
        "\n",
        "In the cell below write the code to create a shallow copy of the DataFrame `df2`. Call your copy `df2_copy`. Print out the first 5 records in `df2_copy` using the `head()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScYGZtaVe0fm"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDeO8zcbe0fm"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_01/Pima.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZJjJPmqe0fm"
      },
      "source": [
        "## **Dropping field in a Pandas DataFrame**\n",
        "\n",
        "**Dropping a field** in a Pandas DataFrame is a way of removing a column from the dataset. This can be done using the Panda `drop()` method, which takes the label of the column that you want to remove. You may want to do this if the field is irrelevant to the analysis you are performing, or if it contains redundant information. For example, you will need to drop fields that are of no value to the training of a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3burOw1e0fn"
      },
      "source": [
        "### Example 7: Use the Pandas `drop()` method to drop a field in a DataFrame.\n",
        "\n",
        "The code in the cell below uses the Pandas `drop()` method is used to drop the field `species` from the Iris data set. To preserve the original data, the `drop()` method will be used with the shallow copy created in **Example 6**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGZ5UXQ6e0fn"
      },
      "outputs": [],
      "source": [
        "# Example 7: Drop a field in a data frame using drop() method\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Print the column names before dropping\n",
        "print(f\"Before drop: {list(df1_copy.columns)}\")\n",
        "\n",
        "# Use the drop method to drop the species column\n",
        "df1_copy.drop(columns=['species'], inplace=True)\n",
        "\n",
        "# Print out the columns after the drop\n",
        "print(f\"After drop: {list(df1_copy.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWqC0NdJe0fn"
      },
      "source": [
        "If the code is correct your should see:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjSdG44Ge0fn"
      },
      "source": [
        "~~~text\n",
        "Before drop: ['sepal_l', 'sepal_w', 'petal_l', 'petal_w', 'species']\n",
        "After drop: ['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7I7EoSre0fn"
      },
      "source": [
        "However, if you see the following error:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZtJ0_Mve0fn"
      },
      "source": [
        "~~~text\n",
        "KeyError: \"['species'] not found in axis\"\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNnc6cXHe0fn"
      },
      "source": [
        "it probably means that you have run the example already so that `df1_copy` no longer contains the column `species`. This is one reason to make a backup copy when you are making changes to a data frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJmZ3cHse0fn"
      },
      "source": [
        "### **Exercise 7: Use the Pandas `drop()` method to drop a field in a DataFrame.**\n",
        "\n",
        "In the cell below use the Pandas `drop()` method to drop the `type` column from the **_copy_** of the Pima data stored in `df2_copy`. Print the column names before and after dropping the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNefjmQJe0fn"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 7 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKxo9YjZe0fn"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDM8yvZe0fn"
      },
      "source": [
        "~~~text\n",
        "Before drop: ['npreg', 'glu', 'bp', 'skin', 'bmi', 'ped', 'age', 'type']\n",
        "After drop: ['npreg', 'glu', 'bp', 'skin', 'bmi', 'ped', 'age']\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peeYUhPOe0fn"
      },
      "source": [
        "However, if you see the following error:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8MUzoGEe0fn"
      },
      "source": [
        "~~~text\n",
        "KeyError: \"['type'] not found in axis\"\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk7QmDuie0fn"
      },
      "source": [
        "it probably means that you have run the example already so that `df2_copy` no longer contains the column `type`. To get rid of this errow you will need to re-run **Exercise 6** to create a new copy of `df2_copy` and then re-run **Exercise 7** using the new copy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvXkLPo0e0fn"
      },
      "source": [
        "### Example 8: Use Pandas `map()` method to map strings to integer values\n",
        "\n",
        "Mapping string values to integer values can be important when using certain machine learning models. Some machine learning algorithms do not work well with string values, so they must be converted to integer values. This is done by assigning a unique number to each distinct string value. This helps the machine learning algorithm to recognize patterns in the data and make more accurate predictions.\n",
        "\n",
        "In this example, the column `species` in the Iris flower data set will have the following string values mapped to the following integers:\n",
        "\n",
        "* Iris-setosa: mapped to the value 0\n",
        "* Iris-versicolor: mapped to the value 1\n",
        "* Iris-virginica: mapped to the value 2\n",
        "\n",
        "using the Pandas `map()` method.\n",
        "\n",
        "The code starts by making a new, shallow copy of the `df1` data frame, called `df1_copy`, for the remapping. This keeps intact the original Iris data stored in `df1`.\n",
        "\n",
        "The code then set the display options so that the values of the `species` column can be more easily observed. Many data sets used in machine learning can have too many columns as well as too many rows to easily display in a Jupyter Lab notebook.\n",
        "\n",
        "To verify that the mapping worked as expected, the contents of the `df_copy` data frame are printed out before, and after, the mapping using the `display()` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7vmVunXre0fn"
      },
      "outputs": [],
      "source": [
        "# Example 8: Map strings to integers\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Make a new copy of df1\n",
        "df1_copy = df1.copy()\n",
        "\n",
        "# Set max columns and max rows\n",
        "pd.set_option('display.max_columns', 4)\n",
        "pd.set_option('display.max_rows', 6)\n",
        "\n",
        "# Describe() method with df2\n",
        "print(\"Iris data before mapping:\")\n",
        "display(df1_copy)\n",
        "\n",
        "# Define the mapping dictionary\n",
        "mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica' :2}\n",
        "\n",
        "# Map the integer column to strings\n",
        "df1_copy['species'] = df1_copy['species'].map(mapping)\n",
        "\n",
        "print(\"Iris data after mapping:\")\n",
        "display(df1_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfV8x8RMe0fn"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_01/class_01_9_Exm9.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn-csmZ7e0fn"
      },
      "source": [
        "You should notice that the species names have been converted in `0` for Iris-setosa and `2` for Iris-virginica, in the `species` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmt2LTLze0fn"
      },
      "source": [
        "### **Exercise 8: Use Pandas `map()` method to map strings to integer values**\n",
        "\n",
        "In the cell below, use the Pandas `map()` method to map string values in the column `type` from `No` to 0, and `Yes` to 1.\n",
        "\n",
        "Start by making a new copy of `df2` and call it `df2_copy`. Perform the mapping on the `df2_copy` data frame. As in **Example 8**, use the Pandas `display()` function to print out 4 columns and 6 row of `df2_copy` before and after the mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-xMDAMC0e0fn"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nRhd28Ue0fn"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_01/class_01_9_Exe9.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww2v0035e0fo"
      },
      "source": [
        "The output above shows that the string values, `Yes` and `No` in the `type` column have been converted to the integer values `0` and `1` respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmJG1EFSe0fo"
      },
      "source": [
        "## **Convert Pandas DataFrame to a Numpy Array**\n",
        "\n",
        "Pandas has a built-in function called `to_numpy()` which can be used to convert a Pandas DataFrame or series to Numpy array. This can be useful for manipulating or plotting data using Numpy functions.\n",
        "\n",
        "The `to_numpy()` function takes no arguments and returns a Numpy array containing the data from the Pandas DataFrame or series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wmb_fzCe0fo"
      },
      "source": [
        "### Example 9: Use Pandas `to_numpy()` method to convert a DataFrame into Numpy array\n",
        "\n",
        "The code in the cell below uses the `to_numpy()` method to covert the Iris data stored in the `df1` DataFrame, into a Numpy array called `dft_ar`. It then uses square bracket indexing to print out the first 5 rows of the new Numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA6NuQaBe0fo"
      },
      "outputs": [],
      "source": [
        "# Example 9: Convert data frame to numpy array\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Use the to_numpy method\n",
        "df1_ar = df1.to_numpy()\n",
        "\n",
        "# Print the first 5 rows of the numpy array\n",
        "print(df1_ar[ :5, ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TwqhJboe0fo"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "[[5.1 3.5 1.4 0.2 'Iris-setosa']\n",
        " [4.9 3.0 1.4 0.2 'Iris-setosa']\n",
        " [4.7 3.2 1.3 0.2 'Iris-setosa']\n",
        " [4.6 3.1 1.5 0.2 'Iris-setosa']\n",
        " [5.0 3.6 1.4 0.2 'Iris-setosa']]\n",
        "~~~\n",
        "You should notice that the `to_numpy()` converted both the numerical values as well as the strings in the `species` column to the new array. While arrays generally contain the same data type (e.g. all numbers or all strings), Numpy arrays can contain both data types. However, a Numpy array is very different than a Dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGR2sPPe0fo"
      },
      "source": [
        "### **Exercise 9: Use Pandas `to_numpy()` method to convert a DataFrame into Numpy array**\n",
        "\n",
        "In the cell below, use the Pandas `to_numpy()` method to convert the Pima data stored in `df2` into a new Numpy array called `df2_ar`. Print out the first 10 rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ploZpYDe0fo"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWfeAG68e0fo"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqYyMZ3e0fo"
      },
      "source": [
        "~~~text\n",
        "[[5 86 68 28 30.2 0.364 24 'No']\n",
        " [7 195 70 33 25.1 0.163 55 'Yes']\n",
        " [5 77 82 41 35.8 0.156 35 'No']\n",
        " [0 165 76 43 47.9 0.259 26 'No']\n",
        " [0 107 60 25 26.4 0.133 23 'No']\n",
        " [5 97 76 27 35.6 0.378 52 'Yes']\n",
        " [3 83 58 31 34.3 0.336 25 'No']\n",
        " [1 193 50 16 25.9 0.655 24 'No']\n",
        " [3 142 80 15 32.4 0.2 63 'No']\n",
        " [2 128 78 37 43.3 1.224 31 'Yes']]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqnao7dre0fo"
      },
      "source": [
        "You should notice that when you generate a Numpy array, you lose the column headers (titles) that are present in the DataFrame as well as the sequential index values at the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fxy5NVze0fo"
      },
      "source": [
        "### Example 10: Download an read a text file, line-by-line\n",
        "\n",
        "Reading very large text files can be challenging, especially with a laptop computer. One approach is to use a **_streaming method_**, which only reads data from the file, one line at a time, instead of the entire file. For example, you might only want to work on a small part of a large file. Reading the file, line-by-line, would allow you to process only the data you need without overflowing your laptop's memory by reading the entire huge file.\n",
        "\n",
        "For convience, the example below only downloads a **_small_** textfile from the website [Textfiles.com](Textfile.com) called _The Utimate Turning Test_. The textfile was written by David Barberi at the University of North Carolina (UNC) back in 1992.\n",
        "\n",
        "Again, what is important about this example is **_how_** the textfile is being processed **_line-by-line_**. You can use the same code if you need to handle a huge textfile without overflowing your laptop's memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrYEJ8XEe0fo"
      },
      "outputs": [],
      "source": [
        "# Example 10: Download and read a textfile line-by-line\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import codecs\n",
        "\n",
        "# Specify the url for the textfile\n",
        "url = \"http://textfiles.com/programming/AI/thexvirt.tes\"\n",
        "\n",
        "# Use a while loop to read the textfile line-by-line\n",
        "with urllib.request.urlopen(url) as urlstream:\n",
        "    for line in codecs.iterdecode(urlstream, 'utf-8'):\n",
        "        print(line.rstrip())  # print each line as it is read.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFMlN6NVe0fo"
      },
      "source": [
        "### **Exercise 10: Download an read a text file line-by-line**\n",
        "\n",
        "Use the code example in **Example 10** to download and print the textfile, line-by-line, that is located at:\n",
        "\n",
        "\"https://data.heatonresearch.com/data/t81-558/datasets/sonnet_18.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgJqQOnNe0fo"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 10 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_vfH0Se0fo"
      },
      "source": [
        "If your code is correct your should see William Shakespeare's _Sonnet 18_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryz_Vboee0fo"
      },
      "source": [
        "## **Image Processing**\n",
        "\n",
        "Computer vision is one of the areas that neural networks outshine other models. To support computer vision, the Python programmer needs to understand how to process images.\n",
        "\n",
        "For this course, we will use the **_Pillow_** package for image processing. _Pillow_ is an open source Python library that builds on the powerful features of the Python Imaging Library (PIL) and provides a range of image processing capabilities. Pillow is a fork of PIL and provides a more modern and easy-to-use API (Application Programming Interface).\n",
        "\n",
        "Pillow supports a wide range of image formats, such as PNG, JPEG, BMP, and GIF. In addtion, Pillow provides efficient image manipulation functions such as resizing, cropping, rotating, and creating thumbnails.\n",
        "\n",
        "Before you use _Pillow_ you will need to make sure your conda environment has the necessary packages by running the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udnzY2hRe0fo"
      },
      "outputs": [],
      "source": [
        "!pip install pillow\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cyy-nr6e0fo"
      },
      "source": [
        "Before you use _Pillow_ you will need to run the next cell to import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv2c9ZADe0fp"
      },
      "outputs": [],
      "source": [
        "# Run this cell to load PIL and related support packages\n",
        "\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViZDveQ9e0fp"
      },
      "source": [
        "You will not observe any output from running the previous cell if all of the necessary packages are installed in your current `conda` environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9CsaI8je0fp"
      },
      "source": [
        "### Example 11: Download and open an image file\n",
        "\n",
        "The code below uses the `requests.get()` function to download an image from the course web server. The `requests.get()` command is often used to get data from an internet website by making an HTTP request.\n",
        "\n",
        "The command takes a URL as its argument. URL stands for Uniform Resource Locator. It is a unique address that identifies a web page or other online resource, such as an image, document, or video. It returns a **_response object_**, which contains the \"response\" from the server. In this example, the response object is an image (photograph). The image can be accessed using the command's `content()` method, i.e. `reponse.content`.\n",
        "\n",
        "The command `Image.open()` from the Pillow library is then used to open the image stored in `response.content`. The command opens an image file and returns a Pillow Image object.\n",
        "\n",
        "The Pillow object contains all of the properties associated with the image file, such as the width, height, and color mode. It also provides methods for manipulating the image, such as resizing, cropping, rotating, and creating thumbnails.\n",
        "\n",
        "In the code below, the Pillow Image object is assigned to the variable `Roadrunner_img`. Typing the word `Roadrunner_img` displays the image to your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymHU_jn9e0fp"
      },
      "outputs": [],
      "source": [
        "# Example 11: Download an open an image file\n",
        "\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Specify the url for the data\n",
        "url = \"https://biologicslab.co/BIO1173/images/roadrunner.jpg\"\n",
        "\n",
        "# Use the requests.get () function to download the image\n",
        "response = requests.get(url)\n",
        "\n",
        "# Use Image.open to open the image\n",
        "Roadrunner_img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Display the image in Jupyter Lab\n",
        "Roadrunner_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dob5jXge0fp"
      },
      "source": [
        "You should see a color picture of a roadrunner (_Geococcyx californianus_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_MpJxToe0fp"
      },
      "source": [
        "### **Exercise 11: Download and open an image file**\n",
        "\n",
        "In the cell below, download an image of showing an artist's dipiction of the Cambrian ocean located at the URL:\"https://biologicslab.co/BIO1173/images/cambrian.jpg\". Call your image `Cambrian_img`.\n",
        "\n",
        "Open `Cambrian_img` and display it in your notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk5GSDg3e0fp"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 11 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNkPdSFae0fp"
      },
      "source": [
        "If your code is correct you should see an artist's recreation of a [Cambrian ocean](https://en.wikipedia.org/wiki/Cambrian_explosion) about 541 million years ago. The largest animal is called [Anomalocaris](https://en.wikipedia.org/wiki/Anomalocaris)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWX8L-Ve0fp"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_01_6.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------\n",
        "\n",
        "## **Lizard Tail**\n",
        "\n",
        "\n",
        "## **History of Artificial Neural Networks**\n",
        "\n",
        "\n",
        "**Artificial neural networks (ANNs)** are models created using machine learning to perform a number of tasks. Their creation was inspired by biological neural circuitry.[1][a] While some of the computational implementations ANNs relate to earlier discoveries in mathematics, the first implementation of ANNs was by psychologist Frank Rosenblatt, who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s, with the AAAI calling this period an \"AI winter\".\n",
        "\n",
        "Later, advances in hardware and the development of the backpropagation algorithm, as well as recurrent neural networks and convolutional neural networks, renewed interest in ANNs. The 2010s saw the development of a deep neural network (i.e., one with many layers) called AlexNet. It greatly outperformed other image recognition models, and is thought to have launched the ongoing AI spring, and further increasing interest in deep learning. The transformer architecture was first described in 2017 as a method to teach ANNs grammatical dependencies in language, and is the predominant architecture used by large language models such as GPT-4. Diffusion models were first described in 2015, and became the basis of image generation models such as DALL-E in the 2020s\n",
        "\n",
        "* **Perceptrons and other early neural networks**\n",
        "\n",
        "The simplest feedforward network consists of a single weight layer without activation functions. It would be just a linear map, and training it would be linear regression. Linear regression by least squares method was used by Adrien-Marie Legendre (1805) and Carl Friedrich Gauss (1795) for the prediction of planetary movement.\n",
        "\n",
        "A logical calculus of the ideas immanent in nervous activity (Warren McCulloch and Walter Pitts, 1943) studied several abstract models for neural networks using symbolic logic of Rudolf Carnap and Principia Mathematica. The paper argued that several abstract models of neural networks (some learning, some not learning) have the same computational power as Turing machines. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.\n",
        "\n",
        "In the early 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. B. Farley and Wesley A. Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).\n",
        "\n",
        "Frank Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition. A multilayer perceptron (MLP) comprised 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells. He later published a 1962 book also introduced variants and computer experiments, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).[16]:section 16 Some consider that the 1962 book developed and explored all of the basic ingredients of the deep learning systems of today.\n",
        "\n",
        "Some say that research stagnated following Marvin Minsky and Papert Perceptrons (1969).\n",
        "\n",
        "Group method of data handling, a method to train arbitrarily deep neural networks was published by Alexey Ivakhnenko and Lapa in 1967, which they regarded as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method.\n",
        "\n",
        "The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\n",
        "\n",
        "* **Backpropagation**\n",
        "\n",
        "**Backpropagation** is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was developed multiple times in early 1970s. The earliest published instance was Seppo Linnainmaa's master thesis (1970). Paul Werbos developed it independently in 1971, but had difficulty publishing it until 1982. In 1986, David E. Rumelhart et al. popularized backpropagation.\n",
        "\n",
        "* **Recurrent network architectures (RNN)**\n",
        "\n",
        "One origin of RNN was statistical mechanics. The Ising model was developed by Wilhelm Lenz and Ernst Ising in the 1920s as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time. Shun'ichi Amari in 1972 proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.[36] This was popularized as the Hopfield network (1982).\n",
        "\n",
        "Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. In 1933, Lorente de N discovered \"recurrent, reciprocal connections\" by Golgi's method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. (McCulloch & Pitts 1943) considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.\n",
        "\n",
        "Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\n",
        "\n",
        "* **LSTM**\n",
        "\n",
        "Sepp Hochreiter's diploma thesis (1991) proposed the neural history compressor, and identified and analyzed the vanishing gradient problem. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture.\n",
        "\n",
        "**Long short-term memory (LSTM)** networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. It became the default choice for RNN architecture.\n",
        "\n",
        "Around 2006, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. LSTM also improved large-vocabulary speech recognition[52][53] and text-to-speech synthesis and was used in Google voice search, and dictation on Android devices.\n",
        "\n",
        "LSTM broke records for improved machine translation, language modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.\n",
        "\n",
        "* **Convolutional neural networks (CNNs)**\n",
        "\n",
        "The origin of the CNN architecture is the \"neocognitron\" introduced by Kunihiko Fukushima in 1980. It was inspired by work of Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.\n",
        "\n",
        "In 1969, Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.\n",
        "\n",
        "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel and was one of the first CNNs, as it achieved shift invariance. It did so by utilizing weight sharing in combination with backpropagation training. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.\n",
        "\n",
        "In 1988, Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.\n",
        "\n",
        "Kunihiko Fukushima published the neocognitron in 1980. Max pooling appears in a 1982 publication on the neocognitron. In 1989, Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. It used max pooling. Learning was fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. Subsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.\n",
        "\n",
        "In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.\n",
        "\n",
        "LeNet-5, a 7-level CNN by Yann LeCun et al. in 1998, that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs, so this technique is constrained by the availability of computing resources.\n",
        "\n",
        "In 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop is a first-order optimization algorithm created by Martin Riedmiller and Heinrich Braun in 1992.\n",
        "\n",
        "* **Deep learning**\n",
        "\n",
        "The deep learning revolution started around CNN- and GPU-based computer vision.\n",
        "\n",
        "Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.\n",
        "\n",
        "A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.\n",
        "\n",
        "In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.\n",
        "\n",
        "Many discoveries were empirical and focused on engineering. For example, in 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that the ReLU worked better than widely used activation functions prior to 2011.\n",
        "\n",
        "In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.\n",
        "\n",
        "The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.\n",
        "\n",
        "In 2014, the state of the art was training very deep neural network with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed concurrently to train very deep networks: highway network and residual neural network (ResNet). The ResNet research team attempted to train deeper ones by empirically testing various tricks for training deeper networks until they discovered the deep residual network architecture.\n",
        "\n",
        "* **Generative adversarial networks**\n",
        "\n",
        "In 1991, Juergen Schmidhuber published \"artificial curiosity\", neural networks in a zero-sum game.[105] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. GANs can be regarded as a case where the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. It was extended to \"predictability minimization\" to create disentangled representations of input patterns.\n",
        "\n",
        "Other people had similar ideas but did not develop them similarly. An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo.[109] This idea was never implemented and did not involve stochasticity in the generator and thus was not a generative model. It is now known as a conditional GAN or cGAN. An idea similar to GANs was used to model animal behavior by Li, Gauci and Gross in 2013.\n",
        "\n",
        "Another inspiration for GANs was noise-contrastive estimation, which uses the same loss function as GANs and which Goodfellow studied during his PhD in 20102014.\n",
        "\n",
        "Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALLE 2 (2022) and Stable Diffusion (2022).\n",
        "\n",
        "* **Attention mechanism and Transformer**\n",
        "\n",
        "The human selective attention had been studied in neuroscience and cognitive psychology. Selective attention of audition was studied in the cocktail party effect (Colin Cherry, 1953). (Donald Broadbent, 1958) proposed the filter model of attention. Selective attention of vision was studied in the 1960s by George Sperling's partial report paradigm. It was also noticed that saccade control is modulated by cognitive processes, in that the eye moves preferentially towards areas of high salience. As the fovea of the eye is small, the eye cannot sharply resolve all of the visual field at once. The use of saccade control allows the eye to quickly scan important features of a scene.\n",
        "\n",
        "These researches inspired algorithms, such as a variant of the Neocognitron.Conversely, developments in neural networks had inspired circuit models of biological visual attention.\n",
        "\n",
        "A key aspect of attention mechanism is the use of multiplicative operations, which had been studied under the names of higher-order neural networks,multiplication units, sigma-pi units, fast weight controllers, and hyper-networks.\n",
        "\n",
        "* **Recurrent attention**\n",
        "\n",
        "During the deep learning era, attention mechanism was developed solve similar problems in encoding-decoding.\n",
        "\n",
        "The idea of encoder-decoder sequence transduction had been developed in the early 2010s. The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. A seq2seq architecture employs two RNN, typically LSTM, an \"encoder\" and a \"decoder\", for sequence transduction, such as machine translation. They became state of the art in machine translation, and was instrumental in the development of attention mechanism and Transformer.\n",
        "\n",
        "An image captioning model was proposed in 2015, citing inspiration from the seq2seq model that would encode an input image into a fixed-length vector. (Xu et al. 2015), citing (Bahdanau et al. 2014), applied the attention mechanism as used in the seq2seq model to image captioning.\n",
        "\n",
        "* **Transformer**\n",
        "\n",
        "One problem with seq2seq models was their use of recurrent neural networks, which are not parallelizable as both the encoder and the decoder processes the sequence token-by-token. The decomposable attention attempted to solve this problem by processing the input sequence in parallel, before computing a \"soft alignment matrix\" (\"alignment\" is the terminology used by (Bahdanau et al. 2014). This allowed parallel processing.\n",
        "\n",
        "The idea of using attention mechanism for self-attention, instead of in an encoder-decoder (cross-attention), was also proposed during this period, such as in differentiable neural computers and neural Turing machines. It was termed intra-attention where an LSTM is augmented with a memory network as it encodes an input sequence.\n",
        "\n",
        "These strands of development were combined in the Transformer architecture, published in Attention Is All You Need (2017). Subsequently, attention mechanisms were extended within the framework of Transformer architecture.\n",
        "\n",
        "Seq2seq models with attention still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them to be accelerated on GPUs. In 2016, decomposable attention applied attention mechanism to the feedforward network, which are easy to parallelize.[139] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title \"attention is all you need\".\n",
        "\n",
        "In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to processes all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. Its parallelizability was an important factor to its widespread use in large neural networks."
      ],
      "metadata": {
        "id": "38MsFApEDsSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4phXFAMHEW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}