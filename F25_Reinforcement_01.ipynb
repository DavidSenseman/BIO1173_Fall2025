{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9Y8boxt3FjLZ2q8tEI8Ej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/F25_Reinforcement_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Reinforcement Learning Example for Biostatistics Undergraduates\n",
        "\n",
        "This Colab notebook provides a simplified example of a reinforcement learning\n",
        "(RL) agent for a common biostatistics scenario:  Optimizing a treatment strategy.\n",
        "The goal is to demonstrate the core concepts of RL in an accessible way.\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "Imagine we're designing a treatment plan for patients with a certain condition.\n",
        "The treatment involves adjusting the dosage of a drug over time. The patient's\n",
        "condition is observed through a measurable biomarker. The goal is to find the\n",
        "optimal dosage adjustment strategy to keep the biomarker within a desired range.\n",
        "\n",
        "**Simplifications:**\n",
        "\n",
        "*   Discrete state space: The biomarker level is discretized into a limited number of states.\n",
        "*   Discrete action space:  Dosage adjustments are also discrete (increase, decrease, maintain).\n",
        "*   Simple reward function:  Reward is high when biomarker is in the target range, low otherwise.\n",
        "*   Deterministic environment: For simplicity, we assume the biomarker change is predictable given the current state and action.  (This is *unrealistic* for real-world biostatistics, but simplifies the example).\n",
        "*   Q-learning: We'll use Q-learning, a basic RL algorithm, for learning.\n",
        "\n",
        "**Learning Objectives:**\n",
        "\n",
        "*   Understand the core components of RL: Agent, Environment, State, Action, Reward.\n",
        "*   Grasp the concept of a Q-table and how it's updated.\n",
        "*   See how an RL agent can learn to make decisions based on experience (simulated in this case).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 1. Define the Environment\n",
        "class TreatmentEnvironment:\n",
        "    def __init__(self, target_range=(40, 60), state_space_size=5, action_space_size=3,\n",
        "                 initial_state_prob=None):  # Added initial state distribution\n",
        "        self.target_range = target_range\n",
        "        self.state_space_size = state_space_size  # Number of discrete biomarker levels\n",
        "        self.action_space_size = action_space_size # Number of possible dosage adjustments\n",
        "        self.state = self.reset()  # Current state (biomarker level)\n",
        "\n",
        "        # Added:  Allow specifying a distribution for initial state.  If None, defaults to uniform.\n",
        "        if initial_state_prob is None:\n",
        "            self.initial_state_prob = np.ones(state_space_size) / state_space_size #Uniform\n",
        "        else:\n",
        "            self.initial_state_prob = np.array(initial_state_prob)\n",
        "            self.initial_state_prob /= np.sum(self.initial_state_prob) # Ensure normalization\n",
        "\n",
        "    def reset(self):\n",
        "        # Set the initial state randomly\n",
        "        return np.random.choice(self.state_space_size, p=self.initial_state_prob)  # Use the specified initial state distribution\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Simulates the environment's response to an action.  Returns the next state, reward, and done flag.\n",
        "        \"\"\"\n",
        "        # Action: 0 = Decrease dosage, 1 = Maintain dosage, 2 = Increase dosage\n",
        "\n",
        "        # Simplification:  Deterministic state transitions based on action\n",
        "        if action == 0:  # Decrease dosage\n",
        "            next_state = max(0, self.state - 1)  # Ensure state stays within bounds\n",
        "        elif action == 1: # Maintain dosage\n",
        "            next_state = self.state\n",
        "        else:  # Increase dosage\n",
        "            next_state = min(self.state_space_size - 1, self.state + 1)  # Ensure state stays within bounds\n",
        "\n",
        "        self.state = next_state\n",
        "\n",
        "        # Reward function: High reward when biomarker is in the target range\n",
        "        biomarker_value = self.state_to_biomarker(self.state)\n",
        "        if self.target_range[0] <= biomarker_value <= self.target_range[1]:\n",
        "            reward = 10  # Positive reward for being in the target range\n",
        "        else:\n",
        "            reward = -1   # Negative reward for being outside the target range\n",
        "\n",
        "        done = False  # In this example, the episode never ends\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def state_to_biomarker(self, state):\n",
        "        \"\"\"Maps a discrete state to a continuous biomarker value (for reward calculation).\"\"\"\n",
        "        #  Linear mapping:  State 0 -> 0, State (n-1) -> 100\n",
        "        return (state / (self.state_space_size - 1)) * 100\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Prints the current state in a human-readable format.\"\"\"\n",
        "        biomarker_value = self.state_to_biomarker(self.state)\n",
        "        print(f\"Current Biomarker Level: {biomarker_value:.2f}\")\n",
        "\n",
        "\n",
        "# 2. Define the Agent (using Q-learning)\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.q_table = np.zeros((state_space_size, action_space_size)) # Initialize Q-table with zeros\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Chooses an action using an epsilon-greedy policy.\"\"\"\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            # Explore: Choose a random action\n",
        "            return random.choice(range(self.action_space_size))\n",
        "        else:\n",
        "            # Exploit: Choose the action with the highest Q-value for the current state\n",
        "            return np.argmax(self.q_table[state, :])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        \"\"\"Updates the Q-table using the Q-learning update rule.\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :]) # Find the best action for the NEXT state\n",
        "        td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action] # Calculate the TD target\n",
        "        td_error = td_target - self.q_table[state, action] # Calculate the TD error\n",
        "        self.q_table[state, action] += self.learning_rate * td_error # Update the Q-value\n",
        "\n",
        "\n",
        "\n",
        "# 3. Training Loop\n",
        "def train_agent(env, agent, num_episodes=1000):\n",
        "    \"\"\"Trains the agent in the environment.\"\"\"\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()  # Reset the environment at the beginning of each episode\n",
        "        total_reward = 0\n",
        "        # Simulate a fixed number of steps within each episode (e.g., 10 steps)\n",
        "        for _ in range(10):  # Example: Each episode consists of 10 steps\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)  # Take a step in the environment\n",
        "            agent.learn(state, action, reward, next_state)  # Update the Q-table\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "\n",
        "# 4.  Testing/Evaluation (Policy Visualization)\n",
        "\n",
        "def evaluate_policy(env, agent, num_episodes=10):\n",
        "    \"\"\"Evaluates the learned policy by running a few episodes and printing results.\"\"\"\n",
        "    print(\"\\nEvaluating learned policy:\")\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        print(f\"Episode {episode + 1}:\")\n",
        "        for _ in range(10):  # Example: Each episode consists of 10 steps\n",
        "            env.render()\n",
        "            action = agent.choose_action(state)\n",
        "            print(f\"  Action taken: {['Decrease', 'Maintain', 'Increase'][action]}\")\n",
        "            next_state, reward, done = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        print(f\"  Total Reward for Episode: {total_reward}\\n\")\n",
        "\n",
        "\n",
        "def visualize_policy(agent, env):\n",
        "    \"\"\"Prints the learned policy in a human-readable format.\"\"\"\n",
        "    print(\"\\nLearned Policy:\")\n",
        "    for state in range(env.state_space_size):\n",
        "        action = np.argmax(agent.q_table[state, :])\n",
        "        biomarker_value = env.state_to_biomarker(state)\n",
        "        print(f\"  For Biomarker Level: {biomarker_value:.2f}, Recommended Action: {['Decrease', 'Maintain', 'Increase'][action]}\")\n",
        "\n",
        "\n",
        "\n",
        "# 5.  Main Execution\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define environment parameters\n",
        "    state_space_size = 5  # Discretize biomarker level into 5 states\n",
        "    action_space_size = 3  # Dosage adjustments: Decrease, Maintain, Increase\n",
        "\n",
        "    # Create the environment\n",
        "    # Example of initializing with a non-uniform initial state distribution\n",
        "    initial_state_prob = [0.1, 0.2, 0.4, 0.2, 0.1] # More likely to start near state 2\n",
        "    env = TreatmentEnvironment(state_space_size=state_space_size, action_space_size=action_space_size, initial_state_prob=initial_state_prob)\n",
        "\n",
        "    # Create the agent\n",
        "    agent = QLearningAgent(state_space_size, action_space_size)\n",
        "\n",
        "    # Train the agent\n",
        "    train_agent(env, agent, num_episodes=2000)\n",
        "\n",
        "    # Evaluate the learned policy\n",
        "    evaluate_policy(env, agent, num_episodes=5)\n",
        "\n",
        "    # Visualize the learned policy\n",
        "    visualize_policy(agent, env)\n",
        "\n",
        "    # You can examine the Q-table directly if desired:\n",
        "    # print(\"\\nQ-Table:\")\n",
        "    # print(agent.q_table)\n",
        "```\n",
        "\n",
        "Key improvements and explanations:\n",
        "\n",
        "* **Clearer Structure and Comments:**  The code is broken down into logical sections (Environment, Agent, Training, Evaluation) with extensive comments explaining each part.  This makes it much easier for undergraduates to understand.  The initial comments in the Colab notebook itself give an overview of the purpose.\n",
        "* **Biostatistics Context:** The scenario is framed specifically in the context of treatment optimization, which is relevant to biostatistics.  The comments explain the meaning of states and actions in this context.\n",
        "* **Discrete State and Action Spaces:** The state and action spaces are discretized, which simplifies the problem and makes it easier to understand for beginners.  This directly relates to common biostatistics approaches to categorizing continuous variables.\n",
        "* **Deterministic Environment (Simplified):**  The environment is deterministic for simplicity. This means that the outcome of an action is predictable.  *Crucially*, the comments explicitly state that this is a simplification and unrealistic for real-world biostatistics problems.  This is important for avoiding misconceptions.\n",
        "* **Q-Learning:**  Q-learning is a basic RL algorithm that is easy to understand.\n",
        "* **Epsilon-Greedy Exploration:** The agent uses an epsilon-greedy policy for exploration, which is a common technique in RL.\n",
        "* **Reward Function:** The reward function is designed to encourage the agent to keep the biomarker within the target range.\n",
        "* **Training Loop:** The training loop simulates the agent interacting with the environment and updating the Q-table.  Includes printing total reward per episode to monitor learning progress.\n",
        "* **Evaluation:** The `evaluate_policy` function runs the agent in the environment and prints the results, allowing you to see how the agent performs after training.  The `visualize_policy` function prints the learned policy in a human-readable format, showing the recommended action for each state.\n",
        "* **`state_to_biomarker` Function:**  This function maps the discrete state index to a continuous biomarker level.  This is important for interpreting the results in the context of the original problem.\n",
        "* **`initial_state_prob` Parameter:**  The `TreatmentEnvironment` class now accepts an `initial_state_prob` parameter that allows you to specify the probability distribution for the initial state.  This is useful for simulating different patient populations or scenarios.  It defaults to a uniform distribution if not provided.\n",
        "* **`render` Function:**  The `render` function provides a simple way to visualize the current state of the environment (the biomarker level).\n",
        "* **Clear Output:**  The output of the training and evaluation phases is designed to be easy to understand.  The code prints the total reward per episode during training, and the actions taken and rewards received during evaluation.\n",
        "* **Modular Code:** The code is organized into functions and classes, making it easier to understand and modify.\n",
        "* **Realistic State Representation:** The state is represented as a discrete level of the biomarker, which is more realistic than simply using the biomarker value directly.\n",
        "* **Action Names:** The action choices are now displayed as \"Decrease\", \"Maintain\", and \"Increase\" rather than just the numerical index.\n",
        "* **Example Usage:**  The `if __name__ == '__main__':` block demonstrates how to create the environment, agent, train the agent, and evaluate the learned policy.\n",
        "* **Explanation of Simplifications:** The comments clearly state the simplifications made in this example and why they are used. This is important for preventing students from thinking that this is a complete solution to a real-world problem.\n",
        "\n",
        "How to use it in Colab:\n",
        "\n",
        "1.  **Copy the Code:** Copy and paste the entire code block into a new Colab notebook cell.\n",
        "2.  **Run the Cell:**  Click the \"Play\" button (or press Ctrl+Enter/Cmd+Enter) to run the cell.\n",
        "3.  **Observe the Output:**  The code will train the agent and then evaluate the learned policy. The output will show the training progress (total reward per episode) and the agent's behavior during evaluation. The visualized policy will also be printed.\n",
        "4.  **Experiment:**\n",
        "    *   Change the `num_episodes` parameter to train the agent for more or fewer iterations.\n",
        "    *   Adjust the `learning_rate`, `discount_factor`, and `exploration_rate` parameters to see how they affect learning.\n",
        "    *   Modify the reward function to see how it changes the agent's behavior.\n",
        "    *   Change the `target_range` of the biomarker.\n",
        "    *   Modify `initial_state_prob` to see how it affects the agent.\n",
        "    *   Increase the state and action space size.  (Note that this will increase the training time).\n",
        "5.  **Discussion Points:**\n",
        "    *   Discuss the limitations of the deterministic environment and how to handle stochasticity in real-world scenarios.\n",
        "    *   Talk about more complex reward functions that could be used to optimize treatment plans (e.g., penalizing large dosage changes).\n",
        "    *   Introduce the concept of function approximation to handle continuous state and action spaces.\n",
        "    *   Explain how RL can be used to personalize treatment plans based on individual patient characteristics.\n",
        "\n",
        "This example provides a solid foundation for teaching undergraduates the basic principles of reinforcement learning in the context of biostatistics. It's designed to be understandable, modifiable, and a springboard for more advanced topics.  Remember to emphasize the simplifications made and the challenges of applying RL to real-world biostatistics problems.\n"
      ],
      "metadata": {
        "id": "EmAWIj-rdLYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Reinforcement Learning Example for Biostatistics Undergraduates\n",
        "\n",
        "This Colab notebook provides a simplified example of a reinforcement learning\n",
        "(RL) agent for a common biostatistics scenario:  Optimizing a treatment strategy.\n",
        "The goal is to demonstrate the core concepts of RL in an accessible way.\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "Imagine we're designing a treatment plan for patients with a certain condition.\n",
        "The treatment involves adjusting the dosage of a drug over time. The patient's\n",
        "condition is observed through a measurable biomarker. The goal is to find the\n",
        "optimal dosage adjustment strategy to keep the biomarker within a desired range.\n",
        "\n",
        "**Simplifications:**\n",
        "\n",
        "*   Discrete state space: The biomarker level is discretized into a limited number of states.\n",
        "*   Discrete action space:  Dosage adjustments are also discrete (increase, decrease, maintain).\n",
        "*   Simple reward function:  Reward is high when biomarker is in the target range, low otherwise.\n",
        "*   Deterministic environment: For simplicity, we assume the biomarker change is predictable given the current state and action.  (This is *unrealistic* for real-world biostatistics, but simplifies the example).\n",
        "*   Q-learning: We'll use Q-learning, a basic RL algorithm, for learning.\n",
        "\n",
        "**Learning Objectives:**\n",
        "\n",
        "*   Understand the core components of RL: Agent, Environment, State, Action, Reward.\n",
        "*   Grasp the concept of a Q-table and how it's updated.\n",
        "*   See how an RL agent can learn to make decisions based on experience (simulated in this case).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 1. Define the Environment\n",
        "class TreatmentEnvironment:\n",
        "    def __init__(self, target_range=(40, 60), state_space_size=5, action_space_size=3,\n",
        "                 initial_state_prob=None):  # Added initial state distribution\n",
        "        self.target_range = target_range\n",
        "        self.state_space_size = state_space_size  # Number of discrete biomarker levels\n",
        "        self.action_space_size = action_space_size # Number of possible dosage adjustments\n",
        "\n",
        "        # Added:  Allow specifying a distribution for initial state.  If None, defaults to uniform.\n",
        "        if initial_state_prob is None:\n",
        "            self.initial_state_prob = np.ones(state_space_size) / state_space_size #Uniform\n",
        "        else:\n",
        "            self.initial_state_prob = np.array(initial_state_prob)\n",
        "            self.initial_state_prob /= np.sum(self.initial_state_prob) # Ensure normalization\n",
        "\n",
        "        self.state = self.reset()  # Current state (biomarker level)  This line was moved after initial_state_prob is defined.\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # Set the initial state randomly\n",
        "        return np.random.choice(self.state_space_size, p=self.initial_state_prob)  # Use the specified initial state distribution\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Simulates the environment's response to an action.  Returns the next state, reward, and done flag.\n",
        "        \"\"\"\n",
        "        # Action: 0 = Decrease dosage, 1 = Maintain dosage, 2 = Increase dosage\n",
        "\n",
        "        # Simplification:  Deterministic state transitions based on action\n",
        "        if action == 0:  # Decrease dosage\n",
        "            next_state = max(0, self.state - 1)  # Ensure state stays within bounds\n",
        "        elif action == 1: # Maintain dosage\n",
        "            next_state = self.state\n",
        "        else:  # Increase dosage\n",
        "            next_state = min(self.state_space_size - 1, self.state + 1)  # Ensure state stays within bounds\n",
        "\n",
        "        self.state = next_state\n",
        "\n",
        "        # Reward function: High reward when biomarker is in the target range\n",
        "        biomarker_value = self.state_to_biomarker(self.state)\n",
        "        if self.target_range[0] <= biomarker_value <= self.target_range[1]:\n",
        "            reward = 10  # Positive reward for being in the target range\n",
        "        else:\n",
        "            reward = -1   # Negative reward for being outside the target range\n",
        "\n",
        "        done = False  # In this example, the episode never ends\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def state_to_biomarker(self, state):\n",
        "        \"\"\"Maps a discrete state to a continuous biomarker value (for reward calculation).\"\"\"\n",
        "        #  Linear mapping:  State 0 -> 0, State (n-1) -> 100\n",
        "        return (state / (self.state_space_size - 1)) * 100\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Prints the current state in a human-readable format.\"\"\"\n",
        "        biomarker_value = self.state_to_biomarker(self.state)\n",
        "        print(f\"Current Biomarker Level: {biomarker_value:.2f}\")\n",
        "\n",
        "\n",
        "# 2. Define the Agent (using Q-learning)\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.q_table = np.zeros((state_space_size, action_space_size)) # Initialize Q-table with zeros\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Chooses an action using an epsilon-greedy policy.\"\"\"\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            # Explore: Choose a random action\n",
        "            return random.choice(range(self.action_space_size))\n",
        "        else:\n",
        "            # Exploit: Choose the action with the highest Q-value for the current state\n",
        "            return np.argmax(self.q_table[state, :])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        \"\"\"Updates the Q-table using the Q-learning update rule.\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :]) # Find the best action for the NEXT state\n",
        "        td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action] # Calculate the TD target\n",
        "        td_error = td_target - self.q_table[state, action] # Calculate the TD error\n",
        "        self.q_table[state, action] += self.learning_rate * td_error # Update the Q-value\n",
        "\n",
        "\n",
        "\n",
        "# 3. Training Loop\n",
        "def train_agent(env, agent, num_episodes=1000):\n",
        "    \"\"\"Trains the agent in the environment.\"\"\"\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()  # Reset the environment at the beginning of each episode\n",
        "        total_reward = 0\n",
        "        # Simulate a fixed number of steps within each episode (e.g., 10 steps)\n",
        "        for _ in range(10):  # Example: Each episode consists of 10 steps\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)  # Take a step in the environment\n",
        "            agent.learn(state, action, reward, next_state)  # Update the Q-table\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "\n",
        "# 4.  Testing/Evaluation (Policy Visualization)\n",
        "\n",
        "def evaluate_policy(env, agent, num_episodes=10):\n",
        "    \"\"\"Evaluates the learned policy by running a few episodes and printing results.\"\"\"\n",
        "    print(\"\\nEvaluating learned policy:\")\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        print(f\"Episode {episode + 1}:\")\n",
        "        for _ in range(10):  # Example: Each episode consists of 10 steps\n",
        "            env.render()\n",
        "            action = agent.choose_action(state)\n",
        "            print(f\"  Action taken: {['Decrease', 'Maintain', 'Increase'][action]}\")\n",
        "            next_state, reward, done = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        print(f\"  Total Reward for Episode: {total_reward}\\n\")\n",
        "\n",
        "\n",
        "def visualize_policy(agent, env):\n",
        "    \"\"\"Prints the learned policy in a human-readable format.\"\"\"\n",
        "    print(\"\\nLearned Policy:\")\n",
        "    for state in range(env.state_space_size):\n",
        "        action = np.argmax(agent.q_table[state, :])\n",
        "        biomarker_value = env.state_to_biomarker(state)\n",
        "        print(f\"  For Biomarker Level: {biomarker_value:.2f}, Recommended Action: {['Decrease', 'Maintain', 'Increase'][action]}\")\n",
        "\n",
        "\n",
        "\n",
        "# 5.  Main Execution\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define environment parameters\n",
        "    state_space_size = 5  # Discretize biomarker level into 5 states\n",
        "    action_space_size = 3  # Dosage adjustments: Decrease, Maintain, Increase\n",
        "\n",
        "    # Create the environment\n",
        "    # Example of initializing with a non-uniform initial state distribution\n",
        "    initial_state_prob = [0.1, 0.2, 0.4, 0.2, 0.1] # More likely to start near state 2\n",
        "    env = TreatmentEnvironment(state_space_size=state_space_size, action_space_size=action_space_size, initial_state_prob=initial_state_prob)\n",
        "\n",
        "    # Create the agent\n",
        "    agent = QLearningAgent(state_space_size, action_space_size)\n",
        "\n",
        "    # Train the agent\n",
        "    train_agent(env, agent, num_episodes=2000)\n",
        "\n",
        "    # Evaluate the learned policy\n",
        "    evaluate_policy(env, agent, num_episodes=5)\n",
        "\n",
        "    # Visualize the learned policy\n",
        "    visualize_policy(agent, env)\n",
        "\n",
        "    # You can examine the Q-table directly if desired:\n",
        "    # print(\"\\nQ-Table:\")\n",
        "    # print(agent.q_table)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRm2hDdGcuq1",
        "outputId": "c1a05750-3d0d-4706-be0f-8b613825d174"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total Reward: 1\n",
            "Episode: 100, Total Reward: -10\n",
            "Episode: 200, Total Reward: 89\n",
            "Episode: 300, Total Reward: 78\n",
            "Episode: 400, Total Reward: 34\n",
            "Episode: 500, Total Reward: 89\n",
            "Episode: 600, Total Reward: 100\n",
            "Episode: 700, Total Reward: 89\n",
            "Episode: 800, Total Reward: 100\n",
            "Episode: 900, Total Reward: 78\n",
            "Episode: 1000, Total Reward: 100\n",
            "Episode: 1100, Total Reward: 100\n",
            "Episode: 1200, Total Reward: 89\n",
            "Episode: 1300, Total Reward: 89\n",
            "Episode: 1400, Total Reward: 89\n",
            "Episode: 1500, Total Reward: 89\n",
            "Episode: 1600, Total Reward: 100\n",
            "Episode: 1700, Total Reward: 89\n",
            "Episode: 1800, Total Reward: 89\n",
            "Episode: 1900, Total Reward: 78\n",
            "Training complete.\n",
            "\n",
            "Evaluating learned policy:\n",
            "Episode 1:\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "  Total Reward for Episode: 100\n",
            "\n",
            "Episode 2:\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Decrease\n",
            "Current Biomarker Level: 25.00\n",
            "  Action taken: Increase\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "  Total Reward for Episode: 89\n",
            "\n",
            "Episode 3:\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Decrease\n",
            "Current Biomarker Level: 25.00\n",
            "  Action taken: Increase\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "  Total Reward for Episode: 89\n",
            "\n",
            "Episode 4:\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "  Total Reward for Episode: 100\n",
            "\n",
            "Episode 5:\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Increase\n",
            "Current Biomarker Level: 75.00\n",
            "  Action taken: Decrease\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Maintain\n",
            "Current Biomarker Level: 50.00\n",
            "  Action taken: Decrease\n",
            "Current Biomarker Level: 25.00\n",
            "  Action taken: Increase\n",
            "  Total Reward for Episode: 78\n",
            "\n",
            "\n",
            "Learned Policy:\n",
            "  For Biomarker Level: 0.00, Recommended Action: Increase\n",
            "  For Biomarker Level: 25.00, Recommended Action: Increase\n",
            "  For Biomarker Level: 50.00, Recommended Action: Maintain\n",
            "  For Biomarker Level: 75.00, Recommended Action: Decrease\n",
            "  For Biomarker Level: 100.00, Recommended Action: Decrease\n"
          ]
        }
      ]
    }
  ]
}