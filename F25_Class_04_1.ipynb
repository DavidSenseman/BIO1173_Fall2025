{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/F25_Class_04_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* **Part 4.1: Introduction to LLMs (ChatGTP) and Prompt Engineering**\n",
        "* Part 4.2: Generative AI\n",
        "* Part 4.3: Text to Images with Stable Diffusion\n",
        "* Part 4.4: ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERfMETL_-NoS",
        "outputId": "a2a99c17-6855-4d49-f6aa-78ba6fcb1bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is included as the last line in the output above."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Custom Functions\n",
        "\n",
        "Run the cell below to load custom functions used in this lesson."
      ],
      "metadata": {
        "id": "dfExM5hY3fAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "37ShL3h23sK5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "**Large Language Models (LLMs)** such as `GPT` have brought AI into mainstream use. LLMs allow regular users to interact with AI using natural language. Most of these language models require extreme processing capabilities and hardware. Because of this, application programming interfaces (APIs) accessed through the Internet are becoming common entry points for these models. One of the most compelling features of services like ChatGPT is their availability as an API. But before we dive into the depths of coding and integration, let's understand what an API is and its significance in the AI domain.\n",
        "\n",
        "API stands for **Application Programming Interface**. Think of it as a bridge or a messenger that allows two different software applications to communicate. In the context of AI and machine learning, APIs often allow developers to access a particular model or service without having to house the model on their local machine. This technique can be beneficial when the model in question, like `ChatGPT`, is large and resource-intensive.\n",
        "\n",
        "In the realm of AI, APIs have several distinct advantages:\n",
        "\n",
        "**1 Scalability:** Since the actual model runs on external servers, developers don't need to worry about scaling infrastructure.  \n",
        "**2. Maintenance:** You get to use the latest and greatest version of the model without constantly updating your local copy.  \n",
        "**3. Cost-Effective:** Leveraging external computational resources can be more cost-effective than maintaining high-end infrastructure locally, especially for sporadic or one-off tasks.  \n",
        "**4 Ease of Use:** Instead of diving into the nitty-gritty details of model implementation and optimization, developers can directly utilize its capabilities with a few lines of code.  \n",
        "\n",
        "In this section, we won't be running the neural network computations locally. We will use our PyTorch code to communicate with the `OpenAI API` to access and harness the abilities of `ChatGPT`. The actual execution of the neural network code happens on `OpenAI servers`, bringing forth a unique synergy of PyTorch's flexibility and ChatGPT's conversational mastery. (NOTE: The physical location of these servers is not disclosed for security reasons).\n",
        "\n",
        "In this section, we will make use of the `OpenAI ChatGPT API`. Further information on this API can be found here:\n",
        "\n",
        "* [OpenAI API Login/Registration](https://platform.openai.com/apps)\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/introduction/overview)\n",
        "* [OpenAI Python API Reference](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
        "* [OpenAI Python Library](https://github.com/openai/openai-python)\n",
        "* [OpenAI Cookbook for Python](https://github.com/openai/openai-cookbook/)\n",
        "* [LangChain](https://www.langchain.com/)\n"
      ],
      "metadata": {
        "id": "GFI9xF411UuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing LangChain to use the OpenAI Python Library**\n",
        "\n",
        "As we delve deeper into the intricacies of deep learning, it's crucial to understand that the tools and platforms we use are as versatile as the concepts themselves. When it comes to accessing ChatGPT, a state-of-the-art conversational AI model developed by OpenAI, there are two predominant pathways:\n",
        "\n",
        "**Direct API Access using Python's HTTP Capabilities:** Python, with its rich library ecosystem, provides utilities like requests to directly communicate with APIs over HTTP. This method involves crafting the necessary API calls, handling responses, and error checking, giving the developer a granular control over the process.\n",
        "\n",
        "**Using the Official OpenAI Python Library:** OpenAI offers an official Python library, aptly named openai, that simplifies the process of integrating with ChatGPT and other OpenAI services. This library abstracts many of the intricacies and boilerplate steps of direct API access, offering a streamlined and user-friendly approach to interacting with the model.\n",
        "\n",
        "Each approach has its advantages. `Direct API access` provides a more hands-on, granular approach, allowing developers to intimately understand the intricacies of each API call. On the other hand, using the `openai library` can accelerate development, reduce potential errors, and allow for a more straightforward integration, especially for those new to API interactions.\n",
        "\n",
        "We will make use of the `OpenAI API` through a library called `LangChain`. `LangChain` is a framework designed to simplify the creation of applications using LLMs. As a language model integration framework, `LangChain's` use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. `LangChain` allows you to quickly change between different underlying LLMs with minimal code changes.\n",
        "\n",
        "The following command installs the **LangChain** library and needed OpenAI LLM connectors."
      ],
      "metadata": {
        "id": "pYfiGIW4BL5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't forget to install these packages!\n",
        "\n",
        "!pip install langchain langchain_openai > /dev/null\n",
        "!pip install langchain-community > /dev/null"
      ],
      "metadata": {
        "id": "ZamBocuABWsq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Obtaining an OpenAI API Key**\n",
        "\n",
        "In order to delve into the practical exercises and code demonstrations within this section, you will need to obtain an **OpenAI API key**. This `key` grants access to `OpenAI's` services, including the `ChatGPT` functionality we'll be exploring. It's important to note that there is a nominal cost associated with the usage of this key, depending on the volume and intensity of requests made to OpenAI's servers.\n",
        "\n",
        "To obtain an OpenAI API key, access this [site](https://platform.openai.com/apps)."
      ],
      "metadata": {
        "id": "9ftIaZHNBzgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **OpenAI Models that are Accessible via API**\n",
        "\n",
        "OpenAI currently provides access to several models via its API, catering to a variety of tasks. Here's an overview:\n",
        "\n",
        "**1. GPT Models**\n",
        "- **GPT-4.5**: The largest and most advanced model, designed for creative tasks and agentic planning.\n",
        "- **GPT-4o**: A high-intelligence model for complex tasks, supporting text and vision inputs with a 128k context length.\n",
        "- **GPT-4o Mini**: A smaller, cost-efficient model optimized for lightweight tasks, also supporting text and vision inputs.\n",
        "\n",
        "**2. Reasoning Models**\n",
        "- **o1**: A frontier reasoning model supporting tools, structured outputs, and vision tasks with a 200k context length.\n",
        "- **o3-mini**: A cost-efficient reasoning model optimized for coding, math, and science, with support for tools and structured outputs.\n",
        "\n",
        "**3. API Endpoints**\n",
        "- **Responses API**: A unified endpoint combining functionalities like text/image input, web/file search, and reasoning.\n",
        "- **Chat Completions API**: For conversational AI tasks.\n",
        "- **Realtime API**: Enables low-latency, multimodal experiences, including speech-to-speech.\n",
        "- **Assistants API**: Builds AI assistants capable of complex, multi-step tasks.\n",
        "- **Batch API**: Processes asynchronous workloads at reduced costs.\n",
        "\n",
        "These models and tools are designed to address a wide range of use cases, from general-purpose tasks to specialized reasoning and coding challenges."
      ],
      "metadata": {
        "id": "mE49Bi1aoMcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-3.5-turbo-1106**\n",
        "\n",
        "For this class, we will generally use the model `GTP-3.5-turbo-1106`.\n",
        "\n",
        "The **GPT-3.5-turbo-1106** model is one of `OpenAI's` advanced generative AI models, recently launched as part of the Azure OpenAI Service. Here are some key details:\n",
        "\n",
        "* **Availability:**\n",
        "It was announced alongside GPT-4 Turbo at Microsoft Ignite 2023 and is now available globally through Azure OpenAI Service.\n",
        "\n",
        "* **Performance:**\n",
        "The model is designed to provide improved cost efficiency and generative capabilities for businesses. It supports a wide range of applications, including natural language understanding, text generation, and more.\n",
        "\n",
        "* **Use Cases:**\n",
        "It is optimized for tasks like conversational AI, content creation, and other applications requiring high-quality text generation."
      ],
      "metadata": {
        "id": "ZEvcgQoho_Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the next cell to specify the specific LLM for this lesson."
      ],
      "metadata": {
        "id": "jETWqMZZICno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the model you will generally use for this class\n",
        "\n",
        "LLM_MODEL = 'gpt-3.5-turbo-1106'"
      ],
      "metadata": {
        "id": "CtmYoreIC9z8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Engineering**\n",
        "\n",
        "When working with a large language model (LLM) like ChatGPT, the **prompt** serves as the foundation for interaction. It is the input or instruction provided to the model, guiding it to generate relevant and useful outputs.\n",
        "\n",
        "**1. Role of the Prompt**\n",
        "- **Instructional Guide**: The prompt shapes what the model does. Whether it's answering a question, completing a task, or writing creatively, the prompt provides the necessary context.  \n",
        "- **Boundary Setter**: A well-crafted prompt can define the scope of the task, ensuring the response is focused and doesn't deviate from the topic.  \n",
        "- **Task Optimizer**: By providing clear and concise instructions, the prompt ensures that the LLM generates responses that align with user expectations.\n",
        "\n",
        "**2. Importance of the Prompt**\n",
        "- **Determines Quality of Output**: The quality of the model's response depends heavily on the clarity and specificity of the prompt. A vague prompt can lead to irrelevant or incomplete answers, while a precise one produces accurate and valuable results.\n",
        "- **Customizable Interactions**: Prompts allow users to adapt the model to different scenarios—such as summarization, translation, or brainstorming—making it versatile and dynamic.  \n",
        "- **Reduces Ambiguity**: A good prompt minimizes room for misunderstanding, helping the model interpret the task as intended.  \n",
        "\n",
        "**3. Iterative Improvement**\n",
        "Working with LLMs is often an _iterative_  process. If the initial response isn't quite right, the user can refine the prompt, adding more detail or constraints to guide the model toward the desired result. Instead of starting over from scratch, you just edit the prompt and try it again.\n",
        "\n",
        "The prompt isn't just the input—it’s the bridge between the user’s needs and the model’s capabilities. Mastering prompt design is key to fully leveraging the potential of an LLM like ChatGPT.\n"
      ],
      "metadata": {
        "id": "XG5KgzC0dCxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Basic Query to LangChain\n",
        "\n",
        "We begin by writint a **prompt**, to ask (query) `ChatGPT` a simple question: \"What are the 5 largest cities in the USA?\".\n",
        "\n",
        "The Python code in the cell below interacts with OpenAI's GPT model using `LangChain` and the `ChatOpenAI class` to retrieve our answer.\n",
        "\n",
        "**NOTE:** This cell will not run if you do not have a valid OpenAI_Key and you have already installed your key with Google Colab.\n"
      ],
      "metadata": {
        "id": "TTSOCfbAqc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Basic Query\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0)\n",
        "\n",
        "# Define the question\n",
        "question = \"What are the five largest cities in the USA by population?\"\n",
        "\n",
        "# Use Langchain to call the OpenAI API\n",
        "response = llm.invoke(question)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyd7OBeJEgMP",
        "outputId": "b3b46c24-a0fa-4aaa-e844-37c53cbf8e80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. New York City, New York\n",
            "2. Los Angeles, California\n",
            "3. Chicago, Illinois\n",
            "4. Houston, Texas\n",
            "5. Phoenix, Arizona\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "~~~text\n",
        "1. New York City, New York\n",
        "2. Los Angeles, California\n",
        "3. Chicago, Illinois\n",
        "4. Houston, Texas\n",
        "5. Phoenix, Arizona\n",
        "~~~"
      ],
      "metadata": {
        "id": "K7V2Z3bOrqS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the response from `LangChain` is in regular English, complete with formatting. While the formatting may make it easier to read, we often have to parse the results given to us by LLMs.\n",
        "\n",
        "Later, we will see that `LangChain` can help with this as well. You will also notice that we specified a value of `0` for **temperature**; this instructs the LLM to be less creative with its responses and more consistent. Because we are working primarily with data extraction in this section, a low temperature will give us more consistent results.\n",
        "\n",
        "In `LangChain`, the temperature parameter typically ranges from **0.0** to **1.0**, though some implementations may allow values slightly above 1.0. The temperature controls the randomness of the model's output:\n",
        "\n",
        "* **Low Temperature (e.g., 0.0):** Produces more deterministic and focused responses, ideal for tasks requiring precision.\n",
        "\n",
        "* **High Temperature (e.g., 1.0):** Generates more creative and diverse outputs, useful for brainstorming or creative writing.\n",
        "\n",
        "If you're working with `LangChain` and `OpenAI models`, you can set the temperature when initializing the model or during runtime."
      ],
      "metadata": {
        "id": "9Gy52CVNEvEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Basic Query to LangChain**\n",
        "\n",
        "For **Exercise 1** think about a subject for a `Top Five List` that **you** find interesting and see what response you get back from `ChatGTP`.\n",
        "\n",
        "Feel free to change the **temperature** of your request if you want a more _creative_ response from `LangChain`. There are no \"right\" or \"wrong\" answers here as long as your code works."
      ],
      "metadata": {
        "id": "MvfJCF6wr-lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0)\n",
        "\n",
        "# Define the question\n",
        "question = \"What are the five greatest guitar players of all time?\"\n",
        "\n",
        "# Use Langchain to call the OpenAI API\n",
        "# The method and parameters might differ based on the Langchain version\n",
        "response = llm.invoke(question)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53459cc-41d8-4484-e6b9-870bca643a1a",
        "id": "x_tQt3ejr-lh"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Jimi Hendrix\n",
            "2. Eric Clapton\n",
            "3. Jimmy Page\n",
            "4. Eddie Van Halen\n",
            "5. Stevie Ray Vaughan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I am interested in guitar players, here is `LangChain's` list of the 5 greatest guitart players of all time.\n",
        "~~~text\n",
        "1. Jimi Hendrix\n",
        "2. Eric Clapton\n",
        "3. Jimmy Page\n",
        "4. Eddie Van Halen\n",
        "5. Stevie Ray Vaughan\n",
        "~~~\n",
        "\n",
        "You output will be different."
      ],
      "metadata": {
        "id": "r24AGvJYr-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Working with Prompts\n",
        "\n",
        "As mentioned above, interactions with LLMs is typically accomplished using `prompts`. In fact, there is a whole new field called **Prompt Engineering** that focuses on designing, refining, and optimizing prompts to maximize the effectiveness and relevance of outputs generated by large language models (LLMs) like ChatGPT, GPT-4, and others.\n",
        "\n",
        "In Example 2, we will \"engineer\" a prompt that will have `ChatGPT` translate text from French to English. In this example, we will just be using normal Python F-Strings to build the prompt.\n"
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Working with prompts\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Laissez les bons temps rouler\"\"\"  # French text\n",
        "style = \"American English\"                  # English\n",
        "\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "# Send promt to ChatGPT\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "# Print ChatGPTs response\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPuxwG6U3mPZ",
        "outputId": "5ac2d51c-05ac-4baa-daca-6bece3ec952e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Let the good times roll\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "\"Let the good times roll\"\n",
        "~~~"
      ],
      "metadata": {
        "id": "WySFzrpTtyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "\n",
        "**Why does the code Uses Triple Quotes?**\n",
        "\n",
        "The code in the cell above uses triple double quotes (\"\"\") for the prompt string to allow for clean, multi-line formatting and to include special characters, such as backticks (```) and placeholders ({style} and {text}).\n",
        "\n",
        "~~~text\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\"\"\"\n",
        "~~~\n",
        "\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "0lPEtXSNh42d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Working with Prompts**\n",
        "\n",
        "In the cell below, use ChatGPT to translate the German expression: \"Ein Prosit der Gemütlichkeit\" into English.\n"
      ],
      "metadata": {
        "id": "uq2WA-AYuVR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Ein Prosit der Gemütlichkeit\"\"\"   # German\n",
        "style = \"American English\"                  # English\n",
        "\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "# Send promt to ChatGPT\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "# Print ChatGPTs response\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6837df-dd95-4a3a-b317-ba70d7c56547",
        "id": "kzkL7ARauVR-"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"A toast to comfort and coziness\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "\"A toast to comfort and coziness\"\n",
        "~~~"
      ],
      "metadata": {
        "id": "VGza68NEuVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dynamic Prompts**\n",
        "\n",
        "A **dynamic prompt** is a flexible and adaptive input designed for interaction with language models (LLMs) like `ChatGP`T, where placeholders or variables are used to customize the prompt based on context or user-provided information. This approach allows for reusability, personalization, and automation, ensuring that the output is tailored to specific needs without rewriting the entire prompt.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Characteristics of a Dynamic Prompt**\n",
        "1. **Variables**:\n",
        "   - Dynamic prompts include placeholders for variables, like `{name}`, `{style}`, or `{text}`, which can be filled with different values at runtime.\n",
        "   - For example:\n",
        "     ```python\n",
        "     prompt = f\"Translate this text: {text} into {language}.\"\n",
        "     ```\n",
        "     Here, `{text}` and `{language}` can be dynamically replaced by the desired input values.\n",
        "2. **Context-Aware**:\n",
        "   - They adapt to the context, such as the user’s preferences, conversation history, or specific tasks.\n",
        "   - For instance, a dynamic prompt for summarization might consider the length of the desired output: \"Summarize the following article in less than {words} words.\"\n",
        "3. **Reusable Templates**:\n",
        "   - Instead of hardcoding individual tasks, dynamic prompts use templates that can be applied across multiple scenarios by simply replacing values.\n",
        "   - Example template:\n",
        "     ```python\n",
        "     template_text = \"\"\"Write a {tone} response to the following message:\n",
        "     message: {user_message}\"\"\"\n",
        "     ```\n",
        "4. **Personalization**:\n",
        "   - Dynamic prompts can be personalized based on user inputs or profiles, enhancing user experience. For example:\n",
        "     ```python\n",
        "     f\"Hi {name}, here’s the weather forecast for {city}!\"\n",
        "     ```\n",
        "\n",
        "#### **Why Are Dynamic Prompts Important?**\n",
        "\n",
        "- **Efficiency**: They save time by enabling template reuse.\n",
        "- **Scalability**: Useful for applications needing to handle diverse inputs.\n",
        "- **Adaptability**: They produce tailored outputs depending on the specific context or task.\n",
        "- **User Experience**: Personalization through dynamic prompts improves user satisfaction.\n",
        "\n",
        "---\n",
        "\n",
        "Dynamic prompts are at the heart of effective interactions with LLMs, making them more versatile, context-aware, and user-specific."
      ],
      "metadata": {
        "id": "azkr5E3Cii37"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "### Example 3 - Step 1: Build a Dynamic Prompt\n",
        "\n",
        "We can use LangChain to help us build dynamic prompts.\n",
        "\n",
        "The first step is provide LangChain with a `template prompt`. The code in the cell below defines and creates a prompt template using LangChain's `ChatPromptTemplate` class. The prompt template is called `example_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 1: Create prompt template\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define prompt template\n",
        "template_text = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Create template\n",
        "example_prompt_template = ChatPromptTemplate.from_template(template_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "wu_I4u1U38SI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "AlHkVSSFxhB4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcdwI-0-Nod"
      },
      "source": [
        "### Example 3 - Step 2: Build Dynamic Prompt\n",
        "\n",
        "Now we can fill in the blanks for this prompt and observe the prompt created, which is a text string.\n",
        "\n",
        "The code in the cell below does the following:\n",
        "\n",
        "* Dynamically generates a structured prompt based on a template.\n",
        "* Ensures the prompt includes placeholders (style and text) filled with the\n",
        "provided values.\n",
        "* Inspects the data structure and type of the resulting prompt.\n",
        "* Outputs the first message to verify its content.\n",
        "\n",
        "This code is useful for building prompts in LangChain when interacting with language models for tasks like translation, summarization, or custom instructions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 2: Use template to create prompt\n",
        "\n",
        "example_prompt = example_prompt_template.format_messages(\n",
        "                    style=\"American English\",\n",
        "                    text=\"千里之行，始于足下。\")\n",
        "\n",
        "# Print prompt and its features\n",
        "print(type(example_prompt))\n",
        "print(type(example_prompt[0]))\n",
        "\n",
        "print(example_prompt[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2HpNiaY1NWa",
        "outputId": "2d0622dc-75d4-4880-96eb-12654aa50044"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n",
            "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```千里之行，始于足下。```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, the first part of the output should look like this:\n",
        "\n",
        "~~~text\n",
        "<class 'list'>\n",
        "<class 'langchain_core.messages.human.HumanMessage'>\n",
        "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```千里之行，始于足下。```\\n'\n",
        "~~~"
      ],
      "metadata": {
        "id": "OqIK6nGz31Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are asking `ChatGPT` to translate the `text` \"千里之行，始于足下。\" into the `style` \"American English\"."
      ],
      "metadata": {
        "id": "PisALL0AkZZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3 - Step 3: Build Dynamic Prompt\n",
        "\n",
        "Now that we have buit our dynamic prompt in Steps 1 and 2, we are ready to send it to `ChatGPT` for analysis as shown in the code below."
      ],
      "metadata": {
        "id": "PSn4MbiMGPeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 3: Send prompt to llm for analysis\n",
        "\n",
        "\n",
        "# Call the LLM to translate to the style of the customer message\n",
        "example_response = llm.invoke(example_prompt)\n",
        "\n",
        "# Print response from ChatGPT\n",
        "print(example_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Eb-aLHn4L8_",
        "outputId": "db1a1e4a-54ec-42b0-bff7-9ba0f0523fa0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='\" A journey of a thousand miles begins with a single step.\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 42, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_c66b5540ac', 'id': 'chatcmpl-BGEVgB3WlgP6u4MbtSfpDznOREMJL', 'finish_reason': 'stop', 'logprobs': None} id='run-83141145-faf3-4dd6-a9fa-465fe2d1c92d-0' usage_metadata={'input_tokens': 42, 'output_tokens': 14, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, the first part of the output should look like this:\n",
        "\n",
        "~~~text\n",
        "content='\" A journey of a thousand miles begins with a single step.\"'\n",
        "~~~"
      ],
      "metadata": {
        "id": "nn-lK6Hf3vMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphfSQIf5su_"
      },
      "source": [
        "### **Exercise 3 - Step 1: Build Dynamic Prompt**\n",
        "\n",
        "In the cell below, create a prompt template called `exercise_prompt_template`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 1 here\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define prompt template\n",
        "template_text = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Create template\n",
        "exercise_prompt_template = ChatPromptTemplate.from_template(template_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "F-an-XKj5su_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "lxXCSm4D5su_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVCBV3k5su_"
      },
      "source": [
        "### **Exercise 3 - Step 2: Build Dynamic Prompt**\n",
        "\n",
        "Suppose you are standing watch at the White House and you receive this urgent message: \"Президент Трамп, русская Родина сдаётся\". Use your `exercise_prompt_template` to translate this message into English."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 2 here\n",
        "\n",
        "exercise_prompt = exercise_prompt_template.format_messages(\n",
        "                    style=\"American English\",\n",
        "                    text=\"Президент Трамп, русская Родина сдаётся.\")\n",
        "\n",
        "# Print prompt and its features\n",
        "print(type(exercise_prompt))\n",
        "print(type(exercise_prompt[0]))\n",
        "\n",
        "print(exercise_prompt[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4b86af-1bdd-4421-cebf-a7cdc973227c",
        "id": "32bYXW8K5su_"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n",
            "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```Президент Трамп, русская Родина сдаётся.```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, the first part of the output should look like this:\n",
        "\n",
        "~~~text\n",
        "<class 'langchain_core.messages.human.HumanMessage'>\n",
        "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```Президент Трамп, русская Родина сдаётся.\n",
        "~~~"
      ],
      "metadata": {
        "id": "3n2DHLNY5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3 - Step 3: Build Dynamic Prompt**\n",
        "\n",
        "Finally, send your `exercise_prompt_template` to `ChatGPT` to translate the urgent message in English."
      ],
      "metadata": {
        "id": "6-GzJjFY5svA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 3 here\n",
        "\n",
        "\n",
        "# Call the LLM to translate to the style of the customer message\n",
        "exercise_response = llm.invoke(exercise_prompt)\n",
        "\n",
        "# Print response from ChatGPT\n",
        "print(exercise_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2b9227-bd31-49df-a86f-a1e0939a5bf4",
        "id": "pGbSFa9_5svA"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='President Trump, the Russian Motherland is surrendering.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 52, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_c66b5540ac', 'id': 'chatcmpl-BGEW0PCdmQ7emLeR5KsTbXHJclHcj', 'finish_reason': 'stop', 'logprobs': None} id='run-a487e1fe-1ab6-4723-a969-6a0a54274c35-0' usage_metadata={'input_tokens': 52, 'output_tokens': 12, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, the first part of the output should look like this:\n",
        "\n",
        "~~~text\n",
        "content='President Trump, ...'\n",
        "~~~"
      ],
      "metadata": {
        "id": "oQXyYQMm5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM Memory**\n",
        "\n",
        "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is David\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which `LangChain` provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation."
      ],
      "metadata": {
        "id": "OqG349Nm3WMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conversation Buffer Window Memory**\n",
        "\n",
        "The `LangChain library` includes a conversation object named **ConversationChain**; this object facilitates an ongoing conversation with an LLM. For any conversation object, you must also specify a memory. For this first example, we will use the **ConversationBufferWindowMemory** object. This object keeps a transcript of the most recent conversation to reference. This memory allows the conversation object to remember what you have asked or told it and its responses to you."
      ],
      "metadata": {
        "id": "KOr_RRl53huN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.retrievers import ZepCloudRetriever\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_openai import ChatOpenAI  # Assuming you're using OpenAI; adjust if needed\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "\n",
        "\n",
        "# Define your language model (replace with your actual LLM)\n",
        "#llm = ChatOpenAI(temperature=0.7) #Or whatever parameters you want.\n",
        "\n",
        "LLM_MODEL = 'gpt-3.5-turbo-1106'\n",
        "\n",
        "# Create memory\n",
        "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\") #Important to set memory_key\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create a chain\n",
        "runnable = prompt | llm\n",
        "\n",
        "def get_session_history(session_id: str): # Needed for RunnableWithMessageHistory\n",
        "    return memory.load_memory_variables({})[\"chat_history\"]  # Load full history\n",
        "\n",
        "\n",
        "# Use the RunnableWithMessageHistory\n",
        "conversation = RunnableWithMessageHistory(\n",
        "    runnable=runnable,\n",
        "    get_session_history=get_session_history,\n",
        "    history_buffer=memory,\n",
        "    input_key=\"input\",\n",
        "    output_key=\"content\"\n",
        ")\n",
        "\n",
        "\n",
        "#Example usage\n",
        "# inputs = {\"input\": \"Hi, how are you?\", \"session_id\": \"123\"}\n",
        "# response = conversation.invoke(inputs)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "TSiCJLAuXGU1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.retrievers import ZepCloudRetriever\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_openai import ChatOpenAI  # Assuming you're using OpenAI; adjust if needed\n",
        "import os\n",
        "\n",
        "LLM_MODEL = 'gpt-3.5-turbo-1106'\n",
        "\n",
        "# Set key code for OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"???\"  # Set the API key in the environment\n",
        "\n",
        "# Define your language model (replace with your actual LLM)\n",
        "llm = ChatOpenAI(model_name=LLM_MODEL, temperature=0.7) #Or whatever parameters you want.  Pass the model *name* into ChatOpenAI\n",
        "\n",
        "# Create memory\n",
        "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\") #Important to set memory_key\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create a chain\n",
        "runnable = prompt | llm\n",
        "\n",
        "def get_session_history(session_id: str): # Needed for RunnableWithMessageHistory\n",
        "    return memory.load_memory_variables({})[\"chat_history\"]  # Load full history\n",
        "\n",
        "\n",
        "# Use the RunnableWithMessageHistory\n",
        "conversation = RunnableWithMessageHistory(\n",
        "    runnable=runnable,\n",
        "    get_session_history=get_session_history,\n",
        "    history_buffer=memory,\n",
        "    input_key=\"input\",\n",
        "    output_key=\"content\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "#Example usage\n",
        "# inputs = {\"input\": \"Hi, how are you?\", \"session_id\": \"123\"}\n",
        "# response = conversation.invoke(inputs)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "ikykTezoYenO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.retrievers import ZepCloudRetriever\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_openai import ChatOpenAI  # Assuming you're using OpenAI; adjust if needed\n",
        "\n",
        "LLM_MODEL = 'gpt-3.5-turbo-1106'\n",
        "\n",
        "# Define your language model (replace with your actual LLM)\n",
        "llm = ChatOpenAI(model_name=LLM_MODEL, temperature=0.7) #Or whatever parameters you want.  Pass the model *name* into ChatOpenAI\n",
        "\n",
        "# Create memory\n",
        "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\") #Important to set memory_key\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create a chain\n",
        "runnable = prompt | llm\n",
        "\n",
        "def get_session_history(session_id: str): # Needed for RunnableWithMessageHistory\n",
        "    return memory.load_memory_variables({})[\"chat_history\"]  # Load full history\n",
        "\n",
        "\n",
        "# Use the RunnableWithMessageHistory\n",
        "conversation = RunnableWithMessageHistory(\n",
        "    runnable=runnable, #Runnable must be first\n",
        "    get_session_history=get_session_history, # Then get_session_history\n",
        "    history_buffer=memory,\n",
        "    verbose=False, # Finally the keyword arguments\n",
        "    input_key=\"input\", #And any other keyword arguments\n",
        "    output_key=\"content\"\n",
        ")\n",
        "\n",
        "\n",
        "#Example usage\n",
        "# inputs = {\"input\": \"Hi, how are you?\", \"session_id\": \"123\"}\n",
        "# response = conversation.invoke(inputs)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "fRd_1QDuZLaD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Conversation buffer window memory\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "#RunnableWithMessageHistory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43EDXADt3kiE",
        "outputId": "b8b6a092-a948-4145-aaf2-2abaec81277b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-170d340bf8b3>:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory()\n",
            "<ipython-input-31-170d340bf8b3>:7: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now have a conversation with the LLM.\n"
      ],
      "metadata": {
        "id": "CdM2Z_qT3rY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation.\n"
      ],
      "metadata": {
        "id": "jdcQ--EW308T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "conversation.predict(input=\"Hi, my name is David\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aPOUhgWm3wrT",
        "outputId": "1950c4f1-b462-4fc1-fe1c-dd4c4f6e691c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello David! It's nice to meet you. My name is AI-9000. I am an artificial intelligence designed to assist with various tasks and provide information. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a0R-QtxG36aq",
        "outputId": "4cf6e297-33c0-4031-933b-efe5be35382e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is David.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains.\n"
      ],
      "metadata": {
        "id": "W3M-I6Y237H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvFazK1y4B3K",
        "outputId": "e58575b2-3136-48dc-9584-9c5552f333d2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is David\\nAI: Hello David! It's nice to meet you. My name is AI-9000. I am an artificial intelligence designed to assist with various tasks and provide information. How can I help you today?\\nHuman: What is my name?\\nAI: Your name is David.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Conversation Bots**\n",
        "\n",
        "You can define the prompt template for a conversationbot. This technique allows you to create a bot with a name and perform a specialized task. In this case, we created a bot named \"WashU Assistant\" that we designed to help students."
      ],
      "metadata": {
        "id": "68eHC1aG4Hpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original code\n",
        "# Now we can override it and set it to \"AI Assistant\"\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = \"\"\"The following is a friendly conversation between a human and an\n",
        "AI to assist UTSA Students. The AI should stick to topics\n",
        "about The University of Texas at San Antonion (UTSA). If the AI does not know the answer to a question,\n",
        "it should suggest the student speak to their advisor.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "UTSA Assistant:\"\"\"\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "conversation = ConversationChain(\n",
        "    prompt=PROMPT,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferWindowMemory(ai_prefix=\"UTSA Assistant\"),\n",
        ")"
      ],
      "metadata": {
        "id": "DjCXhmeC4M1p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now have a conversation with the UTSA assistant bot.\n"
      ],
      "metadata": {
        "id": "S-Z6twtm4RY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Orignal code\n",
        "conversation.predict(input=\"Where is the bookstore?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GULY9PQ24RCp",
        "outputId": "2a82efb0-e1bf-46d6-9eb6-ed2a1fea5495"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The bookstore is located at the Main Campus in the University Center. You can also visit their website for more information on their hours and location.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Another question.\n"
      ],
      "metadata": {
        "id": "qQyuN_Vo4bQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is a nice quiet area to study?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fAdIKntq4fPQ",
        "outputId": "17683a19-0f15-443d-d87f-f7f916036d06"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The John Peace Library is a great place to study. It has designated quiet areas and plenty of resources for students. The Sombrilla is also a great outdoor space for quiet study.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often you will have multiple components in langchain that you must call in a \"chain\", to do this you can construct a chain.\n"
      ],
      "metadata": {
        "id": "vrR6kULk4kBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Which of these is closest to the bookstore?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3fe53I824jwA",
        "outputId": "caae5dc9-758e-418e-eede-51972eb20a7c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The John Peace Library is closest to the bookstore, as it is also located on the Main Campus.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jJ-LVr-N4sFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is the meaning of life.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2XtoBkC74r0H",
        "outputId": "bac2c20b-de8c-48b6-c40b-8993c59705b2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm here to assist with UTSA-related questions. If you have any other questions, I recommend speaking to your advisor for guidance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "DnCCeYXB4vo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6shH4iq24vUH",
        "outputId": "4e8ddc25-a3e4-45cb-8172-5617019bd151"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Where is the bookstore?\\nUTSA Assistant: The bookstore is located at the Main Campus in the University Center. You can also visit their website for more information on their hours and location.\\nHuman: What is a nice quiet area to study?\\nUTSA Assistant: The John Peace Library is a great place to study. It has designated quiet areas and plenty of resources for students. The Sombrilla is also a great outdoor space for quiet study.\\nHuman: Which of these is closest to the bookstore?\\nUTSA Assistant: The John Peace Library is closest to the bookstore, as it is also located on the Main Campus.\\nHuman: What is the meaning of life.\\nUTSA Assistant: I'm here to assist with UTSA-related questions. If you have any other questions, I recommend speaking to your advisor for guidance.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conversation Summary Memory**\n",
        "\n",
        "Now, let's look at using a slightly more complex type of memory, the ConversationSummaryMemory object. This type of memory creates a summary of the conversation over time. This memory can be helpful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation and stores the current summary in memory. You can use this memory to inject the conversation summary so far into a prompt/chain. This memory is most useful for more extended conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ],
      "metadata": {
        "id": "Q0wxrdpQ43P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P26cUk8O46WG",
        "outputId": "0ef96ce4-e42e-4c88-f938-9d1cdc6285a3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-c71968a3d896>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I am a computational biologist, what do you do for a living?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fXHXSaop4-jG",
        "outputId": "aa7b1ffc-7496-41e0-e124-4b753b255557"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an artificial intelligence programmed to assist users with a variety of tasks. I have access to a vast amount of information and can perform complex calculations and analyses. My main function is to provide helpful and accurate information to users like yourself. I am constantly learning and updating my knowledge base to better assist with a wide range of inquiries.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D2KPEYq5BqW",
        "outputId": "ed963d58-3a0e-49dd-f483-d3e652341c6d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'The human reveals they are a computational biologist and asks the AI what it does for a living. The AI responds that it is programmed to assist users with tasks, has access to a vast amount of information, and constantly learns and updates its knowledge base to better assist with inquiries.'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What are Embedding Layers in PyTorch**\n",
        "\n",
        "[Embedding Layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.  \n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score."
      ],
      "metadata": {
        "id": "HrvnI_Bc5FsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Embedding Layer Example**\n",
        "\n",
        "* **num_embeddings** = How large is the vocabulary?  How many categories are you encoding? This parameter is the number of items in your \"lookup table.\"\n",
        "* **embedding_dim** = How many numbers in the vector you wish to return.\n",
        "\n",
        "Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to 4 number vectors. This neural network does nothing more than passing the embedding on to the output. But it does let us see what the embedding is doing. Each feature vector coming in will have two such features."
      ],
      "metadata": {
        "id": "7_GFjbN85IO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
        "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cm9sUuvB5HJd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the structure of this neural network to see what is happening inside it."
      ],
      "metadata": {
        "id": "8FPeUOYo5R5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer)"
      ],
      "metadata": {
        "id": "PwZqrW_f5U90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae80c59b-86cb-40ae-cea6-1aec54b2a8fe"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(10, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n",
        "\n",
        "Now, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."
      ],
      "metadata": {
        "id": "0yeTOmBR5adE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor([[1, 2]], dtype=torch.long)\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)\n"
      ],
      "metadata": {
        "id": "ciLevhAK5b-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6a7e98-5acc-483f-fb98-125b13678aa4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([[[-0.8821,  1.1121, -0.1438,  0.2211],\n",
            "         [ 0.7446, -0.6496, -0.0872, -0.2278]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see two length-4 vectors that PyTorch looked up for each input integer. Recall that Python arrays are zero-based. PyTorch replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, PyTorch returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table.\n"
      ],
      "metadata": {
        "id": "5vnKbsJP5g_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer.weight.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA4_7X525iy9",
        "outputId": "894fe5b3-ed45-4d13-a946-c5e50bb85294"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1833,  0.8969, -0.9070,  0.3334],\n",
              "        [-0.8821,  1.1121, -0.1438,  0.2211],\n",
              "        [ 0.7446, -0.6496, -0.0872, -0.2278],\n",
              "        [ 0.1762,  0.1579,  2.1575,  0.1204],\n",
              "        [-0.4457,  0.8898,  0.0449, -1.7915],\n",
              "        [ 0.8981, -0.0472,  0.1385, -1.0344],\n",
              "        [ 1.0725,  0.1875, -0.5093,  0.2953],\n",
              "        [ 0.5581, -0.2503, -0.1009, -0.5546],\n",
              "        [ 0.1786, -0.6790,  0.1381,  0.0634],\n",
              "        [-2.0252, -0.1882,  0.0104,  0.2818]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values above are random parameters that PyTorch generated as starting points. Generally, we will transfer an embedding or train these random values into something useful. The following section demonstrates how to embed a hand-coded embedding."
      ],
      "metadata": {
        "id": "QKH8_tVm5mgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transferring An Embedding**\n",
        "\n",
        "Now, we see how to hard-code an embedding lookup that performs a simple one-hot encoding.  One-hot encoding would transform the input integer values of 0, 1, and 2 to the vectors $[1,0,0]$, $[0,1,0]$, and $[0,0,1]$ respectively. The following code replaced the random lookup values in the embedding layer with this one-hot coding-inspired lookup table."
      ],
      "metadata": {
        "id": "uyGGGkql5oJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the embedding lookup matrix\n",
        "embedding_lookup = torch.tensor([\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [0, 0, 1]\n",
        "], dtype=torch.float32)  # Make sure to use float32 for weight matrices\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_layer = nn.Embedding(num_embeddings=3, embedding_dim=3)\n",
        "\n",
        "# Set the weights of the embedding layer\n",
        "embedding_layer.weight.data = embedding_lookup\n"
      ],
      "metadata": {
        "id": "Pv-bws9k5tKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the following parameters for the Embedding layer:\n",
        "    \n",
        "* input_dim=3 - There are three different integer categorical values allowed.\n",
        "* output_dim=3 - Three columns represent a categorical value with three possible values per one-hot encoding.\n",
        "* input_length=2 - The input vector has two of these categorical values.\n",
        "\n",
        "We query the neural network with two categorical values to see the lookup performed."
      ],
      "metadata": {
        "id": "irPtffc65y0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input tensor directly in PyTorch\n",
        "input_tensor = torch.tensor([[0, 1]], dtype=torch.long)\n",
        "\n",
        "# Forward pass to get the predictions\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws88CnPX50_6",
        "outputId": "e24e2c37-4829-4b42-d2f7-b026f431cb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([[[1., 0., 0.],\n",
            "         [0., 1., 0.]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given output shows that we provided the program with two rows from the one-hot encoding table. This encoding is a correct one-hot encoding for the values 0 and 1, where there are up to 3 unique values possible.\n",
        "\n",
        "The following section demonstrates how to train this embedding lookup table."
      ],
      "metadata": {
        "id": "pctkMP2756BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training an Embedding**\n",
        "\n",
        "First, we make use of the following imports."
      ],
      "metadata": {
        "id": "RhjVde4k59vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "O9bVKJtX6ADR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a neural network that classifies restaurant reviews according to positive or negative. This neural network can accept strings as input, such as given here. This code also includes positive or negative labels for each review."
      ],
      "metadata": {
        "id": "CBKbqehz6FDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 10 resturant reviews.\n",
        "reviews = [\n",
        "    'Never coming back!',\n",
        "    'Horrible service',\n",
        "    'Rude waitress',\n",
        "    'Cold food.',\n",
        "    'Horrible food!',\n",
        "    'Awesome',\n",
        "    'Awesome service!',\n",
        "    'Rocks!',\n",
        "    'poor work',\n",
        "    'Couldn\\'t have done better']\n",
        "\n",
        "# Define labels (1=negative, 0=positive)\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "cOrQSzH76IsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the second to the last label is incorrect.  Errors such as this are not too out of the ordinary, as most training data could have some noise.\n",
        "\n",
        "We define a vocabulary size of 50 words.  Though we do not have 50 words, it is okay to use a value larger than needed.  If there are more than 50 words, the least frequently used words in the training set are automatically dropped by the embedding layer during training.  For input, we one-hot encode the strings.  We use the TensorFlow one-hot encoding method here rather than Scikit-Learn. Scikit-learn would expand these strings to the 0's and 1's as we would typically see for dummy variables.  TensorFlow translates all words to index values and replaces each word with that index."
      ],
      "metadata": {
        "id": "VEnaCapw6N7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode reviews\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n",
        "\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRcI_DwC6Sgw",
        "outputId": "dd54762b-5fe6-4522-ac0d-65cfed6457c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded reviews: [tensor([41,  4, 10]), tensor([ 9, 23]), tensor([ 9, 37]), tensor([45, 13]), tensor([ 9, 43]), tensor([15]), tensor([15, 16]), tensor([20]), tensor([35, 24]), tensor([12, 28, 15, 33])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The program one-hot encodes these reviews to word indexes; however, their lengths are different. We pad these reviews to 4 words and truncate any words beyond the fourth word.\n"
      ],
      "metadata": {
        "id": "Vx9wPDXb6Yqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 4\n",
        "padded_reviews = pad_sequence(encoded_reviews, batch_first=True, padding_value=0).narrow(1, 0, MAX_LENGTH)\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI3_DKYz6b9A",
        "outputId": "4ff1ab74-30f8-4e61-d21a-5c89c5029e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[41,  4, 10,  0],\n",
            "        [ 9, 23,  0,  0],\n",
            "        [ 9, 37,  0,  0],\n",
            "        [45, 13,  0,  0],\n",
            "        [ 9, 43,  0,  0],\n",
            "        [15,  0,  0,  0],\n",
            "        [15, 16,  0,  0],\n",
            "        [20,  0,  0,  0],\n",
            "        [35, 24,  0,  0],\n",
            "        [12, 28, 15, 33]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As specified by the padding=post setting, each review is padded by appending zeros at the end, as specified by the padding=post setting.\n",
        "\n",
        "Next, we create a neural network to learn to classify these reviews.\n"
      ],
      "metadata": {
        "id": "TtOt4D4W6eyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Embedding(VOCAB_SIZE, 8),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(8 * MAX_LENGTH, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n"
      ],
      "metadata": {
        "id": "cf7M4YhF6hzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network accepts four integer inputs that specify the indexes of a padded movie review. The first embedding layer converts these four indexes into four length vectors 8. These vectors come from the lookup table that contains 50 (VOCAB_SIZE) rows of vectors of length 8. This encoding is evident by the 400 (8 times 50) parameters in the embedding layer. The output size from the embedding layer is 32 (4 words expressed as 8-number embedded vectors). A single output neuron is connected to the embedding layer by 33 weights (32 from the embedding layer and a single bias neuron). Because this is a single-class classification network, we use the sigmoid activation function and binary_crossentropy.\n",
        "\n",
        "The program now trains the neural network. The embedding lookup and dense 33 weights are updated to produce a better score.\n"
      ],
      "metadata": {
        "id": "SqEEbJ0G6w8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(padded_reviews.long())\n",
        "    loss = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "uMZXDMY260dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the learned embeddings. Think of each word's vector as a location in the 8 dimension space where words associated with positive reviews are close to other words. Similarly, training places negative reviews close to each other. In addition to the training setting these embeddings, the 33 weights between the embedding layer and output neuron similarly learn to transform these embeddings into an actual prediction. You can see these embeddings here."
      ],
      "metadata": {
        "id": "psYAqjGY64H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights = list(model[0].parameters())[0]\n",
        "print(embedding_weights.shape)\n",
        "print(embedding_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFbGhsyk66me",
        "outputId": "1a77a656-356a-42c3-b67e-0392c4e917e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 8])\n",
            "Parameter containing:\n",
            "tensor([[-3.6938e-03, -1.6528e+00,  5.1293e-01, -6.5018e-01, -1.6780e+00,\n",
            "          1.5641e+00, -7.6913e-02,  1.0598e+00],\n",
            "        [ 1.2130e+00, -9.0166e-01, -8.2498e-01,  9.4886e-01,  3.5045e-02,\n",
            "         -1.0788e+00, -1.3441e+00,  6.8278e-01],\n",
            "        [ 1.1870e+00,  9.0185e-02, -8.9025e-01, -6.7182e-01, -1.9944e+00,\n",
            "         -4.7542e-01, -8.6582e-02,  7.3596e-01],\n",
            "        [ 6.6124e-01, -4.8951e-01, -6.9090e-01, -3.3689e-01,  5.5057e-01,\n",
            "          2.8950e+00,  3.5120e-01,  5.8875e-01],\n",
            "        [-1.1724e+00,  1.3289e+00,  3.5524e-01, -2.6266e-01,  7.2203e-01,\n",
            "          5.1880e-01, -1.2990e+00,  1.0057e+00],\n",
            "        [-4.0645e-01, -8.9822e-01,  1.6478e+00,  2.0814e+00,  3.9192e-01,\n",
            "          1.4387e+00,  8.6763e-02,  2.0673e-02],\n",
            "        [ 5.0871e-01,  1.4726e+00,  1.5182e+00,  1.0419e-01, -1.5893e+00,\n",
            "          7.2092e-01,  1.3414e+00,  1.4816e-01],\n",
            "        [-1.9628e+00,  1.4012e+00,  1.4988e-01, -8.2729e-01,  2.8709e+00,\n",
            "         -5.1300e-01,  5.0232e-01, -4.5018e-02],\n",
            "        [ 2.1090e-01, -2.3844e+00,  2.3657e-01, -1.3568e+00, -6.3592e-01,\n",
            "          1.8317e+00,  2.9286e-01, -2.9368e-01],\n",
            "        [-3.8117e-03, -1.8798e-01, -1.4336e+00,  1.6560e+00,  8.1416e-01,\n",
            "         -7.1018e-01, -1.2023e+00, -4.7098e-01],\n",
            "        [-2.0756e+00,  1.1680e+00,  6.4795e-01,  1.4955e-01, -1.1348e-01,\n",
            "         -1.7249e+00, -1.1993e+00, -9.0226e-02],\n",
            "        [ 2.8480e+00,  1.3275e+00, -4.7149e-01, -5.8171e-01, -1.0796e+00,\n",
            "          8.1513e-01,  1.0025e-01, -1.2724e+00],\n",
            "        [ 2.2106e-01,  6.4634e-02, -1.1952e+00,  3.4600e-01,  7.3538e-02,\n",
            "          3.1526e-01, -7.8745e-01, -1.1729e+00],\n",
            "        [-9.0970e-01, -8.4068e-01,  1.2620e+00,  1.6048e-01,  1.5554e+00,\n",
            "          1.4859e+00, -7.7734e-01, -1.3432e+00],\n",
            "        [-9.1398e-01, -1.3436e+00,  8.7470e-01, -1.8991e+00,  1.0243e+00,\n",
            "          3.6985e-01, -3.2757e-01,  1.2122e-01],\n",
            "        [ 8.4426e-01, -8.4360e-01,  3.9716e-01,  4.9484e-01,  1.2873e+00,\n",
            "          1.2188e-01, -1.7403e+00,  9.3993e-01],\n",
            "        [ 1.2494e+00, -2.8176e-01, -8.2930e-01,  1.2977e-02, -1.8424e-01,\n",
            "          9.4962e-01, -5.2392e-01,  1.0157e+00],\n",
            "        [ 3.4071e-02,  6.6616e-01, -3.2070e-01, -1.1696e-01,  1.0151e+00,\n",
            "         -7.1429e-01, -5.7087e-01,  1.4015e+00],\n",
            "        [ 1.2927e+00, -2.1581e-01,  1.7257e-01,  7.2278e-01,  4.9336e-01,\n",
            "          3.7861e-01,  7.3444e-01, -8.5285e-01],\n",
            "        [ 5.2758e-01, -5.6527e-02,  2.1760e-01,  8.1815e-01,  7.8764e-01,\n",
            "          8.3094e-01,  2.8616e-01,  1.7783e+00],\n",
            "        [-2.9288e-02,  1.8829e+00,  6.6611e-01, -5.6959e-01, -2.4208e-02,\n",
            "         -1.3428e+00,  9.0264e-01, -1.2136e+00],\n",
            "        [ 1.3306e+00,  1.3997e+00, -3.4907e-01,  8.9256e-01,  8.0069e-01,\n",
            "         -7.2267e-01,  1.2575e+00,  1.2173e+00],\n",
            "        [-7.5146e-01,  1.5620e+00, -7.3242e-01, -8.8110e-01, -1.1447e+00,\n",
            "         -6.3454e-01, -1.1905e-01,  3.5124e-01],\n",
            "        [-4.8822e-01, -1.3772e+00,  2.8349e-01,  1.9622e+00, -2.2982e-01,\n",
            "         -9.3833e-01, -8.2176e-01, -2.9139e-01],\n",
            "        [-2.8191e+00,  1.2054e+00,  2.4478e-01,  8.4063e-01,  2.4744e-01,\n",
            "          5.4425e-02,  3.9991e-01,  7.3168e-01],\n",
            "        [-6.2009e-01, -3.2119e-01,  2.5925e-01,  8.0959e-01, -7.1931e-01,\n",
            "         -1.1391e+00, -4.9560e-01,  5.5982e-01],\n",
            "        [ 5.7765e-02, -7.0055e-01,  5.2673e-01, -1.4976e+00, -1.9996e+00,\n",
            "         -1.1379e+00, -1.3899e+00,  1.8719e+00],\n",
            "        [-1.8151e+00,  6.4554e-01, -5.8854e-01,  1.1310e+00, -4.6282e-01,\n",
            "         -5.3839e-01,  3.5288e-01, -2.3653e-01],\n",
            "        [-1.0885e+00, -1.0339e+00, -1.2594e-03, -7.3208e-01,  3.7516e-01,\n",
            "         -7.7218e-02,  1.1068e-01,  4.9829e-01],\n",
            "        [-1.8062e-01,  1.2640e+00, -1.6224e-02, -1.1902e+00, -6.8068e-01,\n",
            "         -8.6657e-01, -1.2172e+00, -9.5064e-01],\n",
            "        [ 3.7848e-01,  1.2944e-01,  1.0018e+00, -7.3414e-01,  1.4653e+00,\n",
            "          6.3409e-01,  3.4847e-01, -9.0373e-01],\n",
            "        [-5.1206e-01,  5.2691e-01, -1.7663e+00, -8.5174e-01,  2.0474e+00,\n",
            "          3.9085e-01, -1.0276e+00, -7.8203e-01],\n",
            "        [ 1.0556e+00, -4.5841e-01,  2.1021e-01,  8.5318e-01,  2.0081e-01,\n",
            "          7.8485e-01,  5.0163e-01, -9.6387e-02],\n",
            "        [ 1.5391e+00, -1.2189e+00,  1.6417e+00, -3.9728e-02, -1.8220e+00,\n",
            "          1.6802e+00,  2.3964e-02, -7.0385e-03],\n",
            "        [-2.7122e-01,  2.0346e-01, -1.3740e-01,  4.1318e-02, -1.0259e+00,\n",
            "          1.1729e+00, -1.2825e+00,  6.5957e-01],\n",
            "        [ 1.3692e+00,  1.2438e+00, -1.1156e-01, -5.2077e-01,  1.2428e+00,\n",
            "          1.3379e+00,  1.9374e-01, -5.6656e-01],\n",
            "        [-6.3548e-01,  5.8969e-01, -6.1207e-01, -1.7671e-01,  1.0826e-01,\n",
            "         -7.5458e-01,  1.5639e+00,  1.5391e+00],\n",
            "        [ 1.2803e-02, -4.2965e-01, -5.4234e-02, -9.9291e-01,  1.2286e+00,\n",
            "         -1.1540e+00, -9.0095e-01,  1.1829e+00],\n",
            "        [-9.7408e-02, -9.8988e-01,  1.3111e+00,  1.5540e+00, -6.0297e-01,\n",
            "          1.3766e+00, -4.5947e-01,  4.3821e-01],\n",
            "        [-2.3888e+00,  1.5832e-01, -8.0642e-01,  8.8134e-01, -1.0454e+00,\n",
            "          1.4086e+00,  1.1739e+00, -1.1232e+00],\n",
            "        [-3.7605e-01, -1.4470e+00, -4.1166e-01, -8.6447e-01, -9.4720e-01,\n",
            "         -2.8762e-01, -1.2449e+00, -6.3845e-01],\n",
            "        [-9.2063e-01, -8.5872e-01,  7.2845e-01,  1.9208e+00, -2.9164e-01,\n",
            "          1.3643e-01, -1.9479e+00, -1.3103e+00],\n",
            "        [-5.7018e-01,  1.4222e+00, -9.1682e-02, -1.6258e+00, -7.7105e-01,\n",
            "         -5.7279e-01, -4.8101e-01, -3.4380e-01],\n",
            "        [ 1.2695e+00,  8.4486e-01,  3.9847e-01, -2.6279e-01, -4.2296e-01,\n",
            "         -1.1717e+00,  7.9000e-01,  9.2814e-01],\n",
            "        [ 1.2673e+00, -3.1080e-01, -6.1957e-01, -1.6149e+00, -1.0762e+00,\n",
            "          1.9464e-01,  9.6664e-01,  6.0639e-01],\n",
            "        [ 1.3245e+00, -1.0192e+00, -9.5953e-01,  1.2699e+00, -5.8873e-01,\n",
            "         -1.3743e-01,  1.1006e+00, -7.0119e-01],\n",
            "        [-3.0104e-01, -1.7007e+00,  1.6879e+00, -3.4543e-01,  1.5311e+00,\n",
            "         -4.1219e-01, -1.6255e+00, -4.2369e-01],\n",
            "        [ 1.1915e+00, -9.7820e-02, -2.3357e+00, -8.9608e-01, -5.9673e-02,\n",
            "         -3.8363e-01,  8.8261e-01,  5.8147e-01],\n",
            "        [ 3.6756e-01, -9.4325e-01, -1.7473e+00, -3.7920e-01, -7.0431e-01,\n",
            "          1.5835e+00, -5.9936e-01, -7.8864e-03],\n",
            "        [ 3.8744e-01,  2.4500e-01,  1.9963e+00, -4.3694e-01, -1.0268e+00,\n",
            "         -2.0131e+00,  1.3468e+00,  2.4654e-01]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now evaluate this neural network's accuracy, including the embeddings and the learned dense layer.\n"
      ],
      "metadata": {
        "id": "GnkHJ17869yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    outputs = model(padded_reviews.long())\n",
        "    predictions = (outputs > 0.5).float().squeeze()\n",
        "    accuracy = (predictions == torch.tensor(labels)).float().mean().item()\n",
        "    loss_value = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float)).item()\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Log-loss: {loss_value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu2WQm5-7A3O",
        "outputId": "545dfaa3-f027-45b6-cf5e-cb027ecd7fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Log-loss: 0.47839298844337463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is great, but there could be overfitting. It would be good to use early stopping to not overfit for a more complex data set. However, the loss is not perfect. Even though the predicted probabilities indicated a correct prediction in every case, the program did not achieve absolute confidence in each correct answer. The lack of confidence was likely due to the small amount of noise (previously discussed) in the data set. Some words that appeared in both positive and negative reviews contributed to this lack of absolute certainty.\n"
      ],
      "metadata": {
        "id": "KldwEUrM7E7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "r6uhybFv7HuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **UNIVAC**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/2/2f/Univac_I_Census_dedication.jpg)\n",
        "\n",
        "**UNIVAC (Universal Automatic Computer)** was a line of electronic digital stored-program computers starting with the products of the Eckert–Mauchly Computer Corporation. Later the name was applied to a division of the Remington Rand company and successor organizations.\n",
        "\n",
        "The BINAC, built by the Eckert–Mauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986.\n",
        "\n",
        "### **History and structure**\n",
        "\n",
        "**UNIVAC Sperry Rand label**\n",
        "\n",
        "J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed Eckert–Mauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania. That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946.[1] UNIVAC was first intended for the Bureau of the Census, which paid for much of the development, and then was put in production.\n",
        "\n",
        "With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves, the retired army general who had previously managed building The Pentagon and led the Manhattan Project.\n",
        "\n",
        "The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson, whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest.\n",
        "\n",
        "The prediction led CBS's news boss in New York, Siegfried Mickelson, to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favour).\n",
        "\n",
        "When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total—Charles Collingwood, the on-air announcer, announced that they had failed to believe the earlier prediction.\n",
        "\n",
        "The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups.\n",
        "\n",
        "The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania. Remington Rand also had an engineering research lab in Norwalk, Connecticut, and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota. In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory.\n",
        "\n",
        "In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur, then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. Harry Franklin Vickers, then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. William Norris was put in charge as Vice-President and General Manager reporting to the President of the Remington Rand Division (of Sperry Rand)."
      ],
      "metadata": {
        "id": "RZrTH409Ps2P"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}