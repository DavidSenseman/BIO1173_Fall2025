{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/F25_Class_02_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 2: Neural Networks with Tensorflow and Keras**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* Part 2.1: Introduction to Neural Networks with Tensorflow and Keras\n",
        "* Part 2.2: Encoding Feature Vectors\n",
        "* Part 2.3: Early Stopping and Dropout to Prevent Overfitting\n",
        "* **Part 2.4: Saving and Loading a Keras Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504206a6-83f2-473c-f8df-4ef39b7d4361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is included as the last line in the output above."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions\n",
        "\n",
        "The cell below creates the function(s) needed for this lesson."
      ],
      "metadata": {
        "id": "Mu5xJAWl_9vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "# Simple function to change column name in a dataframe\n",
        "def rename_col_by_index(dataframe, index_mapping):\n",
        "    dataframe.columns = [index_mapping.get(i, col) for i, col in enumerate(dataframe.columns)]\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "tnwowmLuABDZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving and Loading a Keras Neural Network**\n",
        "\n",
        "Complex neural networks will take a _long_ time to fit/train.  It is helpful to be able to save a trained neural network so that you can reload it and using it again.  Again, a reloaded neural network will **not** require retraining.  \n",
        "\n",
        "Keras provides the following two formats for saving neural networks:\n",
        "\n",
        "* **JSON** - Stores the neural network structure (no weights) in the [JSON file format](https://en.wikipedia.org/wiki/JSON).\n",
        "* **Keras** - Stores the complete neural network (with weights) in the native Keras format.\n",
        "\n",
        "Usually, you will want to save in native Keras format."
      ],
      "metadata": {
        "id": "YPb6R6QD6h6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Early Stopping**\n",
        "\n",
        "We will now see an example of classification training with early stopping. We will train a neural network until the error no longer improves on the validation set."
      ],
      "metadata": {
        "id": "e8bdUJnW7Kfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Build and Train a Classification Neural Network\n",
        "\n",
        "The code in Example 1 builds and trains a neural network called `orModel` that can classify the `Quality` of an orange based on its physical and chemical characteristics.\n",
        "\n",
        "The code in the cell below reads the Orange Quality dataset from the course HTTP server and creates a DataFrame called `orDF` (i.e. \"orange\" DataFrame).\n",
        "\n",
        "In order to create a feature vector, the 3 non-numeric columns in the dataset: `Color`, `Variety` and `Blemished` must be pre-processed. Mapping strings to integers is used to take care of the column `Color` while One-Hot Encoding is used to take care the column `Variety`. To take care of the column `Blemish`, it will simply excluded (dropped) from the column list when generating the X-values.\n",
        "\n",
        "While most of the numerical values are small integers, the weight values are an order of magnitude larger, so they are standardized to their z-scores. This makes weight values closer in magnitude to the other numerical values and significantly improved the accuracy of the model. After the weights are stardardized, X-values are generated and stored in a Numpy array called `orX`.\n",
        "\n",
        "For this assigment we won't bother to split the data into Training/Validation sets, nor will we bother to shuffle the data.\n",
        "\n",
        "Since we are building a classification neural network, we will need to One-Hot Encode the column `Quality` which contains the Y-values. It should be noted that this column is already numeric, so we are **not** using One-Hot Encoding to replace string values with integer. Rather, One-Hot Encoding the Y-values is necessary to give them the **_correct format_** for the neural network.\n",
        "\n",
        "Again, because we want `orModel` to act as a _classifier_, not a \"regressor\", we will use the `softmax` activation function in the output layer.\n",
        "\n",
        "Finally, we will compile the model using `categorical_crossentropy` as the loss function instead of using `mean_squared_error`.   \n"
      ],
      "metadata": {
        "id": "EcUlAWeD7dEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Build and Train Classification Model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# Read dataset and create DataFrame --------------------------------\n",
        "orDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/orange_quality.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "\n",
        "# Create feature vector ---------------------------------------------\n",
        "\n",
        "# Map str to int\n",
        "mapping = {'Orange':0,'Deep Orange':1,\n",
        "           'Light Orange':2,'Orange-Red':3,\n",
        "           'Yellow-Orange':4}\n",
        "orDF['Color'] = orDF['Color'].map(mapping)\n",
        "\n",
        "# Standardize to z-scores\n",
        "orDF['Weight (g)'] = zscore(orDF['Weight (g)'])\n",
        "\n",
        "# Generate X-values\n",
        "orX = orDF[['Size (cm)', 'Weight (g)', 'Brix (Sweetness)', 'pH (Acidity)',\n",
        "       'Softness (1-5)', 'HarvestTime (days)', 'Ripeness (1-5)',\n",
        "        'Color']].values\n",
        "orX = np.asarray(orX).astype('float32')\n",
        "\n",
        "# Generate Y-values\n",
        "dummies = pd.get_dummies(orDF['Quality (1-5)'], dtype=int) # Classification\n",
        "orY = dummies.values\n",
        "orY = np.asarray(orY).astype('float32')\n",
        "\n",
        "# Build neural network-----------------------------------------\n",
        "orModel = Sequential()\n",
        "orModel.add(Input(shape=(orX.shape[1],)))  # Input\n",
        "orModel.add(Dense(50, activation='relu')) # Hidden 1\n",
        "orModel.add(Dense(25, activation='relu')) # Hidden 2\n",
        "orModel.add(Dense(orY.shape[1],activation='softmax')) # Output\n",
        "orModel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train model\n",
        "orModel.fit(orX,orY,verbose=2,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np7QHbJp7iq5",
        "outputId": "ddc48196-44c6-4e34-f7b7-91b713ba265f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 - 4s - 508ms/step - loss: 3.5602\n",
            "Epoch 2/100\n",
            "8/8 - 0s - 7ms/step - loss: 2.7233\n",
            "Epoch 3/100\n",
            "8/8 - 0s - 6ms/step - loss: 2.1594\n",
            "Epoch 4/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.8612\n",
            "Epoch 5/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.7471\n",
            "Epoch 6/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.6832\n",
            "Epoch 7/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.6344\n",
            "Epoch 8/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.6052\n",
            "Epoch 9/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.5773\n",
            "Epoch 10/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.5609\n",
            "Epoch 11/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.5462\n",
            "Epoch 12/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.5318\n",
            "Epoch 13/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.5140\n",
            "Epoch 14/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.5018\n",
            "Epoch 15/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.4912\n",
            "Epoch 16/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.4780\n",
            "Epoch 17/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.4734\n",
            "Epoch 18/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.4593\n",
            "Epoch 19/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.4503\n",
            "Epoch 20/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.4357\n",
            "Epoch 21/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.4329\n",
            "Epoch 22/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.4219\n",
            "Epoch 23/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.4199\n",
            "Epoch 24/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.4017\n",
            "Epoch 25/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3963\n",
            "Epoch 26/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3880\n",
            "Epoch 27/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3818\n",
            "Epoch 28/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3718\n",
            "Epoch 29/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3667\n",
            "Epoch 30/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3621\n",
            "Epoch 31/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3567\n",
            "Epoch 32/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3530\n",
            "Epoch 33/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3451\n",
            "Epoch 34/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3409\n",
            "Epoch 35/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3393\n",
            "Epoch 36/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3322\n",
            "Epoch 37/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3303\n",
            "Epoch 38/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3260\n",
            "Epoch 39/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3200\n",
            "Epoch 40/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3163\n",
            "Epoch 41/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3055\n",
            "Epoch 42/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.3091\n",
            "Epoch 43/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2980\n",
            "Epoch 44/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.3046\n",
            "Epoch 45/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2922\n",
            "Epoch 46/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2935\n",
            "Epoch 47/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2917\n",
            "Epoch 48/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2808\n",
            "Epoch 49/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2837\n",
            "Epoch 50/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2809\n",
            "Epoch 51/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2744\n",
            "Epoch 52/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2678\n",
            "Epoch 53/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2611\n",
            "Epoch 54/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2634\n",
            "Epoch 55/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2565\n",
            "Epoch 56/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2575\n",
            "Epoch 57/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2523\n",
            "Epoch 58/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2474\n",
            "Epoch 59/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2463\n",
            "Epoch 60/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2405\n",
            "Epoch 61/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2449\n",
            "Epoch 62/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2465\n",
            "Epoch 63/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2393\n",
            "Epoch 64/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2351\n",
            "Epoch 65/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2303\n",
            "Epoch 66/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2276\n",
            "Epoch 67/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2268\n",
            "Epoch 68/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2165\n",
            "Epoch 69/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2155\n",
            "Epoch 70/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2195\n",
            "Epoch 71/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2139\n",
            "Epoch 72/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2086\n",
            "Epoch 73/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.2112\n",
            "Epoch 74/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2058\n",
            "Epoch 75/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.2023\n",
            "Epoch 76/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1993\n",
            "Epoch 77/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1943\n",
            "Epoch 78/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.1898\n",
            "Epoch 79/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1923\n",
            "Epoch 80/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.1909\n",
            "Epoch 81/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.1814\n",
            "Epoch 82/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1796\n",
            "Epoch 83/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1799\n",
            "Epoch 84/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1735\n",
            "Epoch 85/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1734\n",
            "Epoch 86/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1701\n",
            "Epoch 87/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1709\n",
            "Epoch 88/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.1635\n",
            "Epoch 89/100\n",
            "8/8 - 0s - 6ms/step - loss: 1.1606\n",
            "Epoch 90/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1632\n",
            "Epoch 91/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1592\n",
            "Epoch 92/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1635\n",
            "Epoch 93/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1501\n",
            "Epoch 94/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1545\n",
            "Epoch 95/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1599\n",
            "Epoch 96/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1400\n",
            "Epoch 97/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1419\n",
            "Epoch 98/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1403\n",
            "Epoch 99/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1417\n",
            "Epoch 100/100\n",
            "8/8 - 0s - 7ms/step - loss: 1.1387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79ef39eeead0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training should go pretty fast since there are only 241 oranges in the dataset.\n",
        "\n",
        "If you code is correct, you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "Epoch 1/100\n",
        "8/8 - 2s - 274ms/step - loss: 4.0258\n",
        "Epoch 2/100\n",
        "8/8 - 0s - 7ms/step - loss: 3.0507\n",
        "Epoch 3/100\n",
        "8/8 - 0s - 8ms/step - loss: 2.3814\n",
        "Epoch 4/100\n",
        "8/8 - 0s - 7ms/step - loss: 1.9822\n",
        "Epoch 5/100\n",
        "8/8 - 0s - 5ms/step - loss: 1.8397\n",
        "\n",
        ".......................................\n",
        "\n",
        "Epoch 95/100\n",
        "8/8 - 0s - 8ms/step - loss: 1.1751\n",
        "Epoch 96/100\n",
        "8/8 - 0s - 7ms/step - loss: 1.1711\n",
        "Epoch 97/100\n",
        "8/8 - 0s - 7ms/step - loss: 1.1699\n",
        "Epoch 98/100\n",
        "8/8 - 0s - 5ms/step - loss: 1.1656\n",
        "Epoch 99/100\n",
        "8/8 - 0s - 7ms/step - loss: 1.1683\n",
        "Epoch 100/100\n",
        "8/8 - 0s - 5ms/step - loss: 1.1603\n",
        "<keras.src.callbacks.history.History at 0x7e7bb6d79060>\n",
        "\n",
        "~~~"
      ],
      "metadata": {
        "id": "SFVZDg3z8OrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Build and Train a Classification Neural Network**\n",
        "\n",
        "In the cell below build and train a new classification neural network called `apModel`.\n",
        "\n",
        "Start by reading the dataset and creating a DataFrame called `apDF` (\"apple\" DataFrame) using this code chunk:\n",
        "~~~text\n",
        "apDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "~~~\n",
        "The goal of your neural network model `apModel` will be to classify the apples in the Apple Quality dataset using the values in the following columns: 'Size', 'Weight', 'Sweetness', 'Crunchiness', 'Juiciness', 'Acidity' and 'Ripeness'. Since all of these columns are numeric, there is no need pre-process any of these columns. Moreover, the numerical values all have a similar magnitude so you don't need to standardize any column to their z-scores. Nor do you need to split the data into Training/Validation sets or suffle the data. When you generate your X-values, you should called them `apX`.\n",
        "\n",
        "Since you are building a classification neural network, you will need to One-Hot Encode the column containing the Y-values, `Quality`. This column is non-numeric, so by One-Hot Encoding it, you will accomplish two things: (1) replace string values with integers and (2) give the Y-values the correct format for the neural network. When you generate your Y-values, you should called them `apY`.\n",
        "\n",
        "Again, because you want your `apModel` to act as a _classifier_, use the `softmax` activation function in the output layer. You should also compile your model using `categorical_crossentropy` as the loss function.\n",
        "\n",
        "Finally, train (fit) your model on your X-values (`apX`) and your Y-values (`apY`) for 100 epochs.  "
      ],
      "metadata": {
        "id": "9Mrt8H0R81h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "# Example 1: Build and Train Classification Model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# Read dataset and create DataFrame --------------------------------\n",
        "apDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "\n",
        "# Create feature vector ---------------------------------------------\n",
        "\n",
        "#  'Size', 'Weight', 'Sweetness', 'Crunchiness', 'Juiciness', 'Acidity' and 'Ripeness'.\n",
        "\n",
        "# Generate list of columns for x\n",
        "apX_columns = apDF.columns.drop('Quality')  # `Quality` is y-value\n",
        "\n",
        "# Generate x-values as numpy array\n",
        "apX = apDF[apX_columns].values\n",
        "# Convert x-values to float 32\n",
        "apX = np.asarray(apX).astype('float32')\n",
        "\n",
        "# Generate Y-values\n",
        "dummies = pd.get_dummies(apDF['Quality'], dtype=int) # Classification\n",
        "apY = dummies.values\n",
        "apY = np.asarray(apY).astype('float32')\n",
        "\n",
        "# Build neural network-----------------------------------------\n",
        "apModel = Sequential()\n",
        "apModel.add(Input(shape=(apX.shape[1],)))  # Input\n",
        "apModel.add(Dense(50, activation='relu')) # Hidden 1\n",
        "apModel.add(Dense(25, activation='relu')) # Hidden 2\n",
        "apModel.add(Dense(apY.shape[1],activation='softmax')) # Output\n",
        "apModel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train model\n",
        "apModel.fit(apX,apY,verbose=2,epochs=100)"
      ],
      "metadata": {
        "id": "ADg2vo8j890E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66d1bc8-9074-4b96-a7b7-e85f79480a47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "125/125 - 2s - 18ms/step - loss: 7.3257\n",
            "Epoch 2/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.3642\n",
            "Epoch 3/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.5140\n",
            "Epoch 4/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.8358\n",
            "Epoch 5/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1544\n",
            "Epoch 6/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9893\n",
            "Epoch 7/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.3449\n",
            "Epoch 8/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8625\n",
            "Epoch 9/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.5724\n",
            "Epoch 10/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.7500\n",
            "Epoch 11/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1413\n",
            "Epoch 12/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1298\n",
            "Epoch 13/100\n",
            "125/125 - 0s - 2ms/step - loss: 2.0004\n",
            "Epoch 14/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0509\n",
            "Epoch 15/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8825\n",
            "Epoch 16/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1063\n",
            "Epoch 17/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9252\n",
            "Epoch 18/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1275\n",
            "Epoch 19/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1446\n",
            "Epoch 20/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0049\n",
            "Epoch 21/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1660\n",
            "Epoch 22/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9618\n",
            "Epoch 23/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0173\n",
            "Epoch 24/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7774\n",
            "Epoch 25/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9853\n",
            "Epoch 26/100\n",
            "125/125 - 0s - 2ms/step - loss: 2.1065\n",
            "Epoch 27/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0619\n",
            "Epoch 28/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9607\n",
            "Epoch 29/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1105\n",
            "Epoch 30/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0457\n",
            "Epoch 31/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6972\n",
            "Epoch 32/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1506\n",
            "Epoch 33/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9305\n",
            "Epoch 34/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0581\n",
            "Epoch 35/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1762\n",
            "Epoch 36/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9236\n",
            "Epoch 37/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.2294\n",
            "Epoch 38/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.1959\n",
            "Epoch 39/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8464\n",
            "Epoch 40/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.2878\n",
            "Epoch 41/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9028\n",
            "Epoch 42/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8457\n",
            "Epoch 43/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0631\n",
            "Epoch 44/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6546\n",
            "Epoch 45/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0958\n",
            "Epoch 46/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8342\n",
            "Epoch 47/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9666\n",
            "Epoch 48/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.3483\n",
            "Epoch 49/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8114\n",
            "Epoch 50/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8448\n",
            "Epoch 51/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7965\n",
            "Epoch 52/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7382\n",
            "Epoch 53/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9842\n",
            "Epoch 54/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7127\n",
            "Epoch 55/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9023\n",
            "Epoch 56/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9762\n",
            "Epoch 57/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6895\n",
            "Epoch 58/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0312\n",
            "Epoch 59/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6823\n",
            "Epoch 60/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7764\n",
            "Epoch 61/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6739\n",
            "Epoch 62/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7419\n",
            "Epoch 63/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7683\n",
            "Epoch 64/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6755\n",
            "Epoch 65/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7234\n",
            "Epoch 66/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8058\n",
            "Epoch 67/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9193\n",
            "Epoch 68/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8309\n",
            "Epoch 69/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8233\n",
            "Epoch 70/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.2634\n",
            "Epoch 71/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6616\n",
            "Epoch 72/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6275\n",
            "Epoch 73/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0436\n",
            "Epoch 74/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6538\n",
            "Epoch 75/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6614\n",
            "Epoch 76/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5514\n",
            "Epoch 77/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5500\n",
            "Epoch 78/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5230\n",
            "Epoch 79/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.8468\n",
            "Epoch 80/100\n",
            "125/125 - 0s - 2ms/step - loss: 1.0413\n",
            "Epoch 81/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6598\n",
            "Epoch 82/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6072\n",
            "Epoch 83/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6647\n",
            "Epoch 84/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7543\n",
            "Epoch 85/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6544\n",
            "Epoch 86/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5813\n",
            "Epoch 87/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5438\n",
            "Epoch 88/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7265\n",
            "Epoch 89/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5473\n",
            "Epoch 90/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5185\n",
            "Epoch 91/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6081\n",
            "Epoch 92/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7595\n",
            "Epoch 93/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7289\n",
            "Epoch 94/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.9603\n",
            "Epoch 95/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5122\n",
            "Epoch 96/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.4753\n",
            "Epoch 97/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6513\n",
            "Epoch 98/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.7925\n",
            "Epoch 99/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.6542\n",
            "Epoch 100/100\n",
            "125/125 - 0s - 2ms/step - loss: 0.5558\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79ef387443d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training your model will take longer since the Apple Quality dataset contains 4,000 items instead of only 241 in the Orange Quality dataset.\n",
        "\n",
        "If your code was correct your should see something similiar to the following output:\n",
        "~~~text\n",
        "Epoch 1/100\n",
        "125/125 - 1s - loss: 0.4866 - 746ms/epoch - 6ms/step\n",
        "Epoch 2/100\n",
        "125/125 - 0s - loss: 0.3555 - 440ms/epoch - 4ms/step\n",
        "Epoch 3/100\n",
        "125/125 - 0s - loss: 0.3111 - 444ms/epoch - 4ms/step\n",
        "Epoch 4/100\n",
        "125/125 - 0s - loss: 0.2861 - 435ms/epoch - 3ms/step\n",
        "Epoch 5/100\n",
        "125/125 - 0s - loss: 0.2686 - 497ms/epoch - 4ms/step\n",
        "\n",
        "............................................\n",
        "\n",
        "Epoch 95/100\n",
        "125/125 - 0s - loss: 0.0764 - 429ms/epoch - 3ms/step\n",
        "Epoch 96/100\n",
        "125/125 - 0s - loss: 0.0734 - 429ms/epoch - 3ms/step\n",
        "Epoch 97/100\n",
        "125/125 - 0s - loss: 0.0741 - 447ms/epoch - 4ms/step\n",
        "Epoch 98/100\n",
        "125/125 - 1s - loss: 0.0732 - 507ms/epoch - 4ms/step\n",
        "Epoch 99/100\n",
        "125/125 - 0s - loss: 0.0723 - 418ms/epoch - 3ms/step\n",
        "Epoch 100/100\n",
        "125/125 - 0s - loss: 0.0719 - 416ms/epoch - 3ms/s\n",
        "~~~\n",
        "\n",
        "Notice that in this example, the loss went from `0.4866` after the 1st epoch down to `0.0719` after the 100th epoch."
      ],
      "metadata": {
        "id": "Ayg-LmKm9FYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Determine the Model's RSME and Accuracy\n",
        "\n",
        "The overall objective of this assignment is to convince you that can save a _trained_ neural network to a file, and then later, recreate the neural network from the file, **_without changing the model's accuracy_**.\n",
        "\n",
        "#### Why is this important?\n",
        "\n",
        "As you already know, it can take significant time and processing power to train even relatively small neural networks that we created so far in this course. Neural networks that are used commercially (think \"Siri\" or \"Alexa\" or ChatGPT) are many times larger and require enormous resources as well as weeks (or months) to train. Obviously, if you had to train a neural network every time you wanted to use it, it won't be very practical and there would be little interest in \"AI\". However, once the neural network has been trained, you can save it to a file, and then re-use it over and over again, without any loss in the neural network's ability to solve problems (i.e. loss in accuracy).      \n",
        "\n",
        "The code in the cell below calculates ability of the `orModel` neural network to predict an orange's quality (Y-value) based on its physical and chemical characteristics (X-values). Two measures of predictive ability are computed, the **_Root Mean Square Error (RMSE)_** and **_Accuracy_**. The code stores the RSME value in the variable `orScore` and the Accuracy value in the variable `orCorrect` and then prints out thes values.\n"
      ],
      "metadata": {
        "id": "yQn3GXQe9OnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Determine the model's RMSE and Accuracy\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Measure RMSE error.\n",
        "orPred = orModel.predict(orX)\n",
        "orScore = np.sqrt(metrics.mean_squared_error(orPred,orY))\n",
        "print(f\"Before save score (RMSE): {orScore}\")\n",
        "\n",
        "# Measure the accuracy\n",
        "orPredict_classes = np.argmax(orPred,axis=1)\n",
        "orExpected_classes = np.argmax(orY,axis=1)\n",
        "orCorrect = accuracy_score(orExpected_classes,orPredict_classes)\n",
        "print(f\"Before save Accuracy: {orCorrect}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1OGthZ09THz",
        "outputId": "70a27221-b41f-4bc4-dd73-b11c6d5d6e6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
            "Before save score (RMSE): 0.2628870834860242\n",
            "Before save Accuracy: 0.5767634854771784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something similar to the following output:\n",
        "~~~text\n",
        "8/8 [==============================] - 0s 3ms/step\n",
        "Before save score (RMSE): 0.26590439677238464\n",
        "Before save Accuracy: 0.5518672199170125\n",
        "~~~\n",
        "\n",
        "The `orModel` isn't doing that great ( Accuracy: 0.5518672199170125 or 55% accurate). A model with only 55% accuracy is about a accurate as flipping a coin, which is 50% accurate.\n",
        "\n",
        "However, as mentioned above, the number of items (oranges) in this dataset is relatively small (_n_=241). As you can see, this is really too few samples for training a neural network and expect to acheive a high level of accuracy."
      ],
      "metadata": {
        "id": "6TkU0X_b9WqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Determine the Model's RSME and Accuracy**\n",
        "\n",
        "In the cell below, determine the RSME and Accuracy of your `apModel`. Store the RSME value in a variable called `apScore` and the Accuracy value in a variable called `apCorrect`. Print out these values as shown in Example 2 above."
      ],
      "metadata": {
        "id": "wP7Yge269bWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Measure RMSE error.\n",
        "apPred = apModel.predict(apX)\n",
        "apScore = np.sqrt(metrics.mean_squared_error(apPred,apY))\n",
        "print(f\"Before save score (RMSE): {apScore}\")\n",
        "\n",
        "# Measure the accuracy\n",
        "apPredict_classes = np.argmax(apPred,axis=1)\n",
        "apExpected_classes = np.argmax(apY,axis=1)\n",
        "apCorrect = accuracy_score(apExpected_classes,apPredict_classes)\n",
        "print(f\"Before save Accuracy: {apCorrect}\")"
      ],
      "metadata": {
        "id": "fTWlNJzA9fti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d85bf49-2b26-41f9-f5ed-4288eb4ec33e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Before save score (RMSE): 0.393977125450243\n",
            "Before save Accuracy: 0.7795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something similar to the following output:\n",
        "~~~text\n",
        "125/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\n",
        "Before save score (RMSE): 0.393977125450243\n",
        "Before save Accuracy: 0.7795\n",
        "~~~\n",
        "\n",
        "According to the output above, your `apModel` is about 75% accurate when it comes to predicting apple quality. Apparently, it's a little easier to predict an apple's `Quality` with a classification neural network than to predict orange quality."
      ],
      "metadata": {
        "id": "Om3ueI6p9idK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below sets up a neural network and reads the data (for predictions), but it does not clear the model directory or fit the neural network. The code loads the weights from the previous fit. Now we reload the network and perform another prediction. The RMSE should match the previous one exactly if we saved and reloaded the neural network correctly."
      ],
      "metadata": {
        "id": "EbceU8VkadES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Save the Model\n",
        "\n",
        "The code in the cell below saves the _trained_ neural network `orModel` as a file in two different file formats: `JSON` and `keras`.\n",
        "\n",
        "The code saves each file in the current working directory (`save_path = \".\"`). The filename of the JSON file is `orModel.json` while the filename of the HDF5 file is `orModel.h5`."
      ],
      "metadata": {
        "id": "7NVtEsrt-xd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Save the model\n",
        "\n",
        "from keras.models import save_model\n",
        "import os\n",
        "\n",
        "# Save path is the current directory\n",
        "save_path = \".\"\n",
        "\n",
        "# Save neural network structure to JSON (no weights)\n",
        "orModel_json = orModel.to_json()\n",
        "with open(os.path.join(save_path,\"orModel.json\"), \"w\") as json_file:\n",
        "    json_file.write(orModel_json)\n",
        "\n",
        "# Save the model in the native Keras format\n",
        "orModel.save('orModel.keras')\n",
        "\n",
        "# Print out the files in current directory\n",
        "files = os.listdir()\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgkJhlPB_Wl7",
        "outputId": "20990a94-7c41-4ad5-830d-c519481b4f75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'orModel.json', 'orModel.keras', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something like the following output:\n",
        "\n",
        "~~~text\n",
        "['.config', drive, 'orModel.json', 'orModel.keras', 'sample_data']\n",
        "~~~\n",
        "\n",
        "After running the code cell above, there should now be two new files in your `Class_02_4` folder called `orModel.jason` and `orModel.keras`."
      ],
      "metadata": {
        "id": "uKVJKqIo_fhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Save the Model**\n",
        "\n",
        "In the code cell below save your _trained_ neural network `apModel` as a JSON file with the filename, `apModel.json`, and as a native Keras file with the filenmane `apModel.keras`. Save both files to your current working directory (`save_path = \".\"`)."
      ],
      "metadata": {
        "id": "OcoPhtQi_kiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "from keras.models import save_model\n",
        "import os\n",
        "\n",
        "# Save path is the current directory\n",
        "save_path = \".\"\n",
        "\n",
        "# Save neural network structure to JSON (no weights)\n",
        "apModel_json = apModel.to_json()\n",
        "with open(os.path.join(save_path,\"apModel.json\"), \"w\") as json_file:\n",
        "    json_file.write(apModel_json)\n",
        "\n",
        "# Save the model in the native Keras format\n",
        "apModel.save('apModel.keras')\n",
        "\n",
        "# Print out the files in current directory\n",
        "files = os.listdir()\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbjbyLCGClTn",
        "outputId": "937c72ba-2c42-492f-cc38-99a8ed0890f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'apModel.keras', 'orModel.json', 'orModel.keras', 'apModel.json', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something similar, but not necessarily identicalm to the following output:\n",
        "\n",
        "~~~text\n",
        "['.config', drive, 'apModel.json', 'apModel.keras', 'orModel.keras', 'orModel.json', 'sample_data']\n",
        "~~~\n",
        "\n",
        "You should now see the two more files with your neural network."
      ],
      "metadata": {
        "id": "rl_-kxWhbuhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of the JSON format is that it can be visually inspected -- just click on the file name in the file browser panel. The JSON file perserves the model's _architecture_ which you can see by looking at the JSON file, but if you want to use it, you will need to train all over again.\n",
        "\n",
        "On the other hand, you can't view the contents of the HDF5 file, since it is not UTF-8 encoded (it's formated). Neverthelss, you should always save your model in the HDF5 format since this **_preserves architecture and the values of the weights_** of the model's connections. By preserving these values you don't have to waste time retraining the model again."
      ],
      "metadata": {
        "id": "v3FMuLDNcExU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Create New Model from Saved Model\n",
        "\n",
        "Once a trained model has been saved in the HDF5 format, it is a simple matter to read the file to make an exact copy of the model using the Keras function `load_model()` as shown in the cell below. In Example 4 we have given the re-loaded neural network the name `or2Model` to differentiate it from the one that we built previously.  "
      ],
      "metadata": {
        "id": "Qk2Sx-0_cHBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Create new model from saved model\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "# Look in current folder\n",
        "save_path = \".\"\n",
        "\n",
        "# Create model2 from the saved model\n",
        "or2Model = load_model(os.path.join(save_path,\"orModel.keras\"))\n",
        "\n",
        "# Print out model summary\n",
        "or2Model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "F_cMX2v1_qIq",
        "outputId": "92eccec2-8b53-4a73-bc02-135adc20c7d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │             \u001b[38;5;34m450\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)                  │           \u001b[38;5;34m1,275\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m208\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">450</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,801\u001b[0m (22.66 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,801</span> (22.66 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,933\u001b[0m (7.55 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,933</span> (7.55 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,868\u001b[0m (15.11 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,868</span> (15.11 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "Model: \"sequential_1\"\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
        "│ dense_3 (Dense)                      │ (None, 50)                  │             450 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_4 (Dense)                      │ (None, 25)                  │           1,275 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_5 (Dense)                      │ (None, 8)                   │             208 │\n",
        "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
        " Total params: 5,801 (22.66 KB)\n",
        " Trainable params: 1,933 (7.55 KB)\n",
        " Non-trainable params: 0 (0.00 B)\n",
        " Optimizer params: 3,868 (15.11 KB)\n",
        "___________________________\n",
        "~~~\n"
      ],
      "metadata": {
        "id": "-1pWvfA5cVcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Create New Model from Saved Model**\n",
        "\n",
        "In the cell below create a new neural network called `ap2Model` from the file `apModel.keras` in your current directory. Print out a summary of your new `ap2Model`."
      ],
      "metadata": {
        "id": "PhDXUkydD64K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "# Look in current folder\n",
        "save_path = \".\"\n",
        "\n",
        "# Create model2 from the saved model\n",
        "ap2Model = load_model(os.path.join(save_path,\"apModel.keras\"))\n",
        "\n",
        "# Print out model summary\n",
        "ap2Model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "igrzYQ6lECiS",
        "outputId": "e0fa84bd-30d1-4b29-e525-c5b3584fcf4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │             \u001b[38;5;34m450\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)                  │           \u001b[38;5;34m1,275\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m52\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">450</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,333\u001b[0m (20.84 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,333</span> (20.84 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,777\u001b[0m (6.94 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,777</span> (6.94 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,556\u001b[0m (13.89 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,556</span> (13.89 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " dense (Dense)               (None, 50)                400       \n",
        "                                                                 \n",
        " dense_1 (Dense)             (None, 25)                1275      \n",
        "                                                                 \n",
        " dense_2 (Dense)             (None, 2)                 52        \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 1,727\n",
        "Trainable params: 1,727\n",
        "Non-trainable params: 0\n",
        "~~~\n"
      ],
      "metadata": {
        "id": "Wq-QSomoEMuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Compare the Predictive Accuracy of the Old and New Models\n",
        "\n",
        "The code in the cell below computes the RMSE error and the Accuracy of our new model `or2Model` and compares these values with the original `orModel`. We are trying to address the question whether re-loaded model has the same accuracy as the original model?"
      ],
      "metadata": {
        "id": "9CuvngfiEUgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Determine new model's RMSE and Accuracy\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Measure RMSE error.\n",
        "or2Pred = or2Model.predict(orX)\n",
        "or2Score = np.sqrt(metrics.mean_squared_error(or2Pred,orY))\n",
        "print(f\"Before save score (RMSE): {orScore}\")\n",
        "print(f\"After save score (RMSE) : {or2Score}\")\n",
        "\n",
        "# Measure the accuracy\n",
        "or2Predict_classes = np.argmax(or2Pred,axis=1)\n",
        "or2Expected_classes = np.argmax(orY,axis=1)\n",
        "or2Correct = accuracy_score(or2Expected_classes,or2Predict_classes)\n",
        "print(f\"Before save Accuracy: {orCorrect}\")\n",
        "print(f\"After save Accuracy : {or2Correct}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DYJ9n67dB0J",
        "outputId": "6a861737-3dc3-4aa6-e8f5-c5003cf44791"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
            "Before save score (RMSE): 0.2628870834860242\n",
            "After save score (RMSE) : 0.2628870834860242\n",
            "Before save Accuracy: 0.5767634854771784\n",
            "After save Accuracy : 0.5767634854771784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "8/8 [==============================] - 0s 4ms/step\n",
        "Before save score (RMSE): 0.26590439677238464\n",
        "After save score (RMSE) : 0.26590439677238464\n",
        "Before save Accuracy: 0.5518672199170125\n",
        "After save Accuracy : 0.5518672199170125\n",
        "~~~~\n",
        "\n",
        "As you can see, there is **_no difference_** in the accuracy of the saved model compared to the original one. Train Once...Use Anywhere!\n",
        "\n",
        "Big generative models like `ChatGTP` can take days or even months to train. But once they are trained and saved, they can process new data very fast."
      ],
      "metadata": {
        "id": "XHbiXkwldEUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Compare the Predictive Accuracy of the Old and New Models**\n",
        "\n",
        "In the cell below write the code to compute the RMSE and Accuracy values for your `ap2Model` and print out these values along with the values for your original `apModel`."
      ],
      "metadata": {
        "id": "axjJIaLgEiPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Measure RMSE error.\n",
        "ap2Pred = ap2Model.predict(apX)\n",
        "ap2Score = np.sqrt(metrics.mean_squared_error(ap2Pred,apY))\n",
        "print(f\"Before save score (RMSE): {apScore}\")\n",
        "print(f\"After save score (RMSE) : {ap2Score}\")\n",
        "\n",
        "# Measure the accuracy\n",
        "ap2Predict_classes = np.argmax(ap2Pred,axis=1)\n",
        "ap2Expected_classes = np.argmax(apY,axis=1)\n",
        "ap2Correct = accuracy_score(ap2Expected_classes,ap2Predict_classes)\n",
        "print(f\"Before save Accuracy: {apCorrect}\")\n",
        "print(f\"After save Accuracy : {ap2Correct}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUcDuK89EnKf",
        "outputId": "5b8278b3-1b2b-4878-c25c-bba535c22d5e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Before save score (RMSE): 0.393977125450243\n",
            "After save score (RMSE) : 0.393977125450243\n",
            "Before save Accuracy: 0.7795\n",
            "After save Accuracy : 0.7795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should something similar to the following output:\n",
        "~~~text\n",
        "125/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\n",
        "Before save score (RMSE): 0.393977125450243\n",
        "After save score (RMSE) : 0.393977125450243\n",
        "Before save Accuracy: 0.7795\n",
        "After save Accuracy : 0.7795\n",
        "~~~"
      ],
      "metadata": {
        "id": "_AFMQlXOEvF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Again, there is no loss in accuracy using a trained neural network that has been saved and then recreated from the saved `keras` file."
      ],
      "metadata": {
        "id": "GLXXPfCSExKP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhC_-6ebE3l"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_02_4.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "\n",
        "## **Stable Diffusion**\n",
        "\n",
        "\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Astronaut_Riding_a_Horse_%28SD3.5%29.webp/1024px-Astronaut_Riding_a_Horse_%28SD3.5%29.webp.png)\n",
        "\n",
        ">*An image generated with Stable Diffusion 3.5 based on the text prompt*\n",
        "\n",
        "TStable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. The generative artificial intelligence technology is the premier product of Stability AI and is considered to be a part of the ongoing artificial intelligence boom.\n",
        "\n",
        "It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.[3] Its development involved researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a computational donation from Stability and training data from non-profit organizations.[4][5][6][7]\n",
        "\n",
        "**Stable Diffusion** is a latent diffusion model, a kind of deep generative artificial neural network. Its code and model weights have been released publicly, and it can run on most consumer hardware equipped with a modest GPU with at least 4 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services.\n",
        "\n",
        "**Development**\n",
        "\n",
        "Stable Diffusion originated from a project called Latent Diffusion, developed in Germany by researchers at Ludwig Maximilian University in Munich and Heidelberg University. Four of the original 5 authors (Robin Rombach, Andreas Blattmann, Patrick Esser and Dominik Lorenz) later joined Stability AI and released subsequent versions of Stable Diffusion.\n",
        "\n",
        "The technical license for the model was released by the CompVis group at Ludwig Maximilian University of Munich. Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion. Stability AI also credited EleutherAI and LAION (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.\n",
        "\n",
        "**Technology**\n",
        "\n",
        "The denoising process used by Stable Diffusion. The model generates images by iteratively denoising random noise until a configured number of steps have been reached, guided by the CLIP text encoder pretrained on concepts along with the attention mechanism, resulting in the desired image depicting a representation of the trained concept.\n",
        "\n",
        "**Architecture**\n",
        "\n",
        "Diffusion models, introduced in 2015, are trained with the objective of removing successive applications of Gaussian noise on training images, which can be thought of as a sequence of denoising autoencoders. The name diffusion is from the thermodynamic diffusion, since they were first developed with inspiration from thermodynamics.\n",
        "\n",
        "Models in Stable Diffusion series before SD 3 all used a variant of diffusion models, called latent diffusion model (LDM), developed in 2021 by the CompVis (Computer Vision & Learning) group at LMU Munich.\n",
        "\n",
        "Stable Diffusion consists of 3 parts: the variational autoencoder (VAE), U-Net, and an optional text encoder. The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image. Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion. The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.\n",
        "\n",
        "The denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism. For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space. Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.\n",
        "\n",
        "With 860 million parameters in the U-Net and 123 million in the text encoder, Stable Diffusion is considered relatively lightweight by 2022 standards, and unlike other diffusion models, it can run on consumer GPUs, and even CPU-only if using the OpenVINO version of Stable Diffusion.\n",
        "\n",
        "**SD XL**\n",
        "The XL version uses the same LDM architecture as previous versions, except larger: larger UNet backbone, larger cross-attention context, two text encoders instead of one, and trained on multiple aspect ratios (not just the square aspect ratio like previous versions).\n",
        "\n",
        "The SD XL Refiner, released at the same time, has the same architecture as SD XL, but it was trained for adding fine details to preexisting images via text-conditional img2img.\n",
        "\n",
        "**SD 3.0**\n",
        "\n",
        "Main article: Diffusion model § Rectified flow\n",
        "The 3.0 version completely changes the backbone. Not a UNet, but a Rectified Flow Transformer, which implements the rectified flow method with a Transformer.\n",
        "\n",
        "The Transformer architecture used for SD 3.0 has three \"tracks\", for original text encoding, transformed text encoding, and image encoding (in latent space). The transformed text encoding and image encoding are mixed during each transformer block.\n",
        "\n",
        "The architecture is named \"multimodal diffusion transformer (MMDiT), where the \"multimodal\" means that it mixes text and image encodings inside its operations. This differs from previous versions of DiT, where the text encoding affects the image encoding, but not vice versa.\n",
        "\n",
        "**Training data**\n",
        "\n",
        "Stable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available dataset derived from Common Crawl data scraped from the web, where 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution, a predicted likelihood of containing a watermark, and predicted \"aesthetic\" score (e.g. subjective visual quality). The dataset was created by LAION, a German non-profit which receives funding from Stability AI. The Stable Diffusion model was trained on three subsets of LAION-5B: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+. A third-party analysis of the model's training data identified that out of a smaller subset of 12 million images taken from the original wider dataset used, approximately 47% of the sample size of images came from 100 different domains, with Pinterest taking up 8.5% of the subset, followed by websites such as WordPress, Blogspot, Flickr, DeviantArt and Wikimedia Commons. An investigation by Bayerischer Rundfunk showed that LAION's datasets, hosted on Hugging Face, contain large amounts of private and sensitive data.\n",
        "\n",
        "**Training procedures**\n",
        "\n",
        "The model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600 million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them. The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a watermark with greater than 80% probability. Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance.\n",
        "\n",
        "The model was trained using 256 Nvidia A100 GPUs on Amazon Web Services for a total of 150,000 GPU-hours, at a cost of $600,000.\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/f/f6/Stable_Diffusion_architecture.png)\n",
        "\n",
        ">Diagram of the latent diffusion architecture used by Stable Diffusion\n",
        "\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "Stable Diffusion has issues with degradation and inaccuracies in certain scenarios. Initial releases of the model were trained on a dataset that consists of 512×512 resolution images, meaning that the quality of generated images noticeably degrades when user specifications deviate from its \"expected\" 512×512 resolution; the version 2.0 update of the Stable Diffusion model later introduced the ability to natively generate images at 768×768 resolution. Another challenge is in generating human limbs due to poor data quality of limbs in the LAION database. The model is insufficiently trained to replicate human limbs and faces due to the lack of representative features in the database, and prompting the model to generate images of such type can confound the model. Stable Diffusion XL (SDXL) version 1.0, released in July 2023, introduced native 1024x1024 resolution and improved generation for limbs and text.\n",
        "\n",
        "Accessibility for individual developers can also be a problem. In order to customize the model for new use cases that are not included in the dataset, such as generating anime characters (\"waifu diffusion\"), new data and further training are required. Fine-tuned adaptations of Stable Diffusion created through additional retraining have been used for a variety of different use-cases, from medical imaging to algorithmically generated music. However, this fine-tuning process is sensitive to the quality of new data; low resolution images or different resolutions from the original data can not only fail to learn the new task but degrade the overall performance of the model. Even when the model is additionally trained on high quality images, it is difficult for individuals to run models in consumer electronics. For example, the training process for waifu-diffusion requires a minimum 30 GB of VRAM, which exceeds the usual resource provided in such consumer GPUs as Nvidia's GeForce 30 series, which has only about 12 GB.\n",
        "\n",
        "The creators of Stable Diffusion acknowledge the potential for algorithmic bias, as the model was primarily trained on images with English descriptions. As a result, generated images reinforce social biases and are from a western perspective, as the creators note that the model lacks data from other communities and cultures. The model gives more accurate results for prompts that are written in English in comparison to those written in other languages, with western or white cultures often being the default representation.\n",
        "\n",
        "**End-user fine-tuning**\n",
        "\n",
        "To address the limitations of the model's initial training, end-users may opt to implement additional training to fine-tune generation outputs to match more specific use-cases, a process also referred to as personalization. There are three methods in which user-accessible fine-tuning can be applied to a Stable Diffusion model checkpoint:\n",
        "\n",
        "An \"embedding\" can be trained from a collection of user-provided images, and allows the model to generate visually similar images whenever the name of the embedding is used within a generation prompt.[44] Embeddings are based on the \"textual inversion\" concept developed by researchers from Tel Aviv University in 2022 with support from Nvidia, where vector representations for specific tokens used by the model's text encoder are linked to new pseudo-words. Embeddings can be used to reduce biases within the original model, or mimic visual styles.\n",
        "A \"hypernetwork\" is a small pretrained neural network that is applied to various points within a larger neural network, and refers to the technique created by NovelAI developer Kurumuz in 2021, originally intended for text-generation transformer models. Hypernetworks steer results towards a particular direction, allowing Stable Diffusion-based models to imitate the art style of specific artists, even if the artist is not recognised by the original model; they process the image by finding key areas of importance such as hair and eyes, and then patch these areas in secondary latent space.\n",
        "DreamBooth is a deep learning generation model developed by researchers from Google Research and Boston University in 2022 which can fine-tune the model to generate precise, personalised outputs that depict a specific subject, following training via a set of images which depict the subject."
      ],
      "metadata": {
        "id": "kiYJyOq4HARA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYdsR4-MH9uu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}