{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/F25_Class_04_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Transformers and Accessing ChatGTP\n",
        "* Part 4.2: LLM Memory and Embedding\n",
        "* **Part 4.3: Generative AI and Generating Faces with StyleGAN3**\n",
        "* Part 4.4: Text to Images with Stable Diffusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is included as the last line in the output above."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Custom Functions\n",
        "\n",
        "Run the cell below to load custom functions used in this lesson."
      ],
      "metadata": {
        "id": "dfExM5hY3fAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "37ShL3h23sK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Generative AI**\n",
        "\n",
        "Generative AI is a fascinating and rapidly evolving subfield of artificial intelligence that focuses on creating new content, from art, music, and stories to more technical applications such as generating code or predicting complex systems. This is in contrast to other types of AI that are more concerned with analyzing and interpreting existing data, such as classifying images or predicting stock prices. While traditional AI systems are built to respond with the most accurate or optimal answer from a set of possibilities, generative AI systems are designed to create new possibilities, generating new data that is similar in some way to the data they were trained on.\n",
        "\n",
        "For example, a generative AI system might be trained on a large dataset of classical music and then be able to create new compositions in the style of Bach or Beethoven. Another application could be training the model on a vast amount of text data to create a chatbot that can generate human-like responses to questions or prompts, as we've seen with OpenAI's GPT models. Other examples include generating realistic images, creating virtual environments, or even designing molecules for new drugs.\n",
        "\n",
        "This remarkable capability of generative AI opens up a world of possibilities and applications that are only beginning to be explored. In this section, we will delve deeper into the workings of generative AI, examining the technologies that power it, such as Generative Adversarial Networks (GANs) and Large Language Models (LLMs), and explore some of the most exciting applications and future directions of this innovative field.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation."
      ],
      "metadata": {
        "id": "ZJrIgZkszA7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Beginnings of Generative AI\n",
        "\n",
        "The history of generative AI can be traced back to the early days of artificial intelligence itself. In the 1950s and 60s, researchers were already experimenting with algorithms that could generate new content, such as the ELIZA chatbot developed at MIT in the 1966, which could generate responses to user input in a manner that mimicked a psychotherapist. However, the true potential of generative AI remained largely untapped until the advent of more powerful computing hardware and larger datasets in the 21st century. While ELIZA seemed advanced for the time; its conversations lacked any significant depth:\n",
        "\n",
        "```\n",
        "> Hello, I am Eliza.\n",
        "* I am afraid I will never learn Python.\n",
        "> Do you believe it is normal to be afraid you will never learn Python?\n",
        "* Yes\n",
        "> I see.\n",
        "```\n",
        "\n",
        "In 2006, Geoffrey Hinton and his colleagues at the University of Toronto developed a type of neural network called a Deep Belief Network (DBN), which could be trained to generate new data that was similar to the data it was trained on. This marked a key milestone in the development of generative AI, as it demonstrated that neural networks could be used to not only classify and analyze data, but also to create new data. Initially, this DBN was trained to recognize MNIST digits, such as seen in Figure 7.MNIST.\n",
        "\n",
        "**Figure 7.MNIST: Sample of the MNIST Digits**\n",
        "\n",
        "![MNIST Digits](https://data.heatonresearch.com/images/wustl/class/class_8_mnist.png)\n",
        "\n",
        "The next major breakthrough came in 2014, with the introduction of Generative Adversarial Networks (GANs) by Ian Goodfellow and his colleagues. GANs consist of two neural networks, a generator and a discriminator, which are trained simultaneously through a kind of game. The generator tries to create data that is indistinguishable from real data, while the discriminator tries to tell if the data is real or fake. This adversarial training process results in the generator becoming increasingly adept at creating realistic data, and has been used to generate everything from realistic images of faces that do not exist to new video game levels.\n",
        "\n",
        "Figure 7.GAN-MNIST shows a GAN that generates new digits based on MNIST digits that it was trained on.\n",
        "\n",
        "**Figure 7.GAN-MNIST: Generated Digits**\n",
        "\n",
        "![MNIST Digits](https://data.heatonresearch.com/images/wustl/class/gan_digits.jpg)\n",
        "\n",
        "The same early technique was applied to create entirely new faces, as demonstrated by Figure 7.\n",
        "\n",
        "**Figure 7.GAN-FACE: Generated Faces**\n",
        "\n",
        "![Early GAN Faces](https://s3.amazonaws.com/data.heatonresearch.com/images/wustl/class/gan_face.jpg)\n",
        "\n",
        "NVIDIA took GAN generated images to an entirely new level with StyleGAN, which could generate photo-realistic faces as seen in figure 7.GAN-STYLE.\n",
        "\n",
        "**Figure 7.GAN-STYLE: Generated Faces**\n",
        "\n",
        "![StyleGAN](https://data.heatonresearch.com/images/wustl/class/stylegan2-hires.jpg)\n",
        "\n",
        "Stable Diffusion is a recent development in the field of generative AI that has garnered significant attention. Traditional methods like GANs and VAEs have their drawbacks, such as mode collapse in GANs. Stable Diffusion, on the other hand, uses a different approach by adopting a diffusion-based probabilistic model. This approach involves transforming the data in a way that spreads or 'diffuses' it out, and then running the process in reverse to generate new data. The model is trained by predicting the next diffusion step in the reverse process from a given state, which allows it to learn a detailed and high-quality representation of the data. This results in the generation of more realistic and detailed images compared to other methods. Moreover, Stable Diffusion models have been found to be more stable and easier to train compared to GANs, making them a promising alternative for generating high-quality content in various applications.\n",
        "\n",
        "With stable diffusion you can use prompts to render the image, giving you great control. Figure 7.GAN-STYLE shows the same woman in multiple poses and settings.\n",
        "\n",
        "**Figure 7.GAN-STYLE: Same Person in Stable Diffusion**\n",
        "\n",
        "![Stable Diffusion](https://data.heatonresearch.com/images/wustl/class/stable-diff.jpg)\n",
        "\n",
        "\n",
        "\n",
        "In recent years, the development of large language models (LLMs) such as OpenAI's GPT (Generative Pre-trained Transformer) series, has brought generative AI to the forefront of public attention. These models are trained on vast amounts of text data and can generate human-like text on a wide variety of topics. This has led to a plethora of applications, from chatbots and virtual assistants to creative writing and content generation.\n",
        "\n",
        "The development of generative AI is still ongoing, and there are many challenges to be addressed and exciting avenues to explore. However, the progress that has been made in recent years is nothing short of astounding, and has opened up a world of possibilities that were once the stuff of science fiction.\n",
        "|"
      ],
      "metadata": {
        "id": "IxYJN58VfcSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Introduction to GANS for Image and Data Generation\n",
        "\n",
        "A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow in 2014. [[Cite:goodfellow2014generative]](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) Two neural networks compete with each other in a game. The GAN training algorithm starts with a training set and learns to generate new data with the same distributions as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.\n",
        "\n",
        "Running this notebook in this notebook in Google CoLab is the most straightforward means of completing this chapter. Because of this, I designed this notebook to run in Google CoLab. It will take some modifications if you wish to run it locally.\n",
        "\n",
        "This original StyleGAN paper used neural networks to automatically generate images for several previously seen datasets: MINST and CIFAR. However, it also included the Toronto Face Dataset (a private dataset used by some researchers). You can see some of these images in Figure 7.GANS.\n",
        "\n",
        "**Figure 7.GANS: GAN Generated Images**\n",
        "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-2.png \"GAN Generated Images\")\n",
        "\n",
        "Only sub-figure D made use of convolutional neural networks. Figures A-C make use of fully connected neural networks. As we will see in this module, the researchers significantly increased the role of convolutional neural networks for GANs.\n",
        "\n",
        "We call a GAN a generative model because it generates new data. You can see the overall process in Figure 7.GAN-FLOW.\n",
        "\n",
        "**Figure 7.GAN-FLOW: GAN Structure**\n",
        "![GAN Structure](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-1.png \"GAN Structure\")\n",
        "\n",
        "## Face Generation with StyleGAN and Python\n",
        "\n",
        "GANs have appeared frequently in the media, showcasing their ability to generate highly photorealistic faces. One significant step forward for realistic face generation was the NVIDIA StyleGAN series. NVIDIA introduced the origional StyleGAN in 2018. [[Cite:karras2019style]](https://arxiv.org/abs/1812.04948) StyleGAN was followed by StyleGAN2 in 2019, which improved the quality of StyleGAN by removing certian artifacts. [[Cite:karras2019analyzing]](https://arxiv.org/abs/1912.04958) Most recently, in 2020, NVIDIA released StyleGAN2 adaptive discriminator augmentation (ADA), which will be the focus of this module. [[Cite:karras2020training]](https://arxiv.org/abs/2006.06676)  We will see both how to train StyleGAN2 ADA on any arbitray set of images; as well as use pretrained weights provided by NVIDIA. The NVIDIA weights allow us to generate high resolution photorealistic looking faces, such seen in Figure 7.STY-GAN.\n",
        "\n",
        "**Figure 7.STY-GAN: StyleGAN2 Generated Faces**\n",
        "![StyleGAN2 Generated Faces](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2_images.jpg \"StyleGAN2 Generated Faces\")\n",
        "\n",
        "The above images were generated with StyleGAN2, using Google CoLab. Following the instructions in this section, you will be able to create faces like this of your own. StyleGAN2 images are usually 1,024 x 1,024 in resolution.  An example of a full-resolution StyleGAN image can be [found here](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2-hires.jpg).\n",
        "\n",
        "The primary advancement introduced by the adaptive discriminator augmentation is that the algorithm augments the training images in real-time. Image augmentation is a common technique in many convolution neural network applications. Augmentation has the effect of increasing the size of the training set. Where StyleGAN2 previously required over 30K images for an effective to develop an effective neural network; now much fewer are needed. I used 2K images to train the fish generating GAN for this section. Figure 7.STY-GAN-ADA demonstrates the ADA process.\n",
        "\n",
        "**Figure 7.STY-GAN-ADA: StyleGAN2 ADA Training**\n",
        "![StyleGAN2 Generated Faces](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2-ada-teaser-1024x252.jpg \"StyleGAN2 Generated Faces\")\n",
        "\n",
        "The figure shows the increasing probability of augmentation as $p$ increases. For small image sets, the discriminator will generally memorize the image set unless the training algorithm makes use of augmentation. Once this memorization occurs, the discriminator is no longer providing useful information to the training of the generator.\n",
        "\n",
        "While the above images look much more realistic than images generated earlier in this course, they are not perfect. Look at Figure 7.STYLEGAN2. There are usually several tell-tail signs that you are looking at a computer-generated image. One of the most obvious is usually the surreal, dream-like backgrounds. The background does not look obviously fake at first glance; however, upon closer inspection, you usually can't quite discern what a GAN-generated background is. Also, look at the image character's left eye. It is slightly unrealistic looking, especially near the eyelashes.\n",
        "\n",
        "Look at the following GAN face. Can you spot any imperfections?\n",
        "\n",
        "**Figure 7.STYLEGAN2: StyleGAN2 Face**\n",
        "![StyleGAN2 Face](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_bad.jpg \"StyleGAN2 Face\")\n",
        "\n",
        "* Image A demonstrates the abstract backgrounds usually associated with a GAN-generated image.\n",
        "* Image B exhibits issues that earrings often present for GANs. GANs sometimes have problems with symmetry, particularly earrings.\n",
        "* Image C contains an abstract background and a highly distorted secondary image.\n",
        "* Image D also contains a highly distorted secondary image that might be a hand.\n",
        "\n",
        "Several websites allow you to generate GANs of your own without any software.\n",
        "\n",
        "* [This Person Does not Exist](https://www.thispersondoesnotexist.com/)\n",
        "* [Which Face is Real](http://www.whichfaceisreal.com/)\n",
        "\n",
        "The first site generates high-resolution images of human faces. The second site presents a quiz to see if you can detect the difference between a real and fake human face image.\n",
        "\n",
        "In this chapter, you will learn to create your own StyleGAN pictures using Python."
      ],
      "metadata": {
        "id": "8-csHyzxgErV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating High Rez GAN Faces with Google CoLab\n",
        "\n",
        "This notebook demonstrates how to run [NVidia StyleGAN2 ADA](https://github.com/NVlabs/stylegan2-ada) inside a Google CoLab notebook.  I suggest you use this to generate GAN faces from a pretrained model.  If you try to train your own, you will run into compute limitations of Google CoLab. Make sure to run this code on a GPU instance.  GPU is assumed."
      ],
      "metadata": {
        "id": "lJKxe8RkgMIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install LongChain"
      ],
      "metadata": {
        "id": "5cKto7x-RVn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVlabs/stylegan3.git\n",
        "!pip install ninja"
      ],
      "metadata": {
        "id": "XWo7LO9NaNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that StyleGAN has been cloned.\n"
      ],
      "metadata": {
        "id": "YYarikQqhZdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/stylegan3"
      ],
      "metadata": {
        "id": "YSD-UaSThbjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run StyleGan From Command Line**\n",
        "\n",
        "Add the StyleGAN folder to Python so that you can import it. I based this code below on code from NVidia for the original StyleGAN paper. When you use StyleGAN you will generally create a GAN from a seed number. This seed is an integer, such as 6600, that will generate a unique image. The seed generates a latent vector containing 512 floating-point values. The GAN code uses the seed to generate these 512 values. The seed value is easier to represent in code than a 512 value vector; however, while a small change to the latent vector results in a slight change to the image, even a small change to the integer seed value will produce a radically different image."
      ],
      "metadata": {
        "id": "9ftIaZHNBzgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/\"\\\n",
        "      \"stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
        "\n",
        "!python /content/stylegan3/gen_images.py \\\n",
        "    --network={URL} \\\n",
        "  --outdir=/content/results --seeds=6600-6625"
      ],
      "metadata": {
        "id": "T7l6X9E9U6Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/results"
      ],
      "metadata": {
        "id": "2hC9hktBh0v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, copy the images to a folder of your choice on GDrive."
      ],
      "metadata": {
        "id": "-NEd73tbh8PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/My Drive/projects/stylegan3\"\n"
      ],
      "metadata": {
        "id": "80L8ZwLxijTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/results/* \\\n",
        "    /content/drive/My\\ Drive/projects/stylegan3"
      ],
      "metadata": {
        "id": "yNog25R9iBez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now display the images created.\n"
      ],
      "metadata": {
        "id": "I3PZOacXhvyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run StyleGAN From Python Code**\n",
        "\n",
        "Add the StyleGAN folder to Python so that you can import it.  "
      ],
      "metadata": {
        "id": "-ery1YvsDEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/stylegan3\")\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display\n",
        "import torch\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "def seed2vec(G, seed):\n",
        "  return np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "\n",
        "def display_image(image):\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "def generate_image(G, z, truncation_psi):\n",
        "    # Render images for dlatents initialized from random seeds.\n",
        "    Gs_kwargs = {\n",
        "        'output_transform': dict(func=tflib.convert_images_to_uint8,\n",
        "         nchw_to_nhwc=True),\n",
        "        'randomize_noise': False\n",
        "    }\n",
        "    if truncation_psi is not None:\n",
        "        Gs_kwargs['truncation_psi'] = truncation_psi\n",
        "\n",
        "    label = np.zeros([1] + G.input_shapes[1][1:])\n",
        "    # [minibatch, height, width, channel]\n",
        "    images = G.run(z, label, **G_kwargs)\n",
        "    return images[0]\n",
        "\n",
        "def get_label(G, device, class_idx):\n",
        "  label = torch.zeros([1, G.c_dim], device=device)\n",
        "  if G.c_dim != 0:\n",
        "      if class_idx is None:\n",
        "          ctx.fail(\"Must specify class label with --class when using \"\\\n",
        "            \"a conditional network\")\n",
        "      label[:, class_idx] = 1\n",
        "  else:\n",
        "      if class_idx is not None:\n",
        "          print (\"warn: --class=lbl ignored when running on \"\\\n",
        "            \"an unconditional network\")\n",
        "  return label\n",
        "\n",
        "def generate_image(device, G, z, truncation_psi=1.0, noise_mode='const',\n",
        "                   class_idx=None):\n",
        "  z = torch.from_numpy(z).to(device)\n",
        "  label = get_label(G, device, class_idx)\n",
        "  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(\\\n",
        "      torch.uint8)\n",
        "  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')"
      ],
      "metadata": {
        "id": "kyd7OBeJEgMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#URL = \"https://github.com/jeffheaton/pretrained-gan-fish/releases/\"\\\n",
        "#  \"download/1.0.0/fish-gan-2020-12-09.pkl\"\n",
        "#URL = \"https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/\"\\\n",
        "#  \"download/v1/christmas-gan-2020-12-03.pkl\"\n",
        "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/\"\\\n",
        "  \"versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
        "\n",
        "print(f'Loading networks from \"{URL}\"...')\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(URL) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore"
      ],
      "metadata": {
        "id": "OJS3QZIDjBuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now generate images from integer seed codes in Python."
      ],
      "metadata": {
        "id": "lrWr0Z5CjHwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your own starting and ending seed.\n",
        "SEED_FROM = 1000\n",
        "SEED_TO = 1003\n",
        "\n",
        "# Generate the images for the seeds.\n",
        "for i in range(SEED_FROM, SEED_TO):\n",
        "  print(f\"Seed {i}\")\n",
        "  z = seed2vec(G, i)\n",
        "  img = generate_image(device, G, z)\n",
        "  display_image(img)"
      ],
      "metadata": {
        "id": "hmA22qkSTyDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DQnYSgI1TxBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Examining the Latent Vector**\n",
        "\n",
        "Figure 7.LVEC shows the effects of transforming the latent vector between two images. We accomplish this transformation by slowly moving one 512-value latent vector to another 512 vector. A high-dimension point between two latent vectors will appear similar to both of the two endpoint latent vectors. Images that have similar latent vectors will appear similar to each other.\n",
        "\n",
        "**Figure 7.LVEC: Transforming the Latent Vector**\n",
        "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_progression.jpg \"GAN\")"
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_seed(seeds, vector_size):\n",
        "  result = []\n",
        "\n",
        "  for seed in seeds:\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    result.append( rnd.randn(1, vector_size) )\n",
        "  return result\n",
        "\n",
        "#URL = \"https://github.com/jeffheaton/pretrained-gan-fish/releases/\"\\\n",
        "#  \"download/1.0.0/fish-gan-2020-12-09.pkl\"\n",
        "#URL = \"https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/\"\\\n",
        "#  \"download/v1/christmas-gan-2020-12-03.pkl\"\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"\n",
        "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/\"\\\n",
        "  \"versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
        "\n",
        "print(f'Loading networks from \"{URL}\"...')\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(URL) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "\n",
        "vector_size = G.z_dim\n",
        "# range(8192,8300)\n",
        "seeds = expand_seed( [8192+1,8192+9], vector_size)\n",
        "#generate_images(Gs, seeds,truncation_psi=0.5)\n",
        "print(seeds[0].shape)"
      ],
      "metadata": {
        "id": "VPuxwG6U3mPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "The following code will move between the provided seeds. The constant STEPS specify how many frames there should be between each seed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HIDE OUTPUT\n",
        "# Choose your seeds to morph through and the number of steps to\n",
        "# take to get to each.\n",
        "\n",
        "SEEDS = [6624,6618,6616] # Better for faces\n",
        "#SEEDS = [1000,1003,1001] # Better for fish\n",
        "STEPS = 100\n",
        "\n",
        "# Remove any prior results\n",
        "!rm /content/results/*\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "os.makedirs(\"./results/\", exist_ok=True)\n",
        "\n",
        "# Generate the images for the video.\n",
        "idx = 0\n",
        "for i in range(len(SEEDS)-1):\n",
        "  v1 = seed2vec(G, SEEDS[i])\n",
        "  v2 = seed2vec(G, SEEDS[i+1])\n",
        "\n",
        "  diff = v2 - v1\n",
        "  step = diff / STEPS\n",
        "  current = v1.copy()\n",
        "\n",
        "  for j in tqdm(range(STEPS), desc=f\"Seed {SEEDS[i]}\"):\n",
        "    current = current + step\n",
        "    img = generate_image(device, G, current)\n",
        "    img.save(f'./results/frame-{idx}.png')\n",
        "    idx+=1\n",
        "\n",
        "# Link the images into a video.\n",
        "!ffmpeg -r 30 -i /content/results/frame-%d.png -vcodec mpeg4 -y movie.mp4"
      ],
      "metadata": {
        "id": "9Eb-aLHn4L8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now download the generated video."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('movie.mp4')"
      ],
      "metadata": {
        "id": "QDaSFl2aGPNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **LARGE LANGUAGE MODEL (LLM)**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Large-scale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg/2560px-Large-scale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg.png)\n",
        "\n",
        "A **large language model (LLM)** is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
        "\n",
        "The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n",
        "\n",
        "**History**\n",
        "\n",
        "The training compute of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models.\n",
        "\n",
        "Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.\n",
        "\n",
        "After neural networks became dominant in image processing around 2012,[9] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before transformers, it was done by seq2seq deep LSTM networks.\n",
        "\n",
        "At the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\n",
        "\n",
        "Although decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work.\n",
        "\n",
        "Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June 2024, The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\n",
        "\n",
        "Since 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).\n",
        "\n",
        "As of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\n"
      ],
      "metadata": {
        "id": "TrgINf3ckz7U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUW9ffO9Yl8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9 (torch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}