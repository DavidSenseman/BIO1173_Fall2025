{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/F25_Class_04_4ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Transformers and Accessing ChatGTP\n",
        "* Part 4.2: LLM Memory and Embedding\n",
        "* Part 4.3: Generative AI and Generating Faces with StyleGAN3\n",
        "* **Part 4.4: Text to Images with Stable Diffusion**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is included as the last line in the output above."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text to Images with StableDiffusion**\n",
        "\n",
        "We will now see how to use Stable Diffusion to create various images from textual prompts. There will be four settings that we will deal with as we generate these images.\n",
        "\n",
        "* **model**: We will use the trained/finetuned model. Different models are optimized for different types of images.\n",
        "* **prompt**: Text that you provide to describe what sort of image you would like created.\n",
        "* **negative prompt**: Text that you describe elements that should not be present in your image.\n",
        "* **seed**: The same image for the prompt/negative prompt will always be produced for the same seed. To get a different image for the same prompts, change the seed.\n"
      ],
      "metadata": {
        "id": "ZJrIgZkszA7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Stable Difussion"
      ],
      "metadata": {
        "id": "1Pea33CMueuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade diffusers[torch] > /dev/null\n",
        "!pip install k_diffusion > /dev/null\n",
        "!pip install transformers scipy ftfy accelerate > /dev/null"
      ],
      "metadata": {
        "id": "XWo7LO9NaNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Basic Models**\n",
        "\n",
        "To make use of Stable Diffusion we will use the HuggingFace DiffusionPipeline. When setting up the pipeline we specify to use the \"CompVis/stable-diffusion-v1-4\" model, which is a basic model created to be used with StableDiffusion. The following code sets up this model and downloads it from HuggingFace."
      ],
      "metadata": {
        "id": "9ftIaZHNBzgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe=pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "T7l6X9E9U6Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will begin by creating a simple picture of an astronaut playing golf on the moon! We first use the seed of 100, you could also use -1 to request a random seed. We specify both the prompt and negative prompt.\n",
        "\n",
        "This pipeline does contain code to ensure that no images generated are \"not safe for work\" (NSFW). This setting tries to protect you from unsafe images. If such an image is generated, you will see the following message.\n",
        "\n",
        "```\n",
        "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
        "```\n",
        "\n",
        "You may wish to disable this feature. To do this, uncomment the pipe.safety_checker line. Be cafeful, if you do disable this, as unsafe images may be generated containing NSFW themes, which might contain violence, nudity, or sexual themes.\n",
        "\n",
        "The following code generates the astronaut image."
      ],
      "metadata": {
        "id": "vRGHWr99vU92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random number seed, -1 for random seed\n",
        "seed = 100 #-1\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompts to generate\n",
        "prompt= \"\"\"\n",
        "astronaut playing golf on the moon\"\"\"\n",
        "\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark\n",
        "\"\"\"\n",
        "\n",
        "#pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "2hC9hktBh0v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Realistic Models**\n",
        "\n",
        "You might like to try adjusting the code in the previous section to create other images. If you are trying to create images of people, you will be better served with a model like Realistic Vision, which will be demonstrated in this section. The following code loads Realistic Vision."
      ],
      "metadata": {
        "id": "-ery1YvsDEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    #'hakurei/waifu-diffusion',\n",
        "    \"SG161222/Realistic_Vision_V2.0\",\n",
        "    custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    generator=generator,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe=pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "kyd7OBeJEgMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now generate an image with a much more complex prompt. The positive and negative prompts describe how to generate an image of a young woman. Stable diffusion prompts are usually comma separated lists of attributes to draw. You will notice that some are enclosed in paranthesis; which designates that this attribute is more important. A number, near the end, separated by a colon specifies how important."
      ],
      "metadata": {
        "id": "DoXoxynov8UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random number seed, -1 for random seed\n",
        "seed = 102 #-1\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompts to generate\n",
        "prompt= \"\"\"\n",
        "(woman age 26 standing by tree), (long blonde hair:1.2), ray traced shadows,\n",
        "RAW, 8k, (eczema:0.7), (sub-surface scattering:1.55), (sweat:1.22), (freckles:0.55),\n",
        "highly detailed skin, (Acne:0.7), (FACE1:0.5), (FACE2:1.2), (FACE3:0.85),\n",
        "perfect eyes, no makeup. (skin spores:1.05), (skin spores:1.05),\n",
        "ultra detailed face, ultra detailed skin, film grain, ray tracing, studio lighting\"\"\"\n",
        "\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, airbrush, photoshop, plastic doll,\n",
        "(ugly eyes, deformed iris, deformed pupils, fused lips and teeth:1.2),\n",
        "(un-detailed skin, semi-realistic, cgi, 3d, render, sketch, cartoon,\n",
        "drawing, anime:1.2), text, close up, cropped, out of frame, worst quality,\n",
        "low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers,\n",
        "mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry,\n",
        "dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured,\n",
        "gross proportions, malformed limbs, missing arms, missing legs, extra arms,\n",
        "extra legs, fused fingers, too many fingers, long neck, head wear, masculine,\n",
        "obese, fat, out of frame\"\"\"\n",
        "\n",
        "#pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "OJS3QZIDjBuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Anime Models**\n",
        "\n",
        "If you would like to generate cartoon or Anime style images, the waifu-diffusion model will work nicely. The code below loads this model and generates an Anime character."
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"hakurei/waifu-diffusion\",\n",
        "    custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    generator=generator,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe=pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "VPuxwG6U3mPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "best_quality (1girl:1.3) bow bride brown_hair closed_mouth frilled_bow\n",
        "frilled_hair_tubes frills (full_body:1.3) fox_ear hair_bow hair_tubes\n",
        "happy hood japanese_clothes kimono long_sleeves red_bow smile solo tabi\n",
        "uchikake white_kimono wide_sleeves cherry_blossoms\"\"\"\n",
        "neg_prompt = \"\"\"\n",
        "lowres, bad_anatomy, error_body, error_hair, error_arm, error_hands, bad_hands,\n",
        "error_fingers, bad_fingers, missing_fingers, error_legs, bad_legs, multiple_legs,\n",
        "missing_legs, error_lighting, error_shadow, error_reflection, text, error,\n",
        "extra_digit, fewer_digits, cropped, worst_quality, low_quality, normal_quality,\n",
        "jpeg_artifacts, signature, watermark, username, blurry\"\"\"\n",
        "\n",
        "# Random number seed, -1 for random seed\n",
        "seed = 105 #-1\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Generate the image\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "5vuRG-_cwcDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "The following code will move between the provided seeds. The constant STEPS specify how many frames there should be between each seed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "best_quality (1panda:1.3) \"\"\"\n",
        "neg_prompt = \"\"\"\n",
        "lowres, bad_anatomy, error_body, error_hair, error_arm, error_hands, bad_hands,\n",
        "error_fingers, bad_fingers, missing_fingers, error_legs, bad_legs, multiple_legs,\n",
        "missing_legs, error_lighting, error_shadow, error_reflection, text, error,\n",
        "extra_digit, fewer_digits, cropped, worst_quality, low_quality, normal_quality,\n",
        "jpeg_artifacts, signature, watermark, username, blurry\"\"\"\n",
        "\n",
        "# Random number seed, -1 for random seed\n",
        "seed = 105 #-1\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Generate the image\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "9Eb-aLHn4L8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_4.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **BACKPROGATION**\n",
        "\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png)\n",
        "\n",
        "\n",
        "In machine learning, **backpropagation** is a gradient estimation method commonly used for training a neural network to compute its parameter updates.\n",
        "\n",
        "It is an efficient application of the chain rule to neural networks. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.\n",
        "\n",
        "Strictly speaking, the term backpropagation refers only to an algorithm for efficiently computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent, or as an intermediate step in a more complicated optimizer, such as Adaptive Moment Estimation.[5] The local minimum convergence, exploding gradient, vanishing gradient, and weak control of learning rate are main disadvantages of these optimization algorithms. The Hessian and quasi-Hessian optimizers solve only local minimum convergence problem, and the backpropagation works longer. These problems caused researchers to develop hybrid and fractional optimization algorithms.\n",
        "\n",
        "Backpropagation had multiple discoveries and partial discoveries, with a tangled history and terminology. See the history section for details. Some other names for the technique include \"reverse mode of automatic differentiation\" or \"reverse accumulation\".\n",
        "\n",
        "## **Intuition**\n",
        "\n",
        "**Motivation**\n",
        "\n",
        "The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.\n",
        "\n",
        "**Learning as an optimization problem**\n",
        "\n",
        "To understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear) that is the weighted sum of its input.\n",
        "\n",
        "**History**\n",
        "\n",
        "Backpropagation had been derived repeatedly, as it is essentially an efficient application of the chain rule (first written down by Gottfried Wilhelm Leibniz in 1676) to neural networks.\n",
        "\n",
        "The terminology \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this. In any case, he only studied neurons whose outputs were discrete levels, which only had zero derivatives, making backpropagation impossible.\n",
        "\n",
        "Precursors to backpropagation appeared in optimal control theory since 1950s. Yann LeCun et al credits 1950s work by Pontryagin and others in optimal control theory, especially the adjoint state method, for being a continuous-time version of backpropagation. Hecht-Nielsen credits the Robbins–Monro algorithm (1951)[23] and Arthur Bryson and Yu-Chi Ho's Applied Optimal Control (1969) as presages of backpropagation. Other precursors were Henry J. Kelley 1960, and Arthur E. Bryson (1961). In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. In 1973, he adapted parameters of controllers in proportion to error gradients. Unlike modern backpropagation, these precursors used standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.\n",
        "\n",
        "The ADALINE (1960) learning algorithm was gradient descent with a squared error loss for a single layer. The first multilayer perceptron (MLP) with more than one layer trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.[29] The MLP had 5 layers, with 2 learnable layers, and it learned to classify patterns not linearly separable.\n",
        "\n",
        "**Modern backpropagation**\n",
        "\n",
        "Modern backpropagation was first published by Seppo Linnainmaa as \"reverse mode of automatic differentiation\" (1970) for discrete connected networks of nested differentiable functions.\n",
        "\n",
        "In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. Werbos described how he developed backpropagation in an interview. In 1971, during his PhD work, he developed backpropagation to mathematicize Freud's \"flow of psychic energy\". He faced repeated difficulty in publishing the work, only managing in 1981. He also claimed that \"the first practical application of back-propagation was for estimating a dynamic model to predict nationalism and social communications in 1974\" by him.\n",
        "\n",
        "Around 1982, David E. Rumelhart independently developed backpropagation and taught the algorithm to others in his research circle. He did not cite previous work as he was unaware of them. He published the algorithm first in a 1985 paper, then in a 1986 Nature paper an experimental analysis of the technique These papers became highly cited, contributed to the popularization of backpropagation, and coincided with the resurging research interest in neural networks during the 1980s.\n",
        "\n",
        "In 1985, the method was also described by David Parker. Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987.\n",
        "\n",
        "Gradient descent took a considerable amount of time to reach acceptance. Some early objections were: there were no guarantees that gradient descent could reach a global minimum, only local minimum; neurons were \"known\" by physiologists as making discrete signals (0/1), not continuous ones, and with discrete signals, there is no gradient to take. See the interview with Geoffrey Hinton,[36] who was awarded the 2024 Nobel Prize in Physics for his contributions to the field.\n",
        "\n",
        "**Early successes**\n",
        "\n",
        "Contributing to the acceptance were several applications in training neural networks via backpropagation, sometimes achieving popularity outside the research circles.\n",
        "\n",
        "In 1987, NETtalk learned to convert English text into pronunciation. Sejnowski tried training it with both backpropagation and Boltzmann machine, but found the backpropagation significantly faster, so he used it for the final NETtalk. The NETtalk program became a popular success, appearing on the Today show.\n",
        "\n",
        "In 1989, Dean A. Pomerleau published ALVINN, a neural network trained to drive autonomously using backpropagation.\n",
        "\n",
        "The LeNet was published in 1989 to recognize handwritten zip codes.\n",
        "\n",
        "In 1992, TD-Gammon achieved top human level play in backgammon. It was a reinforcement learning agent with a neural network with two layers, trained by backpropagation.\n",
        "\n",
        "In 1993, Eric Wan won an international pattern recognition contest through backpropagation.\n",
        "\n",
        "**After backpropagation**\n",
        "\n",
        "During the 2000s it fell out of favour, but returned in the 2010s, benefiting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first and second language learning.\n",
        "\n",
        "Error backpropagation has been suggested to explain human brain event-related potential (ERP) components like the N400 and P600.\n",
        "\n",
        "In 2023, a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University."
      ],
      "metadata": {
        "id": "6x8gMtgQxMMH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUW9ffO9Yl8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9 (torch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}