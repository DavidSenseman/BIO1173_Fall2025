{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPq/I2CInVNugAZXMui4HP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/AI_Scientist_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI-Scientist**"
      ],
      "metadata": {
        "id": "s8M_w6vZggie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LHLlCwFJnq6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    # from google.colab import auth\n",
        "    # auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    # import requests\n",
        "    # gcloud_token = !gcloud auth print-access-token\n",
        "    # gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    # print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g7XGhn1ggT_",
        "outputId": "256d9a0a-e7bb-488e-d5e4-3098006c5960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "arvWVSzIi4U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rCZlwmVNkYxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")"
      ],
      "metadata": {
        "id": "NQvLfMNLi4DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oFYM09f4j-N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Git in Colab"
      ],
      "metadata": {
        "id": "8x8rzWsTlq7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git -y\n",
        "!git clone https://github.com/SakanaAI/AI-Scientist.git\n",
        "%cd AI-Scientist\n",
        "!cp -r /content/AI-Scientist/* /content/\n",
        "!ls /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC7ahgzVlqiv",
        "outputId": "f89a6730-5982-4344-9271-cf63de1bf9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Cloning into 'AI-Scientist'...\n",
            "remote: Enumerating objects: 2634, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 2634 (delta 2), reused 1 (delta 1), pack-reused 2628 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2634/2634), 116.86 MiB | 15.35 MiB/s, done.\n",
            "Resolving deltas: 100% (463/463), done.\n",
            "Updating files: 100% (2319/2319), done.\n",
            "/content/AI-Scientist\n",
            "ai_scientist  docs\t      experimental\t   README.md\t\treview_iclr_bench\n",
            "AI-Scientist  drive\t      launch_scientist.py  requirements.txt\tsample_data\n",
            "data\t      example_papers  LICENSE\t\t   review_ai_scientist\ttemplates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the Respository"
      ],
      "metadata": {
        "id": "4lvNZp6VmPf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "ZlTMa6G7nMtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MobileNetV3**\n",
        "\n",
        "**Description:** This template investigates transformer-based autoregressive next-token prediction tasks."
      ],
      "metadata": {
        "id": "F386d55Mtupz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prepare the data"
      ],
      "metadata": {
        "id": "oxkQVyapt7ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare the data\n",
        "\n",
        "!python data/enwik8/prepare.py\n",
        "!python data/shakespeare_char/prepare.py\n",
        "!python data/text8/prepare.py"
      ],
      "metadata": {
        "id": "i7De_eDBpFi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create baseline runs (machine dependent)"
      ],
      "metadata": {
        "id": "cSQmNHKlplYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI-Scientist/templates/mobilenetV3\n",
        "#!python experiment.py"
      ],
      "metadata": {
        "id": "L-7dTsU9rOY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yRCK5Sj3Pve0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from typing import Callable, List, Optional, Union, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# _make_divisible function from torchvision\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function ensures that all layers have a channel number that is divisible by 8.\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that rounding down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "# Squeeze-and-Excitation block\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_channels: int,\n",
        "            squeeze_channels: int,\n",
        "            activation: Callable[..., nn.Module] = nn.ReLU,\n",
        "            scale_activation: Callable[..., nn.Module] = nn.Hardsigmoid,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
        "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
        "        self.activation = activation(inplace=True)\n",
        "        self.scale_activation = scale_activation(inplace=True)\n",
        "\n",
        "    def _scale(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        scale = self.avgpool(input)\n",
        "        scale = self.fc1(scale)\n",
        "        scale = self.activation(scale)\n",
        "        scale = self.fc2(scale)\n",
        "        scale = self.scale_activation(scale)\n",
        "        return scale\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        scale = self._scale(input)\n",
        "        return input * scale\n",
        "\n",
        "\n",
        "# ConvNormActivation block\n",
        "class ConvNormActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: Union[int, Tuple[int]] = 3,\n",
        "            stride: Union[int, Tuple[int]] = 1,\n",
        "            padding: Optional[Union[int, Tuple[int], str]] = None,\n",
        "            groups: int = 1,\n",
        "            norm_layer: Optional[Callable[..., nn.Module]] = nn.BatchNorm2d,\n",
        "            activation_layer: Optional[Callable[..., nn.Module]] = nn.ReLU,\n",
        "            dilation: Union[int, Tuple[int]] = 1,\n",
        "            bias: Optional[bool] = None,\n",
        "    ) -> None:\n",
        "\n",
        "        if padding is None:\n",
        "            if isinstance(kernel_size, int):\n",
        "                padding = (kernel_size - 1) // 2 * dilation\n",
        "            else:\n",
        "                padding = tuple((k - 1) // 2 * d for k, d in zip(kernel_size, dilation))\n",
        "        if bias is None:\n",
        "            bias = norm_layer is None\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                dilation=dilation,\n",
        "                groups=groups,\n",
        "                bias=bias,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if norm_layer is not None:\n",
        "            layers.append(norm_layer(out_channels))\n",
        "        if activation_layer is not None:\n",
        "            layers.append(activation_layer(inplace=True))\n",
        "        super().__init__(*layers)\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "\n",
        "# InvertedResidualConfig class\n",
        "class InvertedResidualConfig:\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_channels: int,\n",
        "            kernel: int,\n",
        "            expanded_channels: int,\n",
        "            out_channels: int,\n",
        "            use_se: bool,\n",
        "            activation: str,\n",
        "            stride: int,\n",
        "            dilation: int,\n",
        "            width_mult: float,\n",
        "    ):\n",
        "        self.input_channels = self.adjust_channels(input_channels, width_mult)\n",
        "        self.kernel = kernel\n",
        "        self.expanded_channels = self.adjust_channels(expanded_channels, width_mult)\n",
        "        self.out_channels = self.adjust_channels(out_channels, width_mult)\n",
        "        self.use_se = use_se\n",
        "        self.activation = activation\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "\n",
        "    @staticmethod\n",
        "    def adjust_channels(channels: int, width_mult: float):\n",
        "        return _make_divisible(channels * width_mult, 8)\n",
        "\n",
        "\n",
        "# InvertedResidual block\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            cnf: InvertedResidualConfig,\n",
        "            norm_layer: Callable[..., nn.Module],\n",
        "            se_layer: Callable[..., nn.Module] = partial(SqueezeExcitation, scale_activation=nn.Hardsigmoid),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not (1 <= cnf.stride <= 2):\n",
        "            raise ValueError(\"Illegal stride value\")\n",
        "\n",
        "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        activation_layer = nn.Hardswish if cnf.activation == \"HS\" else nn.ReLU\n",
        "\n",
        "        # Expand phase\n",
        "        if cnf.expanded_channels != cnf.input_channels:\n",
        "            layers.append(\n",
        "                ConvNormActivation(\n",
        "                    cnf.input_channels,\n",
        "                    cnf.expanded_channels,\n",
        "                    kernel_size=1,\n",
        "                    norm_layer=norm_layer,\n",
        "                    activation_layer=activation_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Depthwise convolution\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                cnf.expanded_channels,\n",
        "                cnf.expanded_channels,\n",
        "                kernel_size=cnf.kernel,\n",
        "                stride=cnf.stride,\n",
        "                groups=cnf.expanded_channels,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=activation_layer,\n",
        "                dilation=cnf.dilation,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        if cnf.use_se:\n",
        "            squeeze_channels = _make_divisible(cnf.expanded_channels // 4, 8)\n",
        "            layers.append(\n",
        "                se_layer(\n",
        "                    cnf.expanded_channels,\n",
        "                    squeeze_channels,\n",
        "                    activation=nn.ReLU,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Project phase\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                cnf.expanded_channels,\n",
        "                cnf.out_channels,\n",
        "                kernel_size=1,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "        self.out_channels = cnf.out_channels\n",
        "        self.is_strided = cnf.stride > 1\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        result = self.block(input)\n",
        "        if self.use_res_connect:\n",
        "            return input + result\n",
        "        else:\n",
        "            return result\n",
        "\n",
        "\n",
        "# MobileNetV3 Small model\n",
        "class MobileNetV3Small(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_classes: int = 1000,\n",
        "            width_mult: float = 1.0,\n",
        "            dropout: float = 0.2,\n",
        "            reduced_tail: bool = False,\n",
        "            dilated: bool = False,\n",
        "            norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "\n",
        "        bneck_conf = partial(InvertedResidualConfig, width_mult=width_mult)\n",
        "\n",
        "        # Build inverted residual setting\n",
        "        reduce_divider = 2 if reduced_tail else 1\n",
        "        dilation = 2 if dilated else 1\n",
        "\n",
        "        inverted_residual_setting = [\n",
        "            # input_c, kernel, exp_c, out_c, se, nl, s, d\n",
        "            bneck_conf(16, 3, 16, 16, True, \"RE\", 2, 1),\n",
        "            bneck_conf(16, 3, 72, 24, False, \"RE\", 2, 1),\n",
        "            bneck_conf(24, 3, 88, 24, False, \"RE\", 1, 1),\n",
        "            bneck_conf(24, 5, 96, 40, True, \"HS\", 2, 1),\n",
        "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
        "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
        "            bneck_conf(40, 5, 120, 48, True, \"HS\", 1, 1),\n",
        "            bneck_conf(48, 5, 144, 48, True, \"HS\", 1, 1),\n",
        "            bneck_conf(48, 5, 288 // reduce_divider, 96 // reduce_divider, True, \"HS\", 2, dilation),\n",
        "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
        "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
        "        ]\n",
        "\n",
        "        last_channel = _make_divisible(1024 // reduce_divider * width_mult, 8)\n",
        "\n",
        "        # First layer\n",
        "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                3,\n",
        "                firstconv_output_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=nn.Hardswish,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Building inverted residual blocks\n",
        "        for cnf in inverted_residual_setting:\n",
        "            layers.append(InvertedResidual(cnf, norm_layer))\n",
        "\n",
        "        # Building last several layers\n",
        "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
        "        lastconv_output_channels = _make_divisible(576 * width_mult, 8)\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                lastconv_input_channels,\n",
        "                lastconv_output_channels,\n",
        "                kernel_size=1,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=nn.Hardswish,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lastconv_output_channels, last_channel),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Dropout(p=dropout, inplace=True),\n",
        "            nn.Linear(last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.SyncBatchNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Function to create the model and load pretrained weights\n",
        "def mobilenet_v3_small(pretrained=False, progress=True, **kwargs):\n",
        "    model = MobileNetV3Small(**kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        # Load the torchvision model with pretrained weights\n",
        "        from torchvision.models import mobilenet_v3_small as tv_mobilenet_v3_small\n",
        "        from torchvision.models import MobileNet_V3_Small_Weights\n",
        "\n",
        "        # Check for number of classes\n",
        "        if kwargs.get('num_classes', 1000) != 1000:\n",
        "            # We cannot load the classifier weights (different classes)\n",
        "            pretrained_model = tv_mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT, progress=progress)\n",
        "            pretrained_state_dict = pretrained_model.state_dict()\n",
        "            # Remove classifier weights\n",
        "            pretrained_state_dict = {k: v for k, v in pretrained_state_dict.items() if not k.startswith('classifier')}\n",
        "            model_dict = model.state_dict()\n",
        "            print(model_dict.keys())\n",
        "            # Update the model dict\n",
        "            model_dict.update(pretrained_state_dict)\n",
        "            model.load_state_dict(model_dict)\n",
        "        else:\n",
        "            # Load all weights\n",
        "            pretrained_model = tv_mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT, progress=progress)\n",
        "            model.load_state_dict(pretrained_model.state_dict())\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # data\n",
        "    data_path: str = './data'\n",
        "    dataset: str = 'cifar10'\n",
        "    num_classes: int = 10\n",
        "    # model\n",
        "    model: str = 'mobilenet_v3_small'\n",
        "    # training\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 0.01\n",
        "    weight_decay: float = 1e-4\n",
        "    epochs: int = 2\n",
        "    # system\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    num_workers: int = 2\n",
        "    # logging\n",
        "    log_interval: int = 100\n",
        "    eval_interval: int = 1000\n",
        "    # output\n",
        "    out_dir: str = 'run_0'\n",
        "    seed: int = 0\n",
        "    # compile for SPEED!\n",
        "    compile_model: bool = False\n",
        "\n",
        "\n",
        "def get_data_loaders(config):\n",
        "    if config.dataset == 'cifar10':\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "\n",
        "        train_dataset = datasets.CIFAR10(root=config.data_path, train=True, download=True, transform=transform_train)\n",
        "        test_dataset = datasets.CIFAR10(root=config.data_path, train=False, download=True, transform=transform_test)\n",
        "    elif config.dataset == 'cifar100':\n",
        "        # Placeholder for CIFAR-100 (for future use)\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
        "                                 (0.2675, 0.2565, 0.2761)),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
        "                                 (0.2675, 0.2565, 0.2761)),\n",
        "        ])\n",
        "\n",
        "        train_dataset = datasets.CIFAR100(root=config.data_path, train=True, download=True, transform=transform_train)\n",
        "        test_dataset = datasets.CIFAR100(root=config.data_path, train=False, download=True, transform=transform_test)\n",
        "        config.num_classes = 100  # Update number of classes for CIFAR-100\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {config.dataset}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def train(config):\n",
        "    # Set random seeds\n",
        "    torch.manual_seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    random.seed(config.seed)\n",
        "    if config.device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(config.seed)\n",
        "\n",
        "    model = mobilenet_v3_small(pretrained=False, progress=True, num_classes=config.num_classes).to(config.device)\n",
        "\n",
        "    if config.compile_model:\n",
        "        print(\"Compiling the model...\")\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=config.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
        "\n",
        "    train_loader, test_loader = get_data_loaders(config)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    train_log_info = []\n",
        "    val_log_info = []\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if batch_idx % config.log_interval == 0:\n",
        "                train_log_info.append({\n",
        "                    'epoch': epoch,\n",
        "                    'batch': batch_idx,\n",
        "                    'loss': train_loss / (batch_idx + 1),\n",
        "                    'acc': 100. * train_correct / train_total,\n",
        "                    'lr': optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {train_loss / (batch_idx + 1):.3f}, '\n",
        "                      f'Acc: {100. * train_correct / train_total:.3f}%, '\n",
        "                      f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion, config)\n",
        "        val_log_info.append({\n",
        "            'epoch': epoch,\n",
        "            'loss': val_loss,\n",
        "            'acc': val_acc\n",
        "        })\n",
        "        print(f'Validation - Loss: {val_loss:.3f}, Acc: {val_acc:.3f}%')\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), os.path.join(config.out_dir, 'best_model.pth'))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_log_info, val_log_info, best_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, config):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += targets.size(0)\n",
        "            val_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(dataloader)\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "\n",
        "def test(config):\n",
        "    model = MobileNetV3Small(num_classes=config.num_classes).to(config.device)\n",
        "    if config.compile_model:\n",
        "        print(\"Compiling the model for testing...\")\n",
        "        model = torch.compile(model)\n",
        "    model.load_state_dict(torch.load(os.path.join(config.out_dir, 'best_model.pth')))\n",
        "    _, test_loader = get_data_loaders(config)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, config)\n",
        "    print(f'Test - Loss: {test_loss:.3f}, Acc: {test_acc:.3f}%')\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train MobileNetV3 for Image Classification\")\n",
        "    parser.add_argument(\"--data_path\", type=str, default=\"./data\", help=\"Path to save/load the dataset\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"Batch size\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"Initial learning rate\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of epochs to train\")\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"run_0\", help=\"Output directory\")\n",
        "\n",
        "    # Parse arguments and ignore unrecognized ones\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    print(f\"Outputs will be saved to {args.out_dir}\")\n",
        "\n",
        "    # Define datasets and number of seeds per dataset\n",
        "    datasets = ['cifar10']  # For now, only CIFAR-10; can add 'cifar100' in the future\n",
        "    num_seeds = {\n",
        "        'cifar10': 1  # Change the number of seeds as desired\n",
        "    }\n",
        "\n",
        "    all_results = {}\n",
        "    final_infos = {}\n",
        "\n",
        "    for dataset in datasets:\n",
        "        final_info_list = []\n",
        "        for seed_offset in range(num_seeds[dataset]):\n",
        "            # Update the config for each run\n",
        "            config = Config(\n",
        "                data_path=args.data_path,\n",
        "                dataset=dataset,\n",
        "                batch_size=args.batch_size,\n",
        "                learning_rate=args.learning_rate,\n",
        "                epochs=args.epochs,\n",
        "                out_dir=args.out_dir,\n",
        "                seed=seed_offset  # Set the seed\n",
        "            )\n",
        "            os.makedirs(config.out_dir, exist_ok=True)\n",
        "            print(f\"Starting training for {dataset} with seed {seed_offset}\")\n",
        "            start_time = time.time()\n",
        "            train_log_info, val_log_info, best_acc = train(config)\n",
        "            total_time = time.time() - start_time\n",
        "\n",
        "            # Run test after training\n",
        "            test_loss, test_acc = test(config)\n",
        "\n",
        "            # Prepare final_info dictionary\n",
        "            final_info = {\n",
        "                \"best_val_acc\": best_acc,\n",
        "                \"test_acc\": test_acc,\n",
        "                \"total_train_time\": total_time,\n",
        "                \"config\": vars(config)\n",
        "            }\n",
        "            final_info_list.append(final_info)\n",
        "\n",
        "            # Store results in all_results\n",
        "            key_prefix = f\"{dataset}_{seed_offset}\"\n",
        "            all_results[f\"{key_prefix}_final_info\"] = final_info\n",
        "            all_results[f\"{key_prefix}_train_log_info\"] = train_log_info\n",
        "            all_results[f\"{key_prefix}_val_log_info\"] = val_log_info\n",
        "\n",
        "            print(f\"Training completed for {dataset} seed {seed_offset}. Best validation accuracy: {best_acc:.2f}%, Test accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        # Aggregate results over seeds\n",
        "        final_info_dict = {k: [d[k] for d in final_info_list if k in d] for k in final_info_list[0].keys()}\n",
        "        means = {f\"{k}_mean\": np.mean(v) for k, v in final_info_dict.items() if isinstance(v[0], (int, float, float))}\n",
        "        stderrs = {f\"{k}_stderr\": np.std(v) / np.sqrt(len(v)) for k, v in final_info_dict.items() if isinstance(v[0], (int, float, float))}\n",
        "        final_infos[dataset] = {\n",
        "            \"means\": means,\n",
        "            \"stderrs\": stderrs,\n",
        "            \"final_info_dict\": final_info_dict\n",
        "        }\n",
        "\n",
        "    # Save final_infos to final_info.json\n",
        "    with open(os.path.join(args.out_dir, \"final_info.json\"), \"w\") as f:\n",
        "        json.dump(final_infos, f, indent=2)\n",
        "\n",
        "    # Save all_results to all_results.npy\n",
        "    with open(os.path.join(args.out_dir, \"all_results.npy\"), \"wb\") as f:\n",
        "        np.save(f, all_results)\n",
        "\n",
        "    print(f\"All results saved to {args.out_dir}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1wDAOQ6QTrb",
        "outputId": "37d8b825-0778-4257-ce0f-e658f94acb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs will be saved to run_0\n",
            "Starting training for cifar10 with seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 12.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 2.305, Acc: 10.156%, LR: 0.010000\n",
            "Epoch: 0, Batch: 100, Loss: 2.225, Acc: 15.965%, LR: 0.010000\n",
            "Epoch: 0, Batch: 200, Loss: 2.076, Acc: 20.977%, LR: 0.010000\n",
            "Epoch: 0, Batch: 300, Loss: 1.982, Acc: 24.450%, LR: 0.010000\n",
            "Validation - Loss: 2.306, Acc: 10.000%\n",
            "Epoch: 1, Batch: 0, Loss: 1.654, Acc: 37.500%, LR: 0.009973\n",
            "Epoch: 1, Batch: 100, Loss: 1.643, Acc: 38.304%, LR: 0.009973\n",
            "Epoch: 1, Batch: 200, Loss: 1.624, Acc: 39.296%, LR: 0.009973\n",
            "Epoch: 1, Batch: 300, Loss: 1.600, Acc: 40.275%, LR: 0.009973\n",
            "Validation - Loss: 1.569, Acc: 42.660%\n",
            "Epoch: 2, Batch: 0, Loss: 1.592, Acc: 44.531%, LR: 0.009891\n",
            "Epoch: 2, Batch: 100, Loss: 1.498, Acc: 45.073%, LR: 0.009891\n",
            "Epoch: 2, Batch: 200, Loss: 1.492, Acc: 45.406%, LR: 0.009891\n",
            "Epoch: 2, Batch: 300, Loss: 1.486, Acc: 45.531%, LR: 0.009891\n",
            "Validation - Loss: 1.480, Acc: 45.950%\n",
            "Epoch: 3, Batch: 0, Loss: 1.427, Acc: 52.344%, LR: 0.009755\n",
            "Epoch: 3, Batch: 100, Loss: 1.433, Acc: 47.161%, LR: 0.009755\n",
            "Epoch: 3, Batch: 200, Loss: 1.418, Acc: 47.765%, LR: 0.009755\n",
            "Epoch: 3, Batch: 300, Loss: 1.414, Acc: 48.020%, LR: 0.009755\n",
            "Validation - Loss: 1.382, Acc: 50.590%\n",
            "Epoch: 4, Batch: 0, Loss: 1.438, Acc: 45.312%, LR: 0.009568\n",
            "Epoch: 4, Batch: 100, Loss: 1.378, Acc: 49.683%, LR: 0.009568\n",
            "Epoch: 4, Batch: 200, Loss: 1.362, Acc: 50.167%, LR: 0.009568\n",
            "Epoch: 4, Batch: 300, Loss: 1.352, Acc: 50.597%, LR: 0.009568\n",
            "Validation - Loss: 1.383, Acc: 50.630%\n",
            "Epoch: 5, Batch: 0, Loss: 1.284, Acc: 53.125%, LR: 0.009330\n",
            "Epoch: 5, Batch: 100, Loss: 1.295, Acc: 53.403%, LR: 0.009330\n",
            "Epoch: 5, Batch: 200, Loss: 1.296, Acc: 53.164%, LR: 0.009330\n",
            "Epoch: 5, Batch: 300, Loss: 1.286, Acc: 53.465%, LR: 0.009330\n",
            "Validation - Loss: 1.272, Acc: 54.350%\n",
            "Epoch: 6, Batch: 0, Loss: 1.278, Acc: 57.031%, LR: 0.009045\n",
            "Epoch: 6, Batch: 100, Loss: 1.247, Acc: 54.765%, LR: 0.009045\n",
            "Epoch: 6, Batch: 200, Loss: 1.242, Acc: 55.111%, LR: 0.009045\n",
            "Epoch: 6, Batch: 300, Loss: 1.237, Acc: 55.342%, LR: 0.009045\n",
            "Validation - Loss: 1.264, Acc: 55.430%\n",
            "Epoch: 7, Batch: 0, Loss: 1.240, Acc: 59.375%, LR: 0.008716\n",
            "Epoch: 7, Batch: 100, Loss: 1.214, Acc: 56.699%, LR: 0.008716\n",
            "Epoch: 7, Batch: 200, Loss: 1.192, Acc: 57.311%, LR: 0.008716\n",
            "Epoch: 7, Batch: 300, Loss: 1.194, Acc: 56.990%, LR: 0.008716\n",
            "Validation - Loss: 1.167, Acc: 58.620%\n",
            "Epoch: 8, Batch: 0, Loss: 1.061, Acc: 60.156%, LR: 0.008346\n",
            "Epoch: 8, Batch: 100, Loss: 1.166, Acc: 58.075%, LR: 0.008346\n",
            "Epoch: 8, Batch: 200, Loss: 1.156, Acc: 58.388%, LR: 0.008346\n",
            "Epoch: 8, Batch: 300, Loss: 1.147, Acc: 58.625%, LR: 0.008346\n",
            "Validation - Loss: 1.090, Acc: 61.440%\n",
            "Epoch: 9, Batch: 0, Loss: 1.232, Acc: 55.469%, LR: 0.007939\n",
            "Epoch: 9, Batch: 100, Loss: 1.115, Acc: 60.326%, LR: 0.007939\n",
            "Epoch: 9, Batch: 200, Loss: 1.124, Acc: 59.779%, LR: 0.007939\n",
            "Epoch: 9, Batch: 300, Loss: 1.121, Acc: 59.754%, LR: 0.007939\n",
            "Validation - Loss: 1.091, Acc: 61.090%\n",
            "Epoch: 10, Batch: 0, Loss: 1.110, Acc: 49.219%, LR: 0.007500\n",
            "Epoch: 10, Batch: 100, Loss: 1.091, Acc: 60.644%, LR: 0.007500\n",
            "Epoch: 10, Batch: 200, Loss: 1.098, Acc: 60.739%, LR: 0.007500\n",
            "Epoch: 10, Batch: 300, Loss: 1.087, Acc: 60.995%, LR: 0.007500\n",
            "Validation - Loss: 1.070, Acc: 62.110%\n",
            "Epoch: 11, Batch: 0, Loss: 1.110, Acc: 60.156%, LR: 0.007034\n",
            "Epoch: 11, Batch: 100, Loss: 1.045, Acc: 62.415%, LR: 0.007034\n",
            "Epoch: 11, Batch: 200, Loss: 1.057, Acc: 62.065%, LR: 0.007034\n",
            "Epoch: 11, Batch: 300, Loss: 1.060, Acc: 62.134%, LR: 0.007034\n",
            "Validation - Loss: 1.042, Acc: 62.860%\n",
            "Epoch: 12, Batch: 0, Loss: 1.081, Acc: 61.719%, LR: 0.006545\n",
            "Epoch: 12, Batch: 100, Loss: 1.049, Acc: 62.624%, LR: 0.006545\n",
            "Epoch: 12, Batch: 200, Loss: 1.040, Acc: 62.830%, LR: 0.006545\n",
            "Epoch: 12, Batch: 300, Loss: 1.040, Acc: 62.837%, LR: 0.006545\n",
            "Validation - Loss: 1.023, Acc: 63.870%\n",
            "Epoch: 13, Batch: 0, Loss: 0.905, Acc: 65.625%, LR: 0.006040\n",
            "Epoch: 13, Batch: 100, Loss: 1.006, Acc: 63.451%, LR: 0.006040\n",
            "Epoch: 13, Batch: 200, Loss: 1.019, Acc: 63.444%, LR: 0.006040\n",
            "Epoch: 13, Batch: 300, Loss: 1.019, Acc: 63.684%, LR: 0.006040\n",
            "Validation - Loss: 0.981, Acc: 65.240%\n",
            "Epoch: 14, Batch: 0, Loss: 0.971, Acc: 65.625%, LR: 0.005523\n",
            "Epoch: 14, Batch: 100, Loss: 1.004, Acc: 64.302%, LR: 0.005523\n",
            "Epoch: 14, Batch: 200, Loss: 1.001, Acc: 64.560%, LR: 0.005523\n",
            "Epoch: 14, Batch: 300, Loss: 1.003, Acc: 64.291%, LR: 0.005523\n",
            "Validation - Loss: 0.972, Acc: 65.670%\n",
            "Epoch: 15, Batch: 0, Loss: 0.993, Acc: 60.156%, LR: 0.005000\n",
            "Epoch: 15, Batch: 100, Loss: 0.997, Acc: 64.472%, LR: 0.005000\n",
            "Epoch: 15, Batch: 200, Loss: 0.996, Acc: 64.350%, LR: 0.005000\n",
            "Epoch: 15, Batch: 300, Loss: 0.999, Acc: 64.332%, LR: 0.005000\n",
            "Validation - Loss: 0.952, Acc: 66.140%\n",
            "Epoch: 16, Batch: 0, Loss: 1.040, Acc: 65.625%, LR: 0.004477\n",
            "Epoch: 16, Batch: 100, Loss: 0.974, Acc: 65.169%, LR: 0.004477\n",
            "Epoch: 16, Batch: 200, Loss: 0.979, Acc: 64.984%, LR: 0.004477\n",
            "Epoch: 16, Batch: 300, Loss: 0.981, Acc: 65.163%, LR: 0.004477\n",
            "Validation - Loss: 0.960, Acc: 65.530%\n",
            "Epoch: 17, Batch: 0, Loss: 0.925, Acc: 64.844%, LR: 0.003960\n",
            "Epoch: 17, Batch: 100, Loss: 0.958, Acc: 65.308%, LR: 0.003960\n",
            "Epoch: 17, Batch: 200, Loss: 0.962, Acc: 65.559%, LR: 0.003960\n",
            "Epoch: 17, Batch: 300, Loss: 0.961, Acc: 65.664%, LR: 0.003960\n",
            "Validation - Loss: 0.955, Acc: 65.790%\n",
            "Epoch: 18, Batch: 0, Loss: 0.946, Acc: 70.312%, LR: 0.003455\n",
            "Epoch: 18, Batch: 100, Loss: 0.949, Acc: 65.865%, LR: 0.003455\n",
            "Epoch: 18, Batch: 200, Loss: 0.949, Acc: 66.290%, LR: 0.003455\n",
            "Epoch: 18, Batch: 300, Loss: 0.952, Acc: 66.004%, LR: 0.003455\n",
            "Validation - Loss: 0.933, Acc: 66.970%\n",
            "Epoch: 19, Batch: 0, Loss: 0.860, Acc: 71.875%, LR: 0.002966\n",
            "Epoch: 19, Batch: 100, Loss: 0.925, Acc: 67.265%, LR: 0.002966\n",
            "Epoch: 19, Batch: 200, Loss: 0.939, Acc: 66.651%, LR: 0.002966\n",
            "Epoch: 19, Batch: 300, Loss: 0.941, Acc: 66.533%, LR: 0.002966\n",
            "Validation - Loss: 0.940, Acc: 66.740%\n",
            "Epoch: 20, Batch: 0, Loss: 0.965, Acc: 64.062%, LR: 0.002500\n",
            "Epoch: 20, Batch: 100, Loss: 0.937, Acc: 66.476%, LR: 0.002500\n",
            "Epoch: 20, Batch: 200, Loss: 0.928, Acc: 66.799%, LR: 0.002500\n",
            "Epoch: 20, Batch: 300, Loss: 0.930, Acc: 66.666%, LR: 0.002500\n",
            "Validation - Loss: 0.921, Acc: 67.090%\n",
            "Epoch: 21, Batch: 0, Loss: 1.117, Acc: 53.125%, LR: 0.002061\n",
            "Epoch: 21, Batch: 100, Loss: 0.913, Acc: 66.878%, LR: 0.002061\n",
            "Epoch: 21, Batch: 200, Loss: 0.927, Acc: 66.542%, LR: 0.002061\n",
            "Epoch: 21, Batch: 300, Loss: 0.926, Acc: 66.700%, LR: 0.002061\n",
            "Validation - Loss: 0.924, Acc: 67.500%\n",
            "Epoch: 22, Batch: 0, Loss: 0.960, Acc: 66.406%, LR: 0.001654\n",
            "Epoch: 22, Batch: 100, Loss: 0.914, Acc: 67.257%, LR: 0.001654\n",
            "Epoch: 22, Batch: 200, Loss: 0.913, Acc: 67.456%, LR: 0.001654\n",
            "Epoch: 22, Batch: 300, Loss: 0.912, Acc: 67.419%, LR: 0.001654\n",
            "Validation - Loss: 0.912, Acc: 67.540%\n",
            "Epoch: 23, Batch: 0, Loss: 0.897, Acc: 70.312%, LR: 0.001284\n",
            "Epoch: 23, Batch: 100, Loss: 0.902, Acc: 68.031%, LR: 0.001284\n",
            "Epoch: 23, Batch: 200, Loss: 0.902, Acc: 67.837%, LR: 0.001284\n",
            "Epoch: 23, Batch: 300, Loss: 0.901, Acc: 67.805%, LR: 0.001284\n",
            "Validation - Loss: 0.899, Acc: 68.310%\n",
            "Epoch: 24, Batch: 0, Loss: 1.005, Acc: 65.625%, LR: 0.000955\n",
            "Epoch: 24, Batch: 100, Loss: 0.907, Acc: 67.752%, LR: 0.000955\n",
            "Epoch: 24, Batch: 200, Loss: 0.894, Acc: 68.183%, LR: 0.000955\n",
            "Epoch: 24, Batch: 300, Loss: 0.894, Acc: 68.176%, LR: 0.000955\n",
            "Validation - Loss: 0.896, Acc: 68.230%\n",
            "Epoch: 25, Batch: 0, Loss: 0.770, Acc: 70.312%, LR: 0.000670\n",
            "Epoch: 25, Batch: 100, Loss: 0.908, Acc: 68.031%, LR: 0.000670\n",
            "Epoch: 25, Batch: 200, Loss: 0.894, Acc: 68.315%, LR: 0.000670\n",
            "Epoch: 25, Batch: 300, Loss: 0.894, Acc: 68.169%, LR: 0.000670\n",
            "Validation - Loss: 0.892, Acc: 68.750%\n",
            "Epoch: 26, Batch: 0, Loss: 0.919, Acc: 64.844%, LR: 0.000432\n",
            "Epoch: 26, Batch: 100, Loss: 0.896, Acc: 67.938%, LR: 0.000432\n",
            "Epoch: 26, Batch: 200, Loss: 0.895, Acc: 67.992%, LR: 0.000432\n",
            "Epoch: 26, Batch: 300, Loss: 0.893, Acc: 68.054%, LR: 0.000432\n",
            "Validation - Loss: 0.890, Acc: 68.630%\n",
            "Epoch: 27, Batch: 0, Loss: 0.810, Acc: 70.312%, LR: 0.000245\n",
            "Epoch: 27, Batch: 100, Loss: 0.879, Acc: 69.028%, LR: 0.000245\n",
            "Epoch: 27, Batch: 200, Loss: 0.880, Acc: 69.084%, LR: 0.000245\n",
            "Epoch: 27, Batch: 300, Loss: 0.883, Acc: 68.802%, LR: 0.000245\n",
            "Validation - Loss: 0.888, Acc: 68.650%\n",
            "Epoch: 28, Batch: 0, Loss: 0.746, Acc: 75.781%, LR: 0.000109\n",
            "Epoch: 28, Batch: 100, Loss: 0.894, Acc: 68.147%, LR: 0.000109\n",
            "Epoch: 28, Batch: 200, Loss: 0.890, Acc: 68.338%, LR: 0.000109\n",
            "Epoch: 28, Batch: 300, Loss: 0.883, Acc: 68.529%, LR: 0.000109\n",
            "Validation - Loss: 0.888, Acc: 68.680%\n",
            "Epoch: 29, Batch: 0, Loss: 0.885, Acc: 70.312%, LR: 0.000027\n",
            "Epoch: 29, Batch: 100, Loss: 0.881, Acc: 68.425%, LR: 0.000027\n",
            "Epoch: 29, Batch: 200, Loss: 0.882, Acc: 68.466%, LR: 0.000027\n",
            "Epoch: 29, Batch: 300, Loss: 0.882, Acc: 68.275%, LR: 0.000027\n",
            "Validation - Loss: 0.888, Acc: 68.710%\n",
            "Test - Loss: 0.892, Acc: 68.750%\n",
            "Training completed for cifar10 seed 0. Best validation accuracy: 68.75%, Test accuracy: 68.75%\n",
            "All results saved to run_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cbdgfsjQRoGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from typing import Callable, List, Optional, Union, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# _make_divisible function from torchvision\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function ensures that all layers have a channel number that is divisible by 8.\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that rounding down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "# Squeeze-and-Excitation block\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_channels: int,\n",
        "            squeeze_channels: int,\n",
        "            activation: Callable[..., nn.Module] = nn.ReLU,\n",
        "            scale_activation: Callable[..., nn.Module] = nn.Hardsigmoid,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
        "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
        "        self.activation = activation(inplace=True)\n",
        "        self.scale_activation = scale_activation(inplace=True)\n",
        "\n",
        "    def _scale(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        scale = self.avgpool(input)\n",
        "        scale = self.fc1(scale)\n",
        "        scale = self.activation(scale)\n",
        "        scale = self.fc2(scale)\n",
        "        scale = self.scale_activation(scale)\n",
        "        return scale\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        scale = self._scale(input)\n",
        "        return input * scale\n",
        "\n",
        "\n",
        "# ConvNormActivation block\n",
        "class ConvNormActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: Union[int, Tuple[int]] = 3,\n",
        "            stride: Union[int, Tuple[int]] = 1,\n",
        "            padding: Optional[Union[int, Tuple[int], str]] = None,\n",
        "            groups: int = 1,\n",
        "            norm_layer: Optional[Callable[..., nn.Module]] = nn.BatchNorm2d,\n",
        "            activation_layer: Optional[Callable[..., nn.Module]] = nn.ReLU,\n",
        "            dilation: Union[int, Tuple[int]] = 1,\n",
        "            bias: Optional[bool] = None,\n",
        "    ) -> None:\n",
        "\n",
        "        if padding is None:\n",
        "            if isinstance(kernel_size, int):\n",
        "                padding = (kernel_size - 1) // 2 * dilation\n",
        "            else:\n",
        "                padding = tuple((k - 1) // 2 * d for k, d in zip(kernel_size, dilation))\n",
        "        if bias is None:\n",
        "            bias = norm_layer is None\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                dilation=dilation,\n",
        "                groups=groups,\n",
        "                bias=bias,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if norm_layer is not None:\n",
        "            layers.append(norm_layer(out_channels))\n",
        "        if activation_layer is not None:\n",
        "            layers.append(activation_layer(inplace=True))\n",
        "        super().__init__(*layers)\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "\n",
        "# InvertedResidualConfig class\n",
        "class InvertedResidualConfig:\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_channels: int,\n",
        "            kernel: int,\n",
        "            expanded_channels: int,\n",
        "            out_channels: int,\n",
        "            use_se: bool,\n",
        "            activation: str,\n",
        "            stride: int,\n",
        "            dilation: int,\n",
        "            width_mult: float,\n",
        "    ):\n",
        "        self.input_channels = self.adjust_channels(input_channels, width_mult)\n",
        "        self.kernel = kernel\n",
        "        self.expanded_channels = self.adjust_channels(expanded_channels, width_mult)\n",
        "        self.out_channels = self.adjust_channels(out_channels, width_mult)\n",
        "        self.use_se = use_se\n",
        "        self.activation = activation\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "\n",
        "    @staticmethod\n",
        "    def adjust_channels(channels: int, width_mult: float):\n",
        "        return _make_divisible(channels * width_mult, 8)\n",
        "\n",
        "\n",
        "# InvertedResidual block\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            cnf: InvertedResidualConfig,\n",
        "            norm_layer: Callable[..., nn.Module],\n",
        "            se_layer: Callable[..., nn.Module] = partial(SqueezeExcitation, scale_activation=nn.Hardsigmoid),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not (1 <= cnf.stride <= 2):\n",
        "            raise ValueError(\"Illegal stride value\")\n",
        "\n",
        "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        activation_layer = nn.Hardswish if cnf.activation == \"HS\" else nn.ReLU\n",
        "\n",
        "        # Expand phase\n",
        "        if cnf.expanded_channels != cnf.input_channels:\n",
        "            layers.append(\n",
        "                ConvNormActivation(\n",
        "                    cnf.input_channels,\n",
        "                    cnf.expanded_channels,\n",
        "                    kernel_size=1,\n",
        "                    norm_layer=norm_layer,\n",
        "                    activation_layer=activation_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Depthwise convolution\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                cnf.expanded_channels,\n",
        "                cnf.expanded_channels,\n",
        "                kernel_size=cnf.kernel,\n",
        "                stride=cnf.stride,\n",
        "                groups=cnf.expanded_channels,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=activation_layer,\n",
        "                dilation=cnf.dilation,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        if cnf.use_se:\n",
        "            squeeze_channels = _make_divisible(cnf.expanded_channels // 4, 8)\n",
        "            layers.append(\n",
        "                se_layer(\n",
        "                    cnf.expanded_channels,\n",
        "                    squeeze_channels,\n",
        "                    activation=nn.ReLU,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Project phase\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                cnf.expanded_channels,\n",
        "                cnf.out_channels,\n",
        "                kernel_size=1,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "        self.out_channels = cnf.out_channels\n",
        "        self.is_strided = cnf.stride > 1\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        result = self.block(input)\n",
        "        if self.use_res_connect:\n",
        "            return input + result\n",
        "        else:\n",
        "            return result\n",
        "\n",
        "\n",
        "# MobileNetV3 Small model\n",
        "class MobileNetV3Small(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_classes: int = 1000,\n",
        "            width_mult: float = 1.0,\n",
        "            dropout: float = 0.2,\n",
        "            reduced_tail: bool = False,\n",
        "            dilated: bool = False,\n",
        "            norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "\n",
        "        bneck_conf = partial(InvertedResidualConfig, width_mult=width_mult)\n",
        "\n",
        "        # Build inverted residual setting\n",
        "        reduce_divider = 2 if reduced_tail else 1\n",
        "        dilation = 2 if dilated else 1\n",
        "\n",
        "        inverted_residual_setting = [\n",
        "            # input_c, kernel, exp_c, out_c, se, nl, s, d\n",
        "            bneck_conf(16, 3, 16, 16, True, \"RE\", 2, 1),\n",
        "            bneck_conf(16, 3, 72, 24, False, \"RE\", 2, 1),\n",
        "            bneck_conf(24, 3, 88, 24, False, \"RE\", 1, 1),\n",
        "            bneck_conf(24, 5, 96, 40, True, \"HS\", 2, 1),\n",
        "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
        "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
        "            bneck_conf(40, 5, 120, 48, True, \"HS\", 1, 1),\n",
        "            bneck_conf(48, 5, 144, 48, True, \"HS\", 1, 1),\n",
        "            bneck_conf(48, 5, 288 // reduce_divider, 96 // reduce_divider, True, \"HS\", 2, dilation),\n",
        "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
        "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
        "        ]\n",
        "\n",
        "        last_channel = _make_divisible(1024 // reduce_divider * width_mult, 8)\n",
        "\n",
        "        # First layer\n",
        "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                3,\n",
        "                firstconv_output_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=nn.Hardswish,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Building inverted residual blocks\n",
        "        for cnf in inverted_residual_setting:\n",
        "            layers.append(InvertedResidual(cnf, norm_layer))\n",
        "\n",
        "        # Building last several layers\n",
        "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
        "        lastconv_output_channels = _make_divisible(576 * width_mult, 8)\n",
        "        layers.append(\n",
        "            ConvNormActivation(\n",
        "                lastconv_input_channels,\n",
        "                lastconv_output_channels,\n",
        "                kernel_size=1,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=nn.Hardswish,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lastconv_output_channels, last_channel),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Dropout(p=dropout, inplace=True),\n",
        "            nn.Linear(last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.SyncBatchNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Function to create the model and load pretrained weights\n",
        "def mobilenet_v3_small(pretrained=False, progress=True, **kwargs):\n",
        "    model = MobileNetV3Small(**kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        # Load the torchvision model with pretrained weights\n",
        "        from torchvision.models import mobilenet_v3_small as tv_mobilenet_v3_small\n",
        "        from torchvision.models import MobileNet_V3_Small_Weights\n",
        "\n",
        "        # Check for number of classes\n",
        "        if kwargs.get('num_classes', 1000) != 1000:\n",
        "            # We cannot load the classifier weights (different classes)\n",
        "            pretrained_model = tv_mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT, progress=progress)\n",
        "            pretrained_state_dict = pretrained_model.state_dict()\n",
        "            # Remove classifier weights\n",
        "            pretrained_state_dict = {k: v for k, v in pretrained_state_dict.items() if not k.startswith('classifier')}\n",
        "            model_dict = model.state_dict()\n",
        "            print(model_dict.keys())\n",
        "            # Update the model dict\n",
        "            model_dict.update(pretrained_state_dict)\n",
        "            model.load_state_dict(model_dict)\n",
        "        else:\n",
        "            # Load all weights\n",
        "            pretrained_model = tv_mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT, progress=progress)\n",
        "            model.load_state_dict(pretrained_model.state_dict())\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # data\n",
        "    data_path: str = './data'\n",
        "    dataset: str = 'cifar10'\n",
        "    num_classes: int = 10\n",
        "    # model\n",
        "    model: str = 'mobilenet_v3_small'\n",
        "    # training\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 0.01\n",
        "    weight_decay: float = 1e-4\n",
        "    epochs: int = 2\n",
        "    # system\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    num_workers: int = 2\n",
        "    # logging\n",
        "    log_interval: int = 100\n",
        "    eval_interval: int = 1000\n",
        "    # output\n",
        "    out_dir: str = 'run_0'\n",
        "    seed: int = 0\n",
        "    # compile for SPEED!\n",
        "    compile_model: bool = False\n",
        "\n",
        "\n",
        "def get_data_loaders(config):\n",
        "    if config.dataset == 'cifar10':\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "\n",
        "        train_dataset = datasets.CIFAR10(root=config.data_path, train=True, download=True, transform=transform_train)\n",
        "        test_dataset = datasets.CIFAR10(root=config.data_path, train=False, download=True, transform=transform_test)\n",
        "    elif config.dataset == 'cifar100':\n",
        "        # Placeholder for CIFAR-100 (for future use)\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
        "                                 (0.2675, 0.2565, 0.2761)),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
        "                                 (0.2675, 0.2565, 0.2761)),\n",
        "        ])\n",
        "\n",
        "        train_dataset = datasets.CIFAR100(root=config.data_path, train=True, download=True, transform=transform_train)\n",
        "        test_dataset = datasets.CIFAR100(root=config.data_path, train=False, download=True, transform=transform_test)\n",
        "        config.num_classes = 100  # Update number of classes for CIFAR-100\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {config.dataset}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def train(config):\n",
        "    # Set random seeds\n",
        "    torch.manual_seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    random.seed(config.seed)\n",
        "    if config.device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(config.seed)\n",
        "\n",
        "    model = mobilenet_v3_small(pretrained=False, progress=True, num_classes=config.num_classes).to(config.device)\n",
        "\n",
        "    if config.compile_model:\n",
        "        print(\"Compiling the model...\")\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=config.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
        "\n",
        "    train_loader, test_loader = get_data_loaders(config)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    train_log_info = []\n",
        "    val_log_info = []\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if batch_idx % config.log_interval == 0:\n",
        "                train_log_info.append({\n",
        "                    'epoch': epoch,\n",
        "                    'batch': batch_idx,\n",
        "                    'loss': train_loss / (batch_idx + 1),\n",
        "                    'acc': 100. * train_correct / train_total,\n",
        "                    'lr': optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {train_loss / (batch_idx + 1):.3f}, '\n",
        "                      f'Acc: {100. * train_correct / train_total:.3f}%, '\n",
        "                      f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion, config)\n",
        "        val_log_info.append({\n",
        "            'epoch': epoch,\n",
        "            'loss': val_loss,\n",
        "            'acc': val_acc\n",
        "        })\n",
        "        print(f'Validation - Loss: {val_loss:.3f}, Acc: {val_acc:.3f}%')\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), os.path.join(config.out_dir, 'best_model.pth'))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_log_info, val_log_info, best_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, config):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(config.device), targets.to(config.device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += targets.size(0)\n",
        "            val_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(dataloader)\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "\n",
        "def test(config):\n",
        "    model = MobileNetV3Small(num_classes=config.num_classes).to(config.device)\n",
        "    if config.compile_model:\n",
        "        print(\"Compiling the model for testing...\")\n",
        "        model = torch.compile(model)\n",
        "    model.load_state_dict(torch.load(os.path.join(config.out_dir, 'best_model.pth')))\n",
        "    _, test_loader = get_data_loaders(config)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, config)\n",
        "    print(f'Test - Loss: {test_loss:.3f}, Acc: {test_acc:.3f}%')\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train MobileNetV3 for Image Classification\")\n",
        "    parser.add_argument(\"--data_path\", type=str, default=\"./data\", help=\"Path to save/load the dataset\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"Batch size\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"Initial learning rate\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of epochs to train\")\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"run_0\", help=\"Output directory\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    print(f\"Outputs will be saved to {args.out_dir}\")\n",
        "\n",
        "    # Define datasets and number of seeds per dataset\n",
        "    datasets = ['cifar10']  # For now, only CIFAR-10; can add 'cifar100' in the future\n",
        "    num_seeds = {\n",
        "        'cifar10': 1  # Change the number of seeds as desired\n",
        "    }\n",
        "\n",
        "    all_results = {}\n",
        "    final_infos = {}\n",
        "\n",
        "    for dataset in datasets:\n",
        "        final_info_list = []\n",
        "        for seed_offset in range(num_seeds[dataset]):\n",
        "            # Update the config for each run\n",
        "            config = Config(\n",
        "                data_path=args.data_path,\n",
        "                dataset=dataset,\n",
        "                batch_size=args.batch_size,\n",
        "                learning_rate=args.learning_rate,\n",
        "                epochs=args.epochs,\n",
        "                out_dir=args.out_dir,\n",
        "                seed=seed_offset  # Set the seed\n",
        "            )\n",
        "            os.makedirs(config.out_dir, exist_ok=True)\n",
        "            print(f\"Starting training for {dataset} with seed {seed_offset}\")\n",
        "            start_time = time.time()\n",
        "            train_log_info, val_log_info, best_acc = train(config)\n",
        "            total_time = time.time() - start_time\n",
        "\n",
        "            # Run test after training\n",
        "            test_loss, test_acc = test(config)\n",
        "\n",
        "            # Prepare final_info dictionary\n",
        "            final_info = {\n",
        "                \"best_val_acc\": best_acc,\n",
        "                \"test_acc\": test_acc,\n",
        "                \"total_train_time\": total_time,\n",
        "                \"config\": vars(config)\n",
        "            }\n",
        "            final_info_list.append(final_info)\n",
        "\n",
        "            # Store results in all_results\n",
        "            key_prefix = f\"{dataset}_{seed_offset}\"\n",
        "            all_results[f\"{key_prefix}_final_info\"] = final_info\n",
        "            all_results[f\"{key_prefix}_train_log_info\"] = train_log_info\n",
        "            all_results[f\"{key_prefix}_val_log_info\"] = val_log_info\n",
        "\n",
        "            print(f\"Training completed for {dataset} seed {seed_offset}. Best validation accuracy: {best_acc:.2f}%, Test accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        # Aggregate results over seeds\n",
        "        final_info_dict = {k: [d[k] for d in final_info_list if k in d] for k in final_info_list[0].keys()}\n",
        "        means = {f\"{k}_mean\": np.mean(v) for k, v in final_info_dict.items() if isinstance(v[0], (int, float, float))}\n",
        "        stderrs = {f\"{k}_stderr\": np.std(v) / np.sqrt(len(v)) for k, v in final_info_dict.items() if isinstance(v[0], (int, float, float))}\n",
        "        final_infos[dataset] = {\n",
        "            \"means\": means,\n",
        "            \"stderrs\": stderrs,\n",
        "            \"final_info_dict\": final_info_dict\n",
        "        }\n",
        "\n",
        "    # Save final_infos to final_info.json\n",
        "    with open(os.path.join(args.out_dir, \"final_info.json\"), \"w\") as f:\n",
        "        json.dump(final_infos, f, indent=2)\n",
        "\n",
        "    # Save all_results to all_results.npy\n",
        "    with open(os.path.join(args.out_dir, \"all_results.npy\"), \"wb\") as f:\n",
        "        np.save(f, all_results)\n",
        "\n",
        "    print(f\"All results saved to {args.out_dir}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "f40f86cd-f756-4cdd-da21-a99786dc8d26",
        "id": "OMTpBpThRpzf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--data_path DATA_PATH] [--batch_size BATCH_SIZE]\n",
            "                                [--learning_rate LEARNING_RATE] [--epochs EPOCHS]\n",
            "                                [--out_dir OUT_DIR]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-b10069c2-92c5-4d12-8ca0-edb13fb0b8c7.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NanoGPT**\n",
        "\n",
        "**Description:** This template investigates transformer-based autoregressive next-token prediction tasks."
      ],
      "metadata": {
        "id": "1w87MoAKPwIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prepare the data"
      ],
      "metadata": {
        "id": "_Ui8bNsVPwIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare the data\n",
        "\n",
        "!python data/enwik8/prepare.py\n",
        "!python data/shakespeare_char/prepare.py\n",
        "!python data/text8/prepare.py"
      ],
      "metadata": {
        "id": "tl8KeiygPwIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create baseline runs (machine dependent)"
      ],
      "metadata": {
        "id": "gfbzNk6zPwIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI-Scientist/templates/nanoGPT\n",
        "!python experiment.py"
      ],
      "metadata": {
        "id": "ybfGkwvSPwIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2D Diffussion**\n",
        "\n",
        "Description: This template studies improving the performance of diffusion generative models on low-dimensional datasets."
      ],
      "metadata": {
        "id": "EBtTx0Q0jsST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install dependencies:"
      ],
      "metadata": {
        "id": "McfKJWYaunq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up 2D Diffusion\n",
        "!git clone https://github.com/gregversteeg/NPEET.git\n",
        "!cd NPEET\n",
        "#!pip install .\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_Gp4iYfjr6T",
        "outputId": "ac158e96-370e-4972-a9dd-676fce2fbf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NPEET'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 129 (delta 21), reused 35 (delta 19), pack-reused 87 (from 1)\u001b[K\n",
            "Receiving objects: 100% (129/129), 317.14 KiB | 15.10 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create baseline runs:"
      ],
      "metadata": {
        "id": "UYgciyzYknAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkL-AmyG_U0F",
        "outputId": "77a256ef-4ac8-4e94-cebb-25323a7d09a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasaurusDozen.tsv  ema_pytorch.py  ideas.json  plot.py      seed_ideas.json\n",
            "datasets.py          experiment.py   \u001b[0m\u001b[01;34mlatex\u001b[0m/      prompt.json  train_loss.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up 2D Diffusion baseline run\n",
        "\n",
        "%cd /content/AI-Scientist/templates/2d_diffusion\n",
        "#%cd templates/2d_diffusion\n",
        "!python experiment.py --out_dir run_0\n",
        "!python plot.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o4XdPdqux7v",
        "outputId": "865d0af5-0458-48dd-d102-9c70b8f2685b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AI-Scientist/templates/2d_diffusion\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AI-Scientist/templates/2d_diffusion/experiment.py\", line 10, in <module>\n",
            "    import npeet.entropy_estimators as ee\n",
            "ModuleNotFoundError: No module named 'npeet'\n",
            "Figure(1400x800)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AI-Scientist/templates/2d_diffusion/plot.py\", line 81, in <module>\n",
            "    fig, axs = plt.subplots(num_runs, 4, figsize=(14, 3 * num_runs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\", line 1776, in subplots\n",
            "    axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/figure.py\", line 918, in subplots\n",
            "    gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/figure.py\", line 1600, in add_gridspec\n",
            "    gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/gridspec.py\", line 363, in __init__\n",
            "    super().__init__(nrows, ncols,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/gridspec.py\", line 48, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Number of rows must be a positive integer, not 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grokking**\n",
        "\n",
        "Description: This template investigates questions about generalization and learning speed in deep neural networks."
      ],
      "metadata": {
        "id": "tewoMBa0wUaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install dependencies:"
      ],
      "metadata": {
        "id": "zaWOhD-YwUaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Grokking\n",
        "\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930c6e43-3819-4571-dfb7-cf38fb5c7448",
        "id": "71zOLTqqwUaE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create baseline runs:"
      ],
      "metadata": {
        "id": "XULjC7HfwUaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up 2D Diffusion baseline run\n",
        "\n",
        "%cd /content/AI-Scientist/templates/grokking\n",
        "\n",
        "!python experiment.py --out_dir run_0\n",
        "!python plot.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqC4xf_AwUaF",
        "outputId": "66d42af4-eb0f-45ea-d76c-7088eb718d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AI-Scientist/templates/grokking\n",
            "Running x_div_y with seed offset 0\n",
            "{'final_train_loss': 0.0043488978408277035, 'final_val_loss': 0.00575057789683342, 'final_train_acc': 1.0, 'final_val_acc': 1.0, 'step_val_acc_99': 4470}\n",
            "Running x_div_y with seed offset 1\n",
            "{'final_train_loss': 0.005255431402474642, 'final_val_loss': 0.006333010271191597, 'final_train_acc': 1.0, 'final_val_acc': 1.0, 'step_val_acc_99': 4200}\n",
            "Running x_div_y with seed offset 2\n",
            "{'final_train_loss': 1.6195591688156128, 'final_val_loss': 0.6360082030296326, 'final_train_acc': 0.644335925579071, 'final_val_acc': 0.873779296875, 'step_val_acc_99': 5380}\n",
            "Running x_minus_y with seed offset 0\n",
            "{'final_train_loss': 0.004558430518954992, 'final_val_loss': 0.005506541114300489, 'final_train_acc': 1.0, 'final_val_acc': 1.0, 'step_val_acc_99': 4210}\n",
            "Running x_minus_y with seed offset 1\n",
            "{'final_train_loss': 0.047082994133234024, 'final_val_loss': 0.054392553865909576, 'final_train_acc': 0.999218761920929, 'final_val_acc': 0.9970703125, 'step_val_acc_99': 5410}\n",
            "Running x_minus_y with seed offset 2\n",
            "{'final_train_loss': 0.008925261907279491, 'final_val_loss': 0.016474248841404915, 'final_train_acc': 1.0, 'final_val_acc': 1.0, 'step_val_acc_99': 5620}\n",
            "Running x_plus_y with seed offset 0\n",
            "{'final_train_loss': 1.8999789953231812, 'final_val_loss': 0.7317614555358887, 'final_train_acc': 0.601367175579071, 'final_val_acc': 0.8935546875, 'step_val_acc_99': 3730}\n",
            "Running x_plus_y with seed offset 1\n",
            "{'final_train_loss': 0.004320650827139616, 'final_val_loss': 0.00445975549519062, 'final_train_acc': 1.0, 'final_val_acc': 1.0, 'step_val_acc_99': 1790}\n",
            "Running x_plus_y with seed offset 2\n",
            "{'final_train_loss': 0.005429127253592014, 'final_val_loss': 0.005694805644452572, 'final_train_acc': 1.0, 'final_val_acc': 1.0, 'step_val_acc_99': 1960}\n",
            "Running permutation with seed offset 0\n",
            "{'final_train_loss': 0.0038787114899605513, 'final_val_loss': 0.383646160364151, 'final_train_acc': 1.0, 'final_val_acc': 0.91162109375, 'step_val_acc_99': 7500}\n",
            "Running permutation with seed offset 1\n",
            "{'final_train_loss': 0.01667962782084942, 'final_val_loss': 7.446049213409424, 'final_train_acc': 0.999804675579071, 'final_val_acc': 0.0185546875, 'step_val_acc_99': 7500}\n",
            "Running permutation with seed offset 2\n",
            "{'final_train_loss': 0.010082538239657879, 'final_val_loss': 8.043066024780273, 'final_train_acc': 1.0, 'final_val_acc': 0.01025390625, 'step_val_acc_99': 7500}\n",
            "dict_keys(['x_div_y_0_final_info', 'x_div_y_0_train_info', 'x_div_y_0_val_info', 'x_div_y_1_final_info', 'x_div_y_1_train_info', 'x_div_y_1_val_info', 'x_div_y_2_final_info', 'x_div_y_2_train_info', 'x_div_y_2_val_info', 'x_minus_y_0_final_info', 'x_minus_y_0_train_info', 'x_minus_y_0_val_info', 'x_minus_y_1_final_info', 'x_minus_y_1_train_info', 'x_minus_y_1_val_info', 'x_minus_y_2_final_info', 'x_minus_y_2_train_info', 'x_minus_y_2_val_info', 'x_plus_y_0_final_info', 'x_plus_y_0_train_info', 'x_plus_y_0_val_info', 'x_plus_y_1_final_info', 'x_plus_y_1_train_info', 'x_plus_y_1_val_info', 'x_plus_y_2_final_info', 'x_plus_y_2_train_info', 'x_plus_y_2_val_info', 'permutation_0_final_info', 'permutation_0_train_info', 'permutation_0_val_info', 'permutation_1_final_info', 'permutation_1_train_info', 'permutation_1_val_info', 'permutation_2_final_info', 'permutation_2_train_info', 'permutation_2_val_info'])\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform_experiments.py\n",
        "\n",
        "import json\n",
        "import os.path as osp\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from subprocess import TimeoutExpired\n",
        "\n",
        "MAX_ITERS = 4\n",
        "MAX_RUNS = 5\n",
        "MAX_STDERR_OUTPUT = 1500\n",
        "\n",
        "coder_prompt = \"\"\"Your goal is to implement the following idea: {title}.\n",
        "The proposed experiment is as follows: {idea}.\n",
        "You are given a total of up to {max_runs} runs to complete the necessary experiments. You do not need to use all {max_runs}.\n",
        "\n",
        "First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.\n",
        "\n",
        "Note that we already provide the vanilla baseline results, so you do not need to re-run it.\n",
        "\n",
        "For reference, the baseline results are as follows:\n",
        "\n",
        "{baseline_results}\n",
        "\n",
        "After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.\n",
        "YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.\n",
        "You can then implement the next thing on your list.\"\"\"\n",
        "\n",
        "\n",
        "# RUN EXPERIMENT\n",
        "def run_experiment(folder_name, run_num, timeout=7200):\n",
        "    cwd = osp.abspath(folder_name)\n",
        "    # COPY CODE SO WE CAN SEE IT.\n",
        "    shutil.copy(\n",
        "        osp.join(folder_name, \"experiment.py\"),\n",
        "        osp.join(folder_name, f\"run_{run_num}.py\"),\n",
        "    )\n",
        "\n",
        "    # LAUNCH COMMAND\n",
        "    command = [\n",
        "        \"python\",\n",
        "        \"experiment.py\",\n",
        "        f\"--out_dir=run_{run_num}\",\n",
        "    ]\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command, cwd=cwd, stderr=subprocess.PIPE, text=True, timeout=timeout\n",
        "        )\n",
        "\n",
        "        if result.stderr:\n",
        "            print(result.stderr, file=sys.stderr)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Run {run_num} failed with return code {result.returncode}\")\n",
        "            if osp.exists(osp.join(cwd, f\"run_{run_num}\")):\n",
        "                shutil.rmtree(osp.join(cwd, f\"run_{run_num}\"))\n",
        "            print(f\"Run failed with the following error {result.stderr}\")\n",
        "            stderr_output = result.stderr\n",
        "            if len(stderr_output) > MAX_STDERR_OUTPUT:\n",
        "                stderr_output = \"...\" + stderr_output[-MAX_STDERR_OUTPUT:]\n",
        "            next_prompt = f\"Run failed with the following error {stderr_output}\"\n",
        "        else:\n",
        "            with open(osp.join(cwd, f\"run_{run_num}\", \"final_info.json\"), \"r\") as f:\n",
        "                results = json.load(f)\n",
        "            results = {k: v[\"means\"] for k, v in results.items()}\n",
        "\n",
        "            next_prompt = f\"\"\"Run {run_num} completed. Here are the results:\n",
        "{results}\n",
        "\n",
        "Decide if you need to re-plan your experiments given the result (you often will not need to).\n",
        "\n",
        "Someone else will be using `notes.txt` to perform a writeup on this in the future.\n",
        "Please include *all* relevant information for the writeup on Run {run_num}, including an experiment description and the run number. Be as verbose as necessary.\n",
        "\n",
        "Then, implement the next thing on your list.\n",
        "We will then run the command `python experiment.py --out_dir=run_{run_num + 1}'.\n",
        "YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.\n",
        "If you are finished with experiments, respond with 'ALL_COMPLETED'.\"\"\"\n",
        "        return result.returncode, next_prompt\n",
        "    except TimeoutExpired:\n",
        "        print(f\"Run {run_num} timed out after {timeout} seconds\")\n",
        "        if osp.exists(osp.join(cwd, f\"run_{run_num}\")):\n",
        "            shutil.rmtree(osp.join(cwd, f\"run_{run_num}\"))\n",
        "        next_prompt = f\"Run timed out after {timeout} seconds\"\n",
        "        return 1, next_prompt\n",
        "\n",
        "\n",
        "# RUN PLOTTING\n",
        "def run_plotting(folder_name, timeout=600):\n",
        "    cwd = osp.abspath(folder_name)\n",
        "    # LAUNCH COMMAND\n",
        "    command = [\n",
        "        \"python\",\n",
        "        \"plot.py\",\n",
        "    ]\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command, cwd=cwd, stderr=subprocess.PIPE, text=True, timeout=timeout\n",
        "        )\n",
        "\n",
        "        if result.stderr:\n",
        "            print(result.stderr, file=sys.stderr)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Plotting failed with return code {result.returncode}\")\n",
        "            next_prompt = f\"Plotting failed with the following error {result.stderr}\"\n",
        "        else:\n",
        "            next_prompt = \"\"\n",
        "        return result.returncode, next_prompt\n",
        "    except TimeoutExpired:\n",
        "        print(f\"Plotting timed out after {timeout} seconds\")\n",
        "        next_prompt = f\"Plotting timed out after {timeout} seconds\"\n",
        "        return 1, next_prompt\n",
        "\n",
        "\n",
        "# PERFORM EXPERIMENTS\n",
        "def perform_experiments(idea, folder_name, coder, baseline_results) -> bool:\n",
        "    ## RUN EXPERIMENT\n",
        "    current_iter = 0\n",
        "    run = 1\n",
        "    next_prompt = coder_prompt.format(\n",
        "        title=idea[\"Title\"],\n",
        "        idea=idea[\"Experiment\"],\n",
        "        max_runs=MAX_RUNS,\n",
        "        baseline_results=baseline_results,\n",
        "    )\n",
        "    while run < MAX_RUNS + 1:\n",
        "        if current_iter >= MAX_ITERS:\n",
        "            print(\"Max iterations reached\")\n",
        "            break\n",
        "        coder_out = coder.run(next_prompt)\n",
        "        print(coder_out)\n",
        "        if \"ALL_COMPLETED\" in coder_out:\n",
        "            break\n",
        "        return_code, next_prompt = run_experiment(folder_name, run)\n",
        "        if return_code == 0:\n",
        "            run += 1\n",
        "            current_iter = 0\n",
        "        current_iter += 1\n",
        "    if current_iter >= MAX_ITERS:\n",
        "        print(\"Not all experiments completed.\")\n",
        "        return False\n",
        "\n",
        "    current_iter = 0\n",
        "    next_prompt = \"\"\"\n",
        "Great job! Please modify `plot.py` to generate the most relevant plots for the final writeup.\n",
        "\n",
        "In particular, be sure to fill in the \"labels\" dictionary with the correct names for each run that you want to plot.\n",
        "\n",
        "Only the runs in the `labels` dictionary will be plotted, so make sure to include all relevant runs.\n",
        "\n",
        "We will be running the command `python plot.py` to generate the plots.\n",
        "\"\"\"\n",
        "    while True:\n",
        "        _ = coder.run(next_prompt)\n",
        "        return_code, next_prompt = run_plotting(folder_name)\n",
        "        current_iter += 1\n",
        "        if return_code == 0 or current_iter >= MAX_ITERS:\n",
        "            break\n",
        "    next_prompt = \"\"\"\n",
        "Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. Please do so in-depth.\n",
        "\n",
        "Somebody else will be using `notes.txt` to write a report on this in the future.\n",
        "\"\"\"\n",
        "    coder.run(next_prompt)\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "ZvWR_A6KPHsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wujSuT95O5-H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tWyBE-IO5rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run AI Scientist Paper Generation Experiments**"
      ],
      "metadata": {
        "id": "AJNuFPRRGELO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run paper generation\n",
        "\n",
        "!python launch_scientist.py --model \"gpt-4o-2024-05-13\" --experiment nanoGPT_lite --num-ideas 2\n",
        "#!python launch_scientist.py --model \"claude-3-5-sonnet-20241022\" --experiment nanoGPT_lite --num-ideas"
      ],
      "metadata": {
        "id": "1j68V8bXGD6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting an LLM-Generated Paper Review**\n"
      ],
      "metadata": {
        "id": "nex6pFyaGeG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install pymupdf\n",
        "!pip install pymupdf4llm\n",
        "!pip install backoff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGAme5bKKU8_",
        "outputId": "710f8cad-35b4-49dd-af51-4fa22f97c29b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pymupdf>=1.24.10 in /usr/local/lib/python3.11/dist-packages (from pymupdf4llm) (1.25.4)\n",
            "Downloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pymupdf4llm\n",
            "Successfully installed pymupdf4llm-0.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNNkoTZHM24m",
        "outputId": "66efc103-04c3-465d-aff8-848fed719bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
            "Downloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/243.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llm.py\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "import anthropic\n",
        "import backoff\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "\n",
        "MAX_NUM_TOKENS = 4096\n",
        "\n",
        "AVAILABLE_LLMS = [\n",
        "    # Anthropic models\n",
        "    \"claude-3-5-sonnet-20240620\",\n",
        "    \"claude-3-5-sonnet-20241022\",\n",
        "    # OpenAI models\n",
        "    \"gpt-4o-mini-2024-07-18\",\n",
        "    \"gpt-4o-2024-05-13\",\n",
        "    \"gpt-4o-2024-08-06\",\n",
        "    \"o1-preview-2024-09-12\",\n",
        "    \"o1-mini-2024-09-12\",\n",
        "    \"o1-2024-12-17\",\n",
        "    # OpenRouter models\n",
        "    \"llama3.1-405b\",\n",
        "    # Anthropic Claude models via Amazon Bedrock\n",
        "    \"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "    \"bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
        "    \"bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
        "    \"bedrock/anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    \"bedrock/anthropic.claude-3-opus-20240229-v1:0\",\n",
        "    # Anthropic Claude models Vertex AI\n",
        "    \"vertex_ai/claude-3-opus@20240229\",\n",
        "    \"vertex_ai/claude-3-5-sonnet@20240620\",\n",
        "    \"vertex_ai/claude-3-5-sonnet-v2@20241022\",\n",
        "    \"vertex_ai/claude-3-sonnet@20240229\",\n",
        "    \"vertex_ai/claude-3-haiku@20240307\",\n",
        "    # DeepSeek models\n",
        "    \"deepseek-chat\",\n",
        "    \"deepseek-coder\",\n",
        "    \"deepseek-reasoner\",\n",
        "    # Google Gemini models\n",
        "    \"gemini-1.5-flash\",\n",
        "    \"gemini-1.5-pro\",\n",
        "]\n",
        "\n",
        "\n",
        "# Get N responses from a single message, used for ensembling.\n",
        "@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APITimeoutError))\n",
        "def get_batch_responses_from_llm(\n",
        "        msg,\n",
        "        client,\n",
        "        model,\n",
        "        system_message,\n",
        "        print_debug=False,\n",
        "        msg_history=None,\n",
        "        temperature=0.75,\n",
        "        n_responses=1,\n",
        "):\n",
        "    if msg_history is None:\n",
        "        msg_history = []\n",
        "\n",
        "    if model in [\n",
        "        \"gpt-4o-2024-05-13\",\n",
        "        \"gpt-4o-mini-2024-07-18\",\n",
        "        \"gpt-4o-2024-08-06\",\n",
        "    ]:\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                *new_msg_history,\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=MAX_NUM_TOKENS,\n",
        "            n=n_responses,\n",
        "            stop=None,\n",
        "            seed=0,\n",
        "        )\n",
        "        content = [r.message.content for r in response.choices]\n",
        "        new_msg_history = [\n",
        "            new_msg_history + [{\"role\": \"assistant\", \"content\": c}] for c in content\n",
        "        ]\n",
        "    elif model == \"llama-3-1-405b-instruct\":\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-3.1-405b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                *new_msg_history,\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=MAX_NUM_TOKENS,\n",
        "            n=n_responses,\n",
        "            stop=None,\n",
        "        )\n",
        "        content = [r.message.content for r in response.choices]\n",
        "        new_msg_history = [\n",
        "            new_msg_history + [{\"role\": \"assistant\", \"content\": c}] for c in content\n",
        "        ]\n",
        "    else:\n",
        "        content, new_msg_history = [], []\n",
        "        for _ in range(n_responses):\n",
        "            c, hist = get_response_from_llm(\n",
        "                msg,\n",
        "                client,\n",
        "                model,\n",
        "                system_message,\n",
        "                print_debug=False,\n",
        "                msg_history=None,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            content.append(c)\n",
        "            new_msg_history.append(hist)\n",
        "\n",
        "    if print_debug:\n",
        "        print()\n",
        "        print(\"*\" * 20 + \" LLM START \" + \"*\" * 20)\n",
        "        for j, msg in enumerate(new_msg_history[0]):\n",
        "            print(f'{j}, {msg[\"role\"]}: {msg[\"content\"]}')\n",
        "        print(content)\n",
        "        print(\"*\" * 21 + \" LLM END \" + \"*\" * 21)\n",
        "        print()\n",
        "\n",
        "    return content, new_msg_history\n",
        "\n",
        "\n",
        "@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APITimeoutError))\n",
        "def get_response_from_llm(\n",
        "        msg,\n",
        "        client,\n",
        "        model,\n",
        "        system_message,\n",
        "        print_debug=False,\n",
        "        msg_history=None,\n",
        "        temperature=0.75,\n",
        "):\n",
        "    if msg_history is None:\n",
        "        msg_history = []\n",
        "\n",
        "    if \"claude\" in model:\n",
        "        new_msg_history = msg_history + [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": msg,\n",
        "                    }\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "        response = client.messages.create(\n",
        "            model=model,\n",
        "            max_tokens=MAX_NUM_TOKENS,\n",
        "            temperature=temperature,\n",
        "            system=system_message,\n",
        "            messages=new_msg_history,\n",
        "        )\n",
        "        content = response.content[0].text\n",
        "        new_msg_history = new_msg_history + [\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": content,\n",
        "                    }\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "    elif model in [\n",
        "        \"gpt-4o-2024-05-13\",\n",
        "        \"gpt-4o-mini-2024-07-18\",\n",
        "        \"gpt-4o-2024-08-06\",\n",
        "    ]:\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                *new_msg_history,\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=MAX_NUM_TOKENS,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "            seed=0,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]\n",
        "    elif model in [\"o1-preview-2024-09-12\", \"o1-mini-2024-09-12\"]:\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": system_message},\n",
        "                *new_msg_history,\n",
        "            ],\n",
        "            temperature=1,\n",
        "            max_completion_tokens=MAX_NUM_TOKENS,\n",
        "            n=1,\n",
        "            seed=0,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]\n",
        "    elif model in [\"meta-llama/llama-3.1-405b-instruct\", \"llama-3-1-405b-instruct\"]:\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-3.1-405b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                *new_msg_history,\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=MAX_NUM_TOKENS,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]\n",
        "    elif model in [\"deepseek-chat\", \"deepseek-coder\"]:\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                *new_msg_history,\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=MAX_NUM_TOKENS,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]\n",
        "    elif model in [\"deepseek-reasoner\"]:\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                *new_msg_history,\n",
        "            ],\n",
        "            n=1,\n",
        "            stop=None,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]\n",
        "    elif \"gemini\" in model:\n",
        "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]\n",
        "        gemini_contents = [{\"role\": \"system\", \"parts\": system_message}]\n",
        "        for m in new_msg_history:\n",
        "            gemini_contents.append({\"role\": m[\"role\"], \"parts\": m[\"content\"]})\n",
        "        response = client.generate_content(\n",
        "            contents=gemini_contents,\n",
        "            generation_config=GenerationConfig(\n",
        "                temperature=temperature,\n",
        "                max_output_tokens=MAX_NUM_TOKENS,\n",
        "                candidate_count=1,\n",
        "            ),\n",
        "        )\n",
        "        content = response.text\n",
        "        new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model} not supported.\")\n",
        "\n",
        "    if print_debug:\n",
        "        print()\n",
        "        print(\"*\" * 20 + \" LLM START \" + \"*\" * 20)\n",
        "        for j, msg in enumerate(new_msg_history):\n",
        "            print(f'{j}, {msg[\"role\"]}: {msg[\"content\"]}')\n",
        "        print(content)\n",
        "        print(\"*\" * 21 + \" LLM END \" + \"*\" * 21)\n",
        "        print()\n",
        "\n",
        "    return content, new_msg_history\n",
        "\n",
        "\n",
        "def extract_json_between_markers(llm_output):\n",
        "    # Regular expression pattern to find JSON content between ```json and ```\n",
        "    json_pattern = r\"```json(.*?)```\"\n",
        "    matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
        "\n",
        "    if not matches:\n",
        "        # Fallback: Try to find any JSON-like content in the output\n",
        "        json_pattern = r\"\\{.*?\\}\"\n",
        "        matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
        "\n",
        "    for json_string in matches:\n",
        "        json_string = json_string.strip()\n",
        "        try:\n",
        "            parsed_json = json.loads(json_string)\n",
        "            return parsed_json\n",
        "        except json.JSONDecodeError:\n",
        "            # Attempt to fix common JSON issues\n",
        "            try:\n",
        "                # Remove invalid control characters\n",
        "                json_string_clean = re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", json_string)\n",
        "                parsed_json = json.loads(json_string_clean)\n",
        "                return parsed_json\n",
        "            except json.JSONDecodeError:\n",
        "                continue  # Try next match\n",
        "\n",
        "    return None  # No valid JSON found\n",
        "\n",
        "\n",
        "def create_client(model):\n",
        "    if model.startswith(\"claude-\"):\n",
        "        print(f\"Using Anthropic API with model {model}.\")\n",
        "        return anthropic.Anthropic(), model\n",
        "    elif model.startswith(\"bedrock\") and \"claude\" in model:\n",
        "        client_model = model.split(\"/\")[-1]\n",
        "        print(f\"Using Amazon Bedrock with model {client_model}.\")\n",
        "        return anthropic.AnthropicBedrock(), client_model\n",
        "    elif model.startswith(\"vertex_ai\") and \"claude\" in model:\n",
        "        client_model = model.split(\"/\")[-1]\n",
        "        print(f\"Using Vertex AI with model {client_model}.\")\n",
        "        return anthropic.AnthropicVertex(), client_model\n",
        "    elif 'gpt' in model:\n",
        "        print(f\"Using OpenAI API with model {model}.\")\n",
        "        return openai.OpenAI(), model\n",
        "    elif model in [\"o1-preview-2024-09-12\", \"o1-mini-2024-09-12\"]:\n",
        "        print(f\"Using OpenAI API with model {model}.\")\n",
        "        return openai.OpenAI(), model\n",
        "    elif model in [\"deepseek-chat\", \"deepseek-reasoner\"]:\n",
        "        print(f\"Using OpenAI API with {model}.\")\n",
        "        return openai.OpenAI(\n",
        "            api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
        "            base_url=\"https://api.deepseek.com\"\n",
        "        ), model\n",
        "    elif model == \"llama3.1-405b\":\n",
        "        print(f\"Using OpenAI API with {model}.\")\n",
        "        return openai.OpenAI(\n",
        "            api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        "            base_url=\"https://openrouter.ai/api/v1\"\n",
        "        ), \"meta-llama/llama-3.1-405b-instruct\"\n",
        "    elif \"gemini\" in model:\n",
        "        print(f\"Using Google Generative AI with model {model}.\")\n",
        "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        client = genai.GenerativeModel(model)\n",
        "        return client, model\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model} not supported.\")"
      ],
      "metadata": {
        "id": "4_qA7wvPKnge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aHIitkjeNGEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform_writeup\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import os.path as osp\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "from ai_scientist.generate_ideas import search_for_papers\n",
        "from ai_scientist.llm import get_response_from_llm, extract_json_between_markers, create_client, AVAILABLE_LLMS\n",
        "\n",
        "\n",
        "# GENERATE LATEX\n",
        "def generate_latex(coder, folder_name, pdf_file, timeout=30, num_error_corrections=5):\n",
        "    folder = osp.abspath(folder_name)\n",
        "    cwd = osp.join(folder, \"latex\")  # Fixed potential issue with path\n",
        "    writeup_file = osp.join(cwd, \"template.tex\")\n",
        "\n",
        "    # Check all references are valid and in the references.bib file\n",
        "    with open(writeup_file, \"r\") as f:\n",
        "        tex_text = f.read()\n",
        "    cites = re.findall(r\"\\\\cite[a-z]*{([^}]*)}\", tex_text)\n",
        "    references_bib = re.search(\n",
        "        r\"\\\\begin{filecontents}{references.bib}(.*?)\\\\end{filecontents}\",\n",
        "        tex_text,\n",
        "        re.DOTALL,\n",
        "    )\n",
        "    if references_bib is None:\n",
        "        print(\"No references.bib found in template.tex\")\n",
        "        return\n",
        "    bib_text = references_bib.group(1)\n",
        "    cites = [cite.strip() for item in cites for cite in item.split(\",\")]\n",
        "    for cite in cites:\n",
        "        if cite not in bib_text:\n",
        "            print(f\"Reference {cite} not found in references.\")\n",
        "            prompt = f\"\"\"Reference {cite} not found in references.bib. Is this included under a different name?\n",
        "If so, please modify the citation in template.tex to match the name in references.bib at the top. Otherwise, remove the cite.\"\"\"\n",
        "            coder.run(prompt)\n",
        "\n",
        "    # Check all included figures are actually in the directory.\n",
        "    with open(writeup_file, \"r\") as f:\n",
        "        tex_text = f.read()\n",
        "    referenced_figs = re.findall(r\"\\\\includegraphics.*?{(.*?)}\", tex_text)\n",
        "    all_figs = [f for f in os.listdir(folder) if f.endswith(\".png\")]\n",
        "    for figure in referenced_figs:\n",
        "        if figure not in all_figs:\n",
        "            print(f\"Figure {figure} not found in directory.\")\n",
        "            prompt = f\"\"\"The image {figure} not found in the directory. The images in the directory are: {all_figs}.\n",
        "Please ensure that the figure is in the directory and that the filename is correct. Check the notes to see what each figure contains.\"\"\"\n",
        "            coder.run(prompt)\n",
        "\n",
        "    # Remove duplicate figures.\n",
        "    with open(writeup_file, \"r\") as f:\n",
        "        tex_text = f.read()\n",
        "    referenced_figs = re.findall(r\"\\\\includegraphics.*?{(.*?)}\", tex_text)\n",
        "    duplicates = {x for x in referenced_figs if referenced_figs.count(x) > 1}\n",
        "    if duplicates:\n",
        "        for dup in duplicates:\n",
        "            print(f\"Duplicate figure found: {dup}.\")\n",
        "            prompt = f\"\"\"Duplicate figures found: {dup}. Ensure any figure is only included once.\n",
        "If duplicated, identify the best location for the figure and remove any other.\"\"\"\n",
        "            coder.run(prompt)\n",
        "\n",
        "    # Remove duplicate section headers.\n",
        "    with open(writeup_file, \"r\") as f:\n",
        "        tex_text = f.read()\n",
        "    sections = re.findall(r\"\\\\section{([^}]*)}\", tex_text)\n",
        "    duplicates = {x for x in sections if sections.count(x) > 1}\n",
        "    if duplicates:\n",
        "        for dup in duplicates:\n",
        "            print(f\"Duplicate section header found: {dup}\")\n",
        "            prompt = f\"\"\"Duplicate section header found: {dup}. Ensure any section header is declared once.\n",
        "If duplicated, identify the best location for the section header and remove any other.\"\"\"\n",
        "            coder.run(prompt)\n",
        "\n",
        "    # Iteratively fix any LaTeX bugs\n",
        "    for i in range(num_error_corrections):\n",
        "        # Filter trivial bugs in chktex\n",
        "        check_output = os.popen(f\"chktex {writeup_file} -q -n2 -n24 -n13 -n1\").read()\n",
        "        if check_output:\n",
        "            prompt = f\"\"\"Please fix the following LaTeX errors in `template.tex` guided by the output of `chktek`:\n",
        "{check_output}.\n",
        "\n",
        "Make the minimal fix required and do not remove or change any packages.\n",
        "Pay attention to any accidental uses of HTML syntax, e.g. </end instead of \\\\end.\n",
        "\"\"\"\n",
        "            coder.run(prompt)\n",
        "        else:\n",
        "            break\n",
        "    compile_latex(cwd, pdf_file, timeout=timeout)\n",
        "\n",
        "\n",
        "def compile_latex(cwd, pdf_file, timeout=30):\n",
        "    print(\"GENERATING LATEX\")\n",
        "\n",
        "    commands = [\n",
        "        [\"pdflatex\", \"-interaction=nonstopmode\", \"template.tex\"],\n",
        "        [\"bibtex\", \"template\"],\n",
        "        [\"pdflatex\", \"-interaction=nonstopmode\", \"template.tex\"],\n",
        "        [\"pdflatex\", \"-interaction=nonstopmode\", \"template.tex\"],\n",
        "    ]\n",
        "\n",
        "    for command in commands:\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                command,\n",
        "                cwd=cwd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True,\n",
        "                timeout=timeout,\n",
        "            )\n",
        "            print(\"Standard Output:\\n\", result.stdout)\n",
        "            print(\"Standard Error:\\n\", result.stderr)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"Latex timed out after {timeout} seconds\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error running command {' '.join(command)}: {e}\")\n",
        "\n",
        "    print(\"FINISHED GENERATING LATEX\")\n",
        "\n",
        "    # Attempt to move the PDF to the desired location\n",
        "    try:\n",
        "        shutil.move(osp.join(cwd, \"template.pdf\"), pdf_file)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Failed to rename PDF.\")\n",
        "\n",
        "\n",
        "per_section_tips = {\n",
        "    \"Abstract\": \"\"\"\n",
        "- TL;DR of the paper\n",
        "- What are we trying to do and why is it relevant?\n",
        "- Why is this hard?\n",
        "- How do we solve it (i.e. our contribution!)\n",
        "- How do we verify that we solved it (e.g. Experiments and results)\n",
        "\n",
        "Please make sure the abstract reads smoothly and is well-motivated. This should be one continuous paragraph with no breaks between the lines.\n",
        "\"\"\",\n",
        "    \"Introduction\": \"\"\"\n",
        "- Longer version of the Abstract, i.e. of the entire paper\n",
        "- What are we trying to do and why is it relevant?\n",
        "- Why is this hard?\n",
        "- How do we solve it (i.e. our contribution!)\n",
        "- How do we verify that we solved it (e.g. Experiments and results)\n",
        "- New trend: specifically list your contributions as bullet points\n",
        "- Extra space? Future work!\n",
        "\"\"\",\n",
        "    \"Related Work\": \"\"\"\n",
        "- Academic siblings of our work, i.e. alternative attempts in literature at trying to solve the same problem.\n",
        "- Goal is to “Compare and contrast” - how does their approach differ in either assumptions or method? If their method is applicable to our Problem Setting I expect a comparison in the experimental section. If not, there needs to be a clear statement why a given method is not applicable.\n",
        "- Note: Just describing what another paper is doing is not enough. We need to compare and contrast.\n",
        "\"\"\",\n",
        "    \"Background\": \"\"\"\n",
        "- Academic Ancestors of our work, i.e. all concepts and prior work that are required for understanding our method.\n",
        "- Usually includes a subsection, Problem Setting, which formally introduces the problem setting and notation (Formalism) for our method. Highlights any specific assumptions that are made that are unusual.\n",
        "- Note: If our paper introduces a novel problem setting as part of its contributions, it's best to have a separate Section.\n",
        "\"\"\",\n",
        "    \"Method\": \"\"\"\n",
        "- What we do. Why we do it. All described using the general Formalism introduced in the Problem Setting and building on top of the concepts / foundations introduced in Background.\n",
        "\"\"\",\n",
        "    \"Experimental Setup\": \"\"\"\n",
        "- How do we test that our stuff works? Introduces a specific instantiation of the Problem Setting and specific implementation details of our Method for this Problem Setting.\n",
        "- Do not imagine unknown hardware details.\n",
        "- Includes a description of the dataset, evaluation metrics, important hyperparameters, and implementation details.\n",
        "\"\"\",\n",
        "    \"Results\": \"\"\"\n",
        "- Shows the results of running Method on our problem described in Experimental Setup.\n",
        "- Includes statements on hyperparameters and other potential issues of fairness.\n",
        "- Only includes results that have actually been run and saved in the logs. Do not hallucinate results that don't exist.\n",
        "- If results exist: compares to baselines and includes statistics and confidence intervals.\n",
        "- If results exist: includes ablation studies to show that specific parts of the method are relevant.\n",
        "- Discusses limitations of the method.\n",
        "- Make sure to include all the results from the experiments, and include all relevant figures.\n",
        "\"\"\",\n",
        "    \"Conclusion\": \"\"\"\n",
        "- Brief recap of the entire paper.\n",
        "- To keep going with the analogy, you can think of future work as (potential) academic offspring.\n",
        "\"\"\",\n",
        "}\n",
        "\n",
        "error_list = \"\"\"- Unenclosed math symbols\n",
        "- Only reference figures that exist in our directory\n",
        "- LaTeX syntax errors\n",
        "- Numerical results that do not come from explicit experiments and logs\n",
        "- Repeatedly defined figure labels\n",
        "- References to papers that are not in the .bib file, DO NOT ADD ANY NEW CITATIONS!\n",
        "- Unnecessary verbosity or repetition, unclear text\n",
        "- Results or insights in the `notes.txt` that have not yet need included\n",
        "- Any relevant figures that have not yet been included in the text\n",
        "- Closing any \\\\begin{{figure}} with a \\\\end{{figure}} and \\\\begin{{table}} with a \\\\end{{table}}, etc.\n",
        "- Duplicate headers, e.g. duplicated \\\\section{{Introduction}} or \\\\end{{document}}\n",
        "- Unescaped symbols, e.g. shakespeare_char should be shakespeare\\\\_char in text\n",
        "- Incorrect closing of environments, e.g. </end{{figure}}> instead of \\\\end{{figure}}\n",
        "\"\"\"\n",
        "\n",
        "refinement_prompt = (\n",
        "    \"\"\"Great job! Now criticize and refine only the {section} that you just wrote.\n",
        "Make this complete in this pass, do not leave any placeholders.\n",
        "\n",
        "Pay particular attention to fixing any errors such as:\n",
        "\"\"\"\n",
        "    + error_list\n",
        ")\n",
        "\n",
        "second_refinement_prompt = (\n",
        "    \"\"\"Criticize and refine the {section} only. Recall the advice:\n",
        "{tips}\n",
        "Make this complete in this pass, do not leave any placeholders.\n",
        "\n",
        "Pay attention to how it fits in with the rest of the paper.\n",
        "Identify any redundancies (e.g. repeated figures or repeated text), if there are any, decide where in the paper things should be cut.\n",
        "Identify where we can save space, and be more concise without weakening the message of the text.\n",
        "Fix any remaining errors as before:\n",
        "\"\"\"\n",
        "    + error_list\n",
        ")\n",
        "\n",
        "# CITATION HELPERS\n",
        "citation_system_msg = \"\"\"You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.\n",
        "You have already written an initial draft of the paper and now you are looking to add missing citations to related papers throughout the paper.\n",
        "The related work section already has some initial comments on which papers to add and discuss.\n",
        "\n",
        "Focus on completing the existing write-up and do not add entirely new elements unless necessary.\n",
        "Ensure every point in the paper is substantiated with sufficient evidence.\n",
        "Feel free to add more cites to a particular point if there is only one or two references.\n",
        "Ensure no paper is cited without a corresponding reference in the `references.bib` file.\n",
        "Ensure each paragraph of the related work has sufficient background, e.g. a few papers cited.\n",
        "You will be given access to the Semantic Scholar API, only add citations that you have found using the API.\n",
        "Aim to discuss a broad range of relevant papers, not just the most popular ones.\n",
        "Make sure not to copy verbatim from prior literature to avoid plagiarism.\n",
        "\n",
        "You will be prompted to give a precise description of where and how to add the cite, and a search query for the paper to be cited.\n",
        "Finally, you will select the most relevant cite from the search results (top 10 results will be shown).\n",
        "You will have {total_rounds} rounds to add to the references, but do not need to use them all.\n",
        "\n",
        "DO NOT ADD A CITATION THAT ALREADY EXISTS!\"\"\"\n",
        "\n",
        "citation_first_prompt = '''Round {current_round}/{total_rounds}:\n",
        "\n",
        "You have written this LaTeX draft so far:\n",
        "\n",
        "\"\"\"\n",
        "{draft}\n",
        "\"\"\"\n",
        "\n",
        "Identify the most important citation that you still need to add, and the query to find the paper.\n",
        "\n",
        "Respond in the following format:\n",
        "\n",
        "THOUGHT:\n",
        "<THOUGHT>\n",
        "\n",
        "RESPONSE:\n",
        "```json\n",
        "<JSON>\n",
        "```\n",
        "\n",
        "In <THOUGHT>, first briefly reason over the paper and identify where citations should be added.\n",
        "If no more citations are needed, add \"No more citations needed\" to your thoughts.\n",
        "Do not add \"No more citations needed\" if you are adding citations this round.\n",
        "\n",
        "In <JSON>, respond in JSON format with the following fields:\n",
        "- \"Description\": A precise description of the required edit, along with the proposed text and location where it should be made.\n",
        "- \"Query\": The search query to find the paper (e.g. attention is all you need).\n",
        "\n",
        "Ensure the description is sufficient to make the change without further context. Someone else will make the change.\n",
        "The query will work best if you are able to recall the exact name of the paper you are looking for, or the authors.\n",
        "This JSON will be automatically parsed, so ensure the format is precise.'''\n",
        "\n",
        "citation_second_prompt = \"\"\"Search has recovered the following articles:\n",
        "\n",
        "{papers}\n",
        "\n",
        "Respond in the following format:\n",
        "\n",
        "THOUGHT:\n",
        "<THOUGHT>\n",
        "\n",
        "RESPONSE:\n",
        "```json\n",
        "<JSON>\n",
        "```\n",
        "\n",
        "In <THOUGHT>, first briefly reason over the search results and identify which citation best fits your paper and the location is to be added at.\n",
        "If none are appropriate, add \"Do not add any\" to your thoughts.\n",
        "\n",
        "In <JSON>, respond in JSON format with the following fields:\n",
        "- \"Selected\": A list of the indices of the selected papers to be cited, e.g. \"[0, 1]\". Can be \"[]\" if no papers are selected. This must be a string.\n",
        "- \"Description\": Update the previous description of the required edit if needed. Ensure that any cites precisely match the name in the bibtex!!!\n",
        "\n",
        "Do not select papers that are already in the `references.bib` file at the top of the draft, or if the same citation exists under a different name.\n",
        "This JSON will be automatically parsed, so ensure the format is precise.\"\"\"\n",
        "\n",
        "\n",
        "def get_citation_aider_prompt(\n",
        "        client, model, draft, current_round, total_rounds, engine=\"semanticscholar\"\n",
        ") -> Tuple[Optional[str], bool]:\n",
        "    msg_history = []\n",
        "    try:\n",
        "        text, msg_history = get_response_from_llm(\n",
        "            citation_first_prompt.format(\n",
        "                draft=draft, current_round=current_round, total_rounds=total_rounds\n",
        "            ),\n",
        "            client=client,\n",
        "            model=model,\n",
        "            system_message=citation_system_msg.format(total_rounds=total_rounds),\n",
        "            msg_history=msg_history,\n",
        "        )\n",
        "        if \"No more citations needed\" in text:\n",
        "            print(\"No more citations needed.\")\n",
        "            return None, True\n",
        "\n",
        "        ## PARSE OUTPUT\n",
        "        json_output = extract_json_between_markers(text)\n",
        "        assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
        "        query = json_output[\"Query\"]\n",
        "        papers = search_for_papers(query, engine=engine)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None, False\n",
        "\n",
        "    if papers is None:\n",
        "        print(\"No papers found.\")\n",
        "        return None, False\n",
        "\n",
        "    paper_strings = []\n",
        "    for i, paper in enumerate(papers):\n",
        "        paper_strings.append(\n",
        "            \"\"\"{i}: {title}. {authors}. {venue}, {year}.\\nAbstract: {abstract}\"\"\".format(\n",
        "                i=i,\n",
        "                title=paper[\"title\"],\n",
        "                authors=paper[\"authors\"],\n",
        "                venue=paper[\"venue\"],\n",
        "                year=paper[\"year\"],\n",
        "                abstract=paper[\"abstract\"],\n",
        "            )\n",
        "        )\n",
        "    papers_str = \"\\n\\n\".join(paper_strings)\n",
        "\n",
        "    try:\n",
        "        text, msg_history = get_response_from_llm(\n",
        "            citation_second_prompt.format(\n",
        "                papers=papers_str,\n",
        "                current_round=current_round,\n",
        "                total_rounds=total_rounds,\n",
        "            ),\n",
        "            client=client,\n",
        "            model=model,\n",
        "            system_message=citation_system_msg.format(total_rounds=total_rounds),\n",
        "            msg_history=msg_history,\n",
        "        )\n",
        "        if \"Do not add any\" in text:\n",
        "            print(\"Do not add any.\")\n",
        "            return None, False\n",
        "        ## PARSE OUTPUT\n",
        "        json_output = extract_json_between_markers(text)\n",
        "        assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
        "        desc = json_output[\"Description\"]\n",
        "        selected_papers = json_output[\"Selected\"]\n",
        "        selected_papers = str(selected_papers)\n",
        "\n",
        "        # convert to list\n",
        "        if selected_papers != \"[]\":\n",
        "            selected_papers = list(map(int, selected_papers.strip(\"[]\").split(\",\")))\n",
        "            assert all(\n",
        "                [0 <= i < len(papers) for i in selected_papers]\n",
        "            ), \"Invalid paper index\"\n",
        "            bibtexs = [papers[i][\"citationStyles\"][\"bibtex\"] for i in selected_papers]\n",
        "            bibtex_string = \"\\n\".join(bibtexs)\n",
        "        else:\n",
        "            return None, False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None, False\n",
        "\n",
        "    # Add citation to draft\n",
        "    aider_format = '''The following citations have just been added to the end of the `references.bib` file definition at the top of the file:\n",
        "\"\"\"\n",
        "{bibtex}\n",
        "\"\"\"\n",
        "You do not need to add them yourself.\n",
        "ABSOLUTELY DO NOT ADD IT AGAIN!!!\n",
        "\n",
        "Make the proposed change to the draft incorporating these new cites:\n",
        "{description}\n",
        "\n",
        "Use your judgment for whether these should be cited anywhere else.\n",
        "Make sure that any citation precisely matches the name in `references.bib`. Change its name to the correct name in the bibtex if needed.\n",
        "Ensure the citation is well-integrated into the text.'''\n",
        "\n",
        "    aider_prompt = (\n",
        "            aider_format.format(bibtex=bibtex_string, description=desc)\n",
        "            + \"\"\"\\n You must use \\cite or \\citet to reference papers, do not manually type out author names.\"\"\"\n",
        "    )\n",
        "    return aider_prompt, False\n",
        "\n",
        "\n",
        "# PERFORM WRITEUP\n",
        "def perform_writeup(\n",
        "        idea, folder_name, coder, cite_client, cite_model, num_cite_rounds=20, engine=\"semanticscholar\"\n",
        "):\n",
        "    # CURRENTLY ASSUMES LATEX\n",
        "    abstract_prompt = f\"\"\"We've provided the `latex/template.tex` file to the project. We will be filling it in section by section.\n",
        "\n",
        "First, please fill in the \"Title\" and \"Abstract\" sections of the writeup.\n",
        "\n",
        "Some tips are provided below:\n",
        "{per_section_tips[\"Abstract\"]}\n",
        "\n",
        "Before every paragraph, please include a brief description of what you plan to write in that paragraph in a comment.\n",
        "\n",
        "Be sure to first name the file and use *SEARCH/REPLACE* blocks to perform these edits.\n",
        "\"\"\"\n",
        "    coder_out = coder.run(abstract_prompt)\n",
        "    coder_out = coder.run(\n",
        "        refinement_prompt.format(section=\"Abstract\")\n",
        "        .replace(r\"{{\", \"{\")\n",
        "        .replace(r\"}}\", \"}\")\n",
        "    )\n",
        "    for section in [\n",
        "        \"Introduction\",\n",
        "        \"Background\",\n",
        "        \"Method\",\n",
        "        \"Experimental Setup\",\n",
        "        \"Results\",\n",
        "        \"Conclusion\",\n",
        "    ]:\n",
        "        section_prompt = f\"\"\"Please fill in the {section} of the writeup. Some tips are provided below:\n",
        "{per_section_tips[section]}\n",
        "\n",
        "Be sure to use \\cite or \\citet where relevant, referring to the works provided in the file.\n",
        "Do not cite anything that is not already in `references.bib`. Do not add any new entries to this.\n",
        "\n",
        "Keep the experimental results (figures and tables) only in the Results section, and make sure that any captions are filled in.\n",
        "In this pass, do not reference anything in later sections of the paper.\n",
        "\n",
        "Before every paragraph, please include a brief description of what you plan to write in that paragraph in a comment.\n",
        "\n",
        "Be sure to first name the file and use *SEARCH/REPLACE* blocks to perform these edits.\n",
        "\"\"\"\n",
        "        coder_out = coder.run(section_prompt)\n",
        "        coder_out = coder.run(\n",
        "            refinement_prompt.format(section=section)\n",
        "            .replace(r\"{{\", \"{\")\n",
        "            .replace(r\"}}\", \"}\")\n",
        "        )\n",
        "\n",
        "    # SKETCH THE RELATED WORK\n",
        "    section_prompt = f\"\"\"Please fill in the Related Work of the writeup. Some tips are provided below:\n",
        "\n",
        "{per_section_tips[\"Related Work\"]}\n",
        "\n",
        "For this section, very briefly sketch out the structure of the section, and clearly indicate what papers you intend to include.\n",
        "Do this all in LaTeX comments using %.\n",
        "The related work should be concise, only plan to discuss the most relevant work.\n",
        "Do not modify `references.bib` to add any new citations, this will be filled in at a later stage.\n",
        "\n",
        "Be sure to first name the file and use *SEARCH/REPLACE* blocks to perform these edits.\n",
        "\"\"\"\n",
        "    coder_out = coder.run(section_prompt)\n",
        "\n",
        "    # Fill paper with cites.\n",
        "    for _ in range(num_cite_rounds):\n",
        "        with open(osp.join(folder_name, \"latex\", \"template.tex\"), \"r\") as f:\n",
        "            draft = f.read()\n",
        "        prompt, done = get_citation_aider_prompt(\n",
        "            cite_client, cite_model, draft, _, num_cite_rounds, engine=engine\n",
        "        )\n",
        "        if done:\n",
        "            break\n",
        "        if prompt is not None:\n",
        "            # extract bibtex string\n",
        "            bibtex_string = prompt.split('\"\"\"')[1]\n",
        "            # insert this into draft before the \"\\end{filecontents}\" line\n",
        "            search_str = r\"\\end{filecontents}\"\n",
        "            draft = draft.replace(search_str, f\"{bibtex_string}{search_str}\")\n",
        "            with open(osp.join(folder_name, \"latex\", \"template.tex\"), \"w\") as f:\n",
        "                f.write(draft)\n",
        "            coder_out = coder.run(prompt)\n",
        "\n",
        "    coder_out = coder.run(\n",
        "        refinement_prompt.format(section=\"Related Work\")\n",
        "        .replace(r\"{{\", \"{\")\n",
        "        .replace(r\"}}\", \"}\")\n",
        "    )\n",
        "\n",
        "    ## SECOND REFINEMENT LOOP\n",
        "    coder.run(\n",
        "        \"\"\"Great job! Now that there is a complete draft of the entire paper, let's refine each section again.\n",
        "First, re-think the Title if necessary. Keep this concise and descriptive of the paper's concept, but try by creative with it.\"\"\"\n",
        "    )\n",
        "    for section in [\n",
        "        \"Abstract\",\n",
        "        \"Related Work\",\n",
        "        \"Introduction\",\n",
        "        \"Background\",\n",
        "        \"Method\",\n",
        "        \"Experimental Setup\",\n",
        "        \"Results\",\n",
        "        \"Conclusion\",\n",
        "    ]:\n",
        "        coder_out = coder.run(\n",
        "            second_refinement_prompt.format(\n",
        "                section=section, tips=per_section_tips[section]\n",
        "            )\n",
        "            .replace(r\"{{\", \"{\")\n",
        "            .replace(r\"}}\", \"}\")\n",
        "        )\n",
        "\n",
        "    generate_latex(coder, folder_name, f\"{folder_name}/{idea['Name']}.pdf\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from aider.coders import Coder\n",
        "    from aider.models import Model\n",
        "    from aider.io import InputOutput\n",
        "    import json\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Perform writeup for a project\")\n",
        "    parser.add_argument(\"--folder\", type=str)\n",
        "    parser.add_argument(\"--no-writing\", action=\"store_true\", help=\"Only generate\")\n",
        "    parser.add_argument(\n",
        "        \"--model\",\n",
        "        type=str,\n",
        "        default=\"gpt-4o-2024-05-13\",\n",
        "        choices=AVAILABLE_LLMS,\n",
        "        help=\"Model to use for AI Scientist.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--engine\",\n",
        "        type=str,\n",
        "        default=\"semanticscholar\",\n",
        "        choices=[\"semanticscholar\", \"openalex\"],\n",
        "        help=\"Scholar engine to use.\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    client, client_model = create_client(args.model)\n",
        "    print(\"Make sure you cleaned the Aider logs if re-generating the writeup!\")\n",
        "    folder_name = args.folder\n",
        "    idea_name = osp.basename(folder_name)\n",
        "    exp_file = osp.join(folder_name, \"experiment.py\")\n",
        "    vis_file = osp.join(folder_name, \"plot.py\")\n",
        "    notes = osp.join(folder_name, \"notes.txt\")\n",
        "    model = args.model\n",
        "    writeup_file = osp.join(folder_name, \"latex\", \"template.tex\")\n",
        "    ideas_file = osp.join(folder_name, \"ideas.json\")\n",
        "    with open(ideas_file, \"r\") as f:\n",
        "        ideas = json.load(f)\n",
        "    for idea in ideas:\n",
        "        if idea[\"Name\"] in idea_name:\n",
        "            print(f\"Found idea: {idea['Name']}\")\n",
        "            break\n",
        "    if idea[\"Name\"] not in idea_name:\n",
        "        raise ValueError(f\"Idea {idea_name} not found\")\n",
        "    fnames = [exp_file, writeup_file, notes]\n",
        "    io = InputOutput(yes=True, chat_history_file=f\"{folder_name}/{idea_name}_aider.txt\")\n",
        "    if args.model == \"deepseek-coder-v2-0724\":\n",
        "        main_model = Model(\"deepseek/deepseek-coder\")\n",
        "    elif args.model == \"llama3.1-405b\":\n",
        "        main_model = Model(\"openrouter/meta-llama/llama-3.1-405b-instruct\")\n",
        "    else:\n",
        "        main_model = Model(model)\n",
        "    coder = Coder.create(\n",
        "        main_model=main_model,\n",
        "        fnames=fnames,\n",
        "        io=io,\n",
        "        stream=False,\n",
        "        use_git=False,\n",
        "        edit_format=\"diff\",\n",
        "    )\n",
        "    if args.no_writing:\n",
        "        generate_latex(coder, args.folder, f\"{args.folder}/test.pdf\")\n",
        "    else:\n",
        "        try:\n",
        "            perform_writeup(idea, folder_name, coder, client, client_model, engine=args.engine)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to perform writeup: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "LbTE1e2LNFbV",
        "outputId": "85c15a3f-e94c-4f48-c2d9-c7837c15d8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'aider'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-17ff1b30d90e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0maider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0maider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aider'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform_review.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from pypdf import PdfReader\n",
        "import pymupdf\n",
        "import pymupdf4llm\n",
        "from ai_scientist.llm import (\n",
        "    get_response_from_llm,\n",
        "    get_batch_responses_from_llm,\n",
        "    extract_json_between_markers,\n",
        ")\n",
        "\n",
        "reviewer_system_prompt_base = (\n",
        "    \"You are an AI researcher who is reviewing a paper that was submitted to a prestigious ML venue.\"\n",
        "    \"Be critical and cautious in your decision.\"\n",
        ")\n",
        "\n",
        "reviewer_system_prompt_neg = (\n",
        "    reviewer_system_prompt_base\n",
        "    + \"If a paper is bad or you are unsure, give it bad scores and reject it.\"\n",
        ")\n",
        "reviewer_system_prompt_pos = (\n",
        "    reviewer_system_prompt_base\n",
        "    + \"If a paper is good or you are unsure, give it good scores and accept it.\"\n",
        ")\n",
        "\n",
        "template_instructions = \"\"\"\n",
        "Respond in the following format:\n",
        "\n",
        "THOUGHT:\n",
        "<THOUGHT>\n",
        "\n",
        "REVIEW JSON:\n",
        "```json\n",
        "<JSON>\n",
        "```\n",
        "\n",
        "In <THOUGHT>, first briefly discuss your intuitions and reasoning for the evaluation.\n",
        "Detail your high-level arguments, necessary choices and desired outcomes of the review.\n",
        "Do not make generic comments here, but be specific to your current paper.\n",
        "Treat this as the note-taking phase of your review.\n",
        "\n",
        "In <JSON>, provide the review in JSON format with the following fields in the order:\n",
        "- \"Summary\": A summary of the paper content and its contributions.\n",
        "- \"Strengths\": A list of strengths of the paper.\n",
        "- \"Weaknesses\": A list of weaknesses of the paper.\n",
        "- \"Originality\": A rating from 1 to 4 (low, medium, high, very high).\n",
        "- \"Quality\": A rating from 1 to 4 (low, medium, high, very high).\n",
        "- \"Clarity\": A rating from 1 to 4 (low, medium, high, very high).\n",
        "- \"Significance\": A rating from 1 to 4 (low, medium, high, very high).\n",
        "- \"Questions\": A set of clarifying questions to be answered by the paper authors.\n",
        "- \"Limitations\": A set of limitations and potential negative societal impacts of the work.\n",
        "- \"Ethical Concerns\": A boolean value indicating whether there are ethical concerns.\n",
        "- \"Soundness\": A rating from 1 to 4 (poor, fair, good, excellent).\n",
        "- \"Presentation\": A rating from 1 to 4 (poor, fair, good, excellent).\n",
        "- \"Contribution\": A rating from 1 to 4 (poor, fair, good, excellent).\n",
        "- \"Overall\": A rating from 1 to 10 (very strong reject to award quality).\n",
        "- \"Confidence\": A rating from 1 to 5 (low, medium, high, very high, absolute).\n",
        "- \"Decision\": A decision that has to be one of the following: Accept, Reject.\n",
        "\n",
        "For the \"Decision\" field, don't use Weak Accept, Borderline Accept, Borderline Reject, or Strong Reject. Instead, only use Accept or Reject.\n",
        "This JSON will be automatically parsed, so ensure the format is precise.\n",
        "\"\"\"\n",
        "\n",
        "neurips_form = (\n",
        "    \"\"\"\n",
        "## Review Form\n",
        "Below is a description of the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.\n",
        "When writing your review, please keep in mind that after decisions have been made, reviews and meta-reviews of accepted papers and opted-in rejected papers will be made public.\n",
        "\n",
        "1. Summary: Briefly summarize the paper and its contributions. This is not the place to critique the paper; the authors should generally agree with a well-written summary.\n",
        "  - Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions:\n",
        "  - Originality: Are the tasks or methods new? Is the work a novel combination of well-known techniques? (This can be valuable!) Is it clear how this work differs from previous contributions? Is related work adequately cited\n",
        "  - Quality: Is the submission technically sound? Are claims well supported (e.g., by theoretical analysis or experimental results)? Are the methods used appropriate? Is this a complete piece of work or work in progress? Are the authors careful and honest about evaluating both the strengths and weaknesses of their work\n",
        "  - Clarity: Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)\n",
        "  - Significance: Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?\n",
        "\n",
        "2. Questions: Please list up and carefully describe any questions and suggestions for the authors. Think of the things where a response from the author can change your opinion, clarify a confusion or address a limitation. This can be very important for a productive rebuttal and discussion phase with the authors.\n",
        "\n",
        "3. Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work? If not, please include constructive suggestions for improvement.\n",
        "In general, authors should be rewarded rather than punished for being up front about the limitations of their work and any potential negative societal impact. You are encouraged to think through whether any critical points are missing and provide these as feedback for the authors.\n",
        "\n",
        "4. Ethical concerns: If there are ethical issues with this paper, please flag the paper for an ethics review. For guidance on when this is appropriate, please review the NeurIPS ethics guidelines.\n",
        "\n",
        "5. Soundness: Please assign the paper a numerical rating on the following scale to indicate the soundness of the technical claims, experimental and research methodology and on whether the central claims of the paper are adequately supported with evidence.\n",
        "  4: excellent\n",
        "  3: good\n",
        "  2: fair\n",
        "  1: poor\n",
        "\n",
        "6. Presentation: Please assign the paper a numerical rating on the following scale to indicate the quality of the presentation. This should take into account the writing style and clarity, as well as contextualization relative to prior work.\n",
        "  4: excellent\n",
        "  3: good\n",
        "  2: fair\n",
        "  1: poor\n",
        "\n",
        "7. Contribution: Please assign the paper a numerical rating on the following scale to indicate the quality of the overall contribution this paper makes to the research area being studied. Are the questions being asked important? Does the paper bring a significant originality of ideas and/or execution? Are the results valuable to share with the broader NeurIPS community.\n",
        "  4: excellent\n",
        "  3: good\n",
        "  2: fair\n",
        "  1: poor\n",
        "\n",
        "8. Overall: Please provide an \"overall score\" for this submission. Choices:\n",
        "  10: Award quality: Technically flawless paper with groundbreaking impact on one or more areas of AI, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations.\n",
        "  9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI and excellent impact on multiple areas of AI, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.\n",
        "  8: Strong Accept: Technically strong paper with, with novel ideas, excellent impact on at least one area of AI or high-to-excellent impact on multiple areas of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.\n",
        "  7: Accept: Technically solid paper, with high impact on at least one sub-area of AI or moderate-to-high impact on more than one area of AI, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.\n",
        "  6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.\n",
        "  5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.\n",
        "  4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.\n",
        "  3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.\n",
        "  2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.\n",
        "  1: Very Strong Reject: For instance, a paper with trivial results or unaddressed ethical considerations\n",
        "\n",
        "9. Confidence:  Please provide a \"confidence score\" for your assessment of this submission to indicate how confident you are in your evaluation. Choices:\n",
        "  5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n",
        "  4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n",
        "  3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n",
        "  2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n",
        "  1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.\n",
        "\"\"\"\n",
        "    + template_instructions\n",
        ")\n",
        "\n",
        "\n",
        "def perform_review(\n",
        "    text,\n",
        "    model,\n",
        "    client,\n",
        "    num_reflections=1,\n",
        "    num_fs_examples=1,\n",
        "    num_reviews_ensemble=1,\n",
        "    temperature=0.75,\n",
        "    msg_history=None,\n",
        "    return_msg_history=False,\n",
        "    reviewer_system_prompt=reviewer_system_prompt_neg,\n",
        "    review_instruction_form=neurips_form,\n",
        "):\n",
        "    if num_fs_examples > 0:\n",
        "        fs_prompt = get_review_fewshot_examples(num_fs_examples)\n",
        "        base_prompt = review_instruction_form + fs_prompt\n",
        "    else:\n",
        "        base_prompt = review_instruction_form\n",
        "\n",
        "    base_prompt += f\"\"\"\n",
        "Here is the paper you are asked to review:\n",
        "```\n",
        "{text}\n",
        "```\"\"\"\n",
        "\n",
        "    if num_reviews_ensemble > 1:\n",
        "        llm_review, msg_histories = get_batch_responses_from_llm(\n",
        "            base_prompt,\n",
        "            model=model,\n",
        "            client=client,\n",
        "            system_message=reviewer_system_prompt,\n",
        "            print_debug=False,\n",
        "            msg_history=msg_history,\n",
        "            # Higher temperature to encourage diversity.\n",
        "            temperature=0.75,\n",
        "            n_responses=num_reviews_ensemble,\n",
        "        )\n",
        "        parsed_reviews = []\n",
        "        for idx, rev in enumerate(llm_review):\n",
        "            try:\n",
        "                parsed_reviews.append(extract_json_between_markers(rev))\n",
        "            except Exception as e:\n",
        "                print(f\"Ensemble review {idx} failed: {e}\")\n",
        "        parsed_reviews = [r for r in parsed_reviews if r is not None]\n",
        "        review = get_meta_review(model, client, temperature, parsed_reviews)\n",
        "\n",
        "        # take first valid in case meta-reviewer fails\n",
        "        if review is None:\n",
        "            review = parsed_reviews[0]\n",
        "\n",
        "        # Replace numerical scores with the average of the ensemble.\n",
        "        for score, limits in [\n",
        "            (\"Originality\", (1, 4)),\n",
        "            (\"Quality\", (1, 4)),\n",
        "            (\"Clarity\", (1, 4)),\n",
        "            (\"Significance\", (1, 4)),\n",
        "            (\"Soundness\", (1, 4)),\n",
        "            (\"Presentation\", (1, 4)),\n",
        "            (\"Contribution\", (1, 4)),\n",
        "            (\"Overall\", (1, 10)),\n",
        "            (\"Confidence\", (1, 5)),\n",
        "        ]:\n",
        "            scores = []\n",
        "            for r in parsed_reviews:\n",
        "                if score in r and limits[1] >= r[score] >= limits[0]:\n",
        "                    scores.append(r[score])\n",
        "            review[score] = int(round(np.mean(scores)))\n",
        "\n",
        "        # Rewrite the message history with the valid one and new aggregated review.\n",
        "        msg_history = msg_histories[0][:-1]\n",
        "        msg_history += [\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"\"\"\n",
        "THOUGHT:\n",
        "I will start by aggregating the opinions of {num_reviews_ensemble} reviewers that I previously obtained.\n",
        "\n",
        "REVIEW JSON:\n",
        "```json\n",
        "{json.dumps(review)}\n",
        "```\n",
        "\"\"\",\n",
        "            }\n",
        "        ]\n",
        "    else:\n",
        "        llm_review, msg_history = get_response_from_llm(\n",
        "            base_prompt,\n",
        "            model=model,\n",
        "            client=client,\n",
        "            system_message=reviewer_system_prompt,\n",
        "            print_debug=False,\n",
        "            msg_history=msg_history,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        review = extract_json_between_markers(llm_review)\n",
        "\n",
        "    if num_reflections > 1:\n",
        "        for j in range(num_reflections - 1):\n",
        "            # print(f\"Relection: {j + 2}/{num_reflections}\")\n",
        "            text, msg_history = get_response_from_llm(\n",
        "                reviewer_reflection_prompt,\n",
        "                client=client,\n",
        "                model=model,\n",
        "                system_message=reviewer_system_prompt,\n",
        "                msg_history=msg_history,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            review = extract_json_between_markers(text)\n",
        "            assert review is not None, \"Failed to extract JSON from LLM output\"\n",
        "\n",
        "            if \"I am done\" in text:\n",
        "                # print(f\"Review generation converged after {j + 2} iterations.\")\n",
        "                break\n",
        "\n",
        "    if return_msg_history:\n",
        "        return review, msg_history\n",
        "    else:\n",
        "        return review\n",
        "\n",
        "\n",
        "reviewer_reflection_prompt = \"\"\"Round {current_round}/{num_reflections}.\n",
        "In your thoughts, first carefully consider the accuracy and soundness of the review you just created.\n",
        "Include any other factors that you think are important in evaluating the paper.\n",
        "Ensure the review is clear and concise, and the JSON is in the correct format.\n",
        "Do not make things overly complicated.\n",
        "In the next attempt, try and refine and improve your review.\n",
        "Stick to the spirit of the original review unless there are glaring issues.\n",
        "\n",
        "Respond in the same format as before:\n",
        "THOUGHT:\n",
        "<THOUGHT>\n",
        "\n",
        "REVIEW JSON:\n",
        "```json\n",
        "<JSON>\n",
        "```\n",
        "\n",
        "If there is nothing to improve, simply repeat the previous JSON EXACTLY after the thought and include \"I am done\" at the end of the thoughts but before the JSON.\n",
        "ONLY INCLUDE \"I am done\" IF YOU ARE MAKING NO MORE CHANGES.\"\"\"\n",
        "\n",
        "\n",
        "def load_paper(pdf_path, num_pages=None, min_size=100):\n",
        "    try:\n",
        "        if num_pages is None:\n",
        "            text = pymupdf4llm.to_markdown(pdf_path)\n",
        "        else:\n",
        "            reader = PdfReader(pdf_path)\n",
        "            min_pages = min(len(reader.pages), num_pages)\n",
        "            text = pymupdf4llm.to_markdown(pdf_path, pages=list(range(min_pages)))\n",
        "        if len(text) < min_size:\n",
        "            raise Exception(\"Text too short\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error with pymupdf4llm, falling back to pymupdf: {e}\")\n",
        "        try:\n",
        "            doc = pymupdf.open(pdf_path)  # open a document\n",
        "            if num_pages:\n",
        "                doc = doc[:num_pages]\n",
        "            text = \"\"\n",
        "            for page in doc:  # iterate the document pages\n",
        "                text = text + page.get_text()  # get plain text encoded as UTF-8\n",
        "            if len(text) < min_size:\n",
        "                raise Exception(\"Text too short\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error with pymupdf, falling back to pypdf: {e}\")\n",
        "            reader = PdfReader(pdf_path)\n",
        "            if num_pages is None:\n",
        "                text = \"\".join(page.extract_text() for page in reader.pages)\n",
        "            else:\n",
        "                text = \"\".join(page.extract_text() for page in reader.pages[:num_pages])\n",
        "            if len(text) < min_size:\n",
        "                raise Exception(\"Text too short\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_review(path):\n",
        "    with open(path, \"r\") as json_file:\n",
        "        loaded = json.load(json_file)\n",
        "    return loaded[\"review\"]\n",
        "\n",
        "\n",
        "# get directory of this file\n",
        "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
        "\n",
        "fewshot_papers = [\n",
        "    os.path.join(dir_path, \"fewshot_examples/132_automated_relational.pdf\"),\n",
        "    os.path.join(dir_path, \"fewshot_examples/attention.pdf\"),\n",
        "    os.path.join(dir_path, \"fewshot_examples/2_carpe_diem.pdf\"),\n",
        "]\n",
        "\n",
        "fewshot_reviews = [\n",
        "    os.path.join(dir_path, \"fewshot_examples/132_automated_relational.json\"),\n",
        "    os.path.join(dir_path, \"fewshot_examples/attention.json\"),\n",
        "    os.path.join(dir_path, \"fewshot_examples/2_carpe_diem.json\"),\n",
        "]\n",
        "\n",
        "\n",
        "def get_review_fewshot_examples(num_fs_examples=1):\n",
        "    fewshot_prompt = \"\"\"\n",
        "Below are some sample reviews, copied from previous machine learning conferences.\n",
        "Note that while each review is formatted differently according to each reviewer's style, the reviews are well-structured and therefore easy to navigate.\n",
        "\"\"\"\n",
        "    for paper, review in zip(\n",
        "        fewshot_papers[:num_fs_examples], fewshot_reviews[:num_fs_examples]\n",
        "    ):\n",
        "        txt_path = paper.replace(\".pdf\", \".txt\")\n",
        "        if os.path.exists(txt_path):\n",
        "            with open(txt_path, \"r\") as f:\n",
        "                paper_text = f.read()\n",
        "        else:\n",
        "            paper_text = load_paper(paper)\n",
        "        review_text = load_review(review)\n",
        "        fewshot_prompt += f\"\"\"\n",
        "Paper:\n",
        "\n",
        "```\n",
        "{paper_text}\n",
        "```\n",
        "\n",
        "Review:\n",
        "\n",
        "```\n",
        "{review_text}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "    return fewshot_prompt\n",
        "\n",
        "\n",
        "meta_reviewer_system_prompt = \"\"\"You are an Area Chair at a machine learning conference.\n",
        "You are in charge of meta-reviewing a paper that was reviewed by {reviewer_count} reviewers.\n",
        "Your job is to aggregate the reviews into a single meta-review in the same format.\n",
        "Be critical and cautious in your decision, find consensus, and respect the opinion of all the reviewers.\"\"\"\n",
        "\n",
        "\n",
        "def get_meta_review(model, client, temperature, reviews):\n",
        "    # Write a meta-review from a set of individual reviews\n",
        "    review_text = \"\"\n",
        "    for i, r in enumerate(reviews):\n",
        "        review_text += f\"\"\"\n",
        "Review {i + 1}/{len(reviews)}:\n",
        "```\n",
        "{json.dumps(r)}\n",
        "```\n",
        "\"\"\"\n",
        "    base_prompt = neurips_form + review_text\n",
        "\n",
        "    llm_review, msg_history = get_response_from_llm(\n",
        "        base_prompt,\n",
        "        model=model,\n",
        "        client=client,\n",
        "        system_message=meta_reviewer_system_prompt.format(reviewer_count=len(reviews)),\n",
        "        print_debug=False,\n",
        "        msg_history=None,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    meta_review = extract_json_between_markers(llm_review)\n",
        "    return meta_review\n",
        "\n",
        "\n",
        "def perform_improvement(review, coder):\n",
        "    improvement_prompt = '''The following review has been created for your research paper:\n",
        "\"\"\"\n",
        "{review}\n",
        "\"\"\"\n",
        "\n",
        "Improve the text using the review.'''.format(\n",
        "        review=json.dumps(review)\n",
        "    )\n",
        "    coder_out = coder.run(improvement_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "CvOIyqnfKGRY",
        "outputId": "8ca783a5-24a5-4bb9-a372-050e8b716767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ffcc24210bca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;31m# get directory of this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m fewshot_papers = [\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "import openai\n",
        "from ai_scientist.perform_review import load_paper, perform_review\n",
        "\n",
        "client = openai.OpenAI()\n",
        "model = \"gpt-4o-2024-05-13\"\n",
        "\n",
        "# Load paper from PDF file (raw text)\n",
        "paper_txt = load_paper(\"report.pdf\")\n",
        "\n",
        "# Get the review dictionary\n",
        "review = perform_review(\n",
        "    paper_txt,\n",
        "    model,\n",
        "    client,\n",
        "    num_reflections=5,\n",
        "    num_fs_examples=1,\n",
        "    num_reviews_ensemble=5,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Inspect review results\n",
        "review[\"Overall\"]    # Overall score (1-10)\n",
        "review[\"Decision\"]   # 'Accept' or 'Reject'\n",
        "review[\"Weaknesses\"] # List of weaknesses (strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "eESbyO-uGdzd",
        "outputId": "d78f2542-cdea-4ed2-8695-5b97dd4c14be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b902e5d7aa88>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#from ai_scientist.perform_review import load_paper, perform_review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-4o-2024-05-13\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1JQT0p-aJ7ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Your Own Template**\n",
        "\n",
        "If there is an area of study you would like The AI Scientist to explore, it is straightforward to create your own templates. In general, follow the structure of the existing templates, which consist of:\n",
        "\n",
        "* **experiment.py** — This is the main script where the core content is. It takes an argument --out_dir, which specifies where it should create the folder and save the relevant information from the run.\n",
        "* **plot.py** — This script takes the information from the run folders and creates plots. The code should be clear and easy to edit.\n",
        "* **prompt.json** — Put information about your template here.\n",
        "* **seed_ideas.json** — Place example ideas here. You can also try to generate ideas without any examples and then pick the best one or two to put here.\n",
        "* **latex/template.tex** — We recommend using our LaTeX folder but be sure to replace the pre-loaded citations with ones that you expect to be more relevant.\n",
        "\n",
        "The key to making new templates work is matching the base filenames and output JSONs to the existing format; everything else is free to change. You should also ensure that the template.tex file is updated to use the correct citation style / base plots for your template.\n",
        "\n",
        "## **Community-Contributed Templates**\n",
        "\n",
        "We welcome community contributions in the form of new templates. While these are not maintained by us, we are delighted to highlight your templates to others. Below, we list community-contributed templates along with links to their pull requests (PRs):\n",
        "\n",
        "* Infectious Disease Modeling (seir) - PR #137\n",
        "* Image Classification with MobileNetV3 (mobilenetV3) - PR #141\n",
        "* Sketch RNN (sketch_rnn) - PR #143\n",
        "* AI in Quantum Chemistry (MACE) - PR#157\n",
        "* Earthquake Prediction (earthquake-prediction) - PR #167\n",
        "* Tensorial Radiance Fields (tensorf) - PR #175"
      ],
      "metadata": {
        "id": "Lrja3ipvGxRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Template Resources**\n",
        "\n",
        "We provide three templates, which heavily use code from other repositories, credited below:\n",
        "\n",
        "* NanoGPT Template uses code from NanoGPT and this PR.\n",
        "* 2D Diffusion Template uses code from tiny-diffusion, ema-pytorch, and Datasaur.\n",
        "* Grokking Template uses code from Sea-Snell/grokking and danielmamay/grokking.\n",
        "\n",
        "We would like to thank the developers of the open-source models and packages for their contributions and for making their work available."
      ],
      "metadata": {
        "id": "Sv39Of2rIe3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Citing The AI Scientist**\n",
        "\n",
        "If you use The AI Scientist in your research, please cite it as follows:\n",
        "\n",
        "~~~text\n",
        "@article{lu2024aiscientist,\n",
        "  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},\n",
        "  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},\n",
        "  journal={arXiv preprint arXiv:2408.06292},\n",
        "  year={2024}\n",
        "}\n",
        "~~~"
      ],
      "metadata": {
        "id": "_sbUbaBbIRF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Frequently Asked Questions**\n",
        "\n",
        "We recommend reading our paper first for any questions you have on The AI Scientist.\n",
        "\n",
        "**Why am I missing files when running The AI Scientist?**\n",
        "\n",
        "Ensure you have completed all the setup and preparation steps before the main experiment script.\n",
        "\n",
        "**Why has a PDF or a review not been generated?**\n",
        "\n",
        "The AI Scientist finishes an idea with a success rate that depends on the template, the base foundation model, and the complexity of the idea. We advise referring to our main paper. The highest success rates are observed with Claude Sonnet 3.5. Reviews are best done with GPT-4o; all other models have issues with positivity bias or failure to conform to required outputs.\n",
        "\n",
        "**What is the cost of each idea generated?**\n",
        "\n",
        "Typically less than $15 per paper with Claude Sonnet 3.5. We recommend DeepSeek Coder V2 for a much more cost-effective approach. A good place to look for new models is the Aider leaderboard.\n",
        "\n",
        "**How do I change the base conference format associated with the write-ups?**\n",
        "\n",
        "Change the base template.tex files contained within each template.\n",
        "\n",
        "**How do I run The AI Scientist for different subject fields?**\n",
        "\n",
        "Please refer to the instructions for different templates. In this current iteration, this is restricted to ideas that can be expressed in code. However, lifting this restriction would represent exciting future work! :)\n",
        "\n",
        "**How do I add support for a new foundation model?**\n",
        "\n",
        "You may modify ai_scientist/llm.py to add support for a new foundation model. We do not advise using any model that is significantly weaker than GPT-4 level for The AI Scientist.\n",
        "\n",
        "**Why do I need to run the baseline runs myself?**\n",
        "\n",
        "These appear as run_0 and should be run per machine you execute The AI Scientist on for accurate run-time comparisons due to hardware differences.\n",
        "\n",
        "**What if I have problems accessing the Semantic Scholar API?**\n",
        "\n",
        "We use the Semantic Scholar API to check ideas for novelty and collect citations for the paper write-up. You may be able to skip these phases if you don't have an API key or the API is slow to access."
      ],
      "metadata": {
        "id": "9SlzF8PiHpXp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "18orCxP4IEio"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}