{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173_Fall2025/blob/main/F25_Class_04_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Transformers and Accessing ChatGTP\n",
        "* **Part 4.2: LLM Memory and Embedding**\n",
        "* Part 4.3: Generative AI\n",
        "* Part 4.4: Text to Images with Stable Diffusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is included as the last line in the output above."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Custom Functions\n",
        "\n",
        "Run the cell below to load custom functions used in this lesson."
      ],
      "metadata": {
        "id": "dfExM5hY3fAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "37ShL3h23sK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Memory**\n",
        "\n",
        "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is Jeff\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which LangChain provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation."
      ],
      "metadata": {
        "id": "ZJrIgZkszA7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install LongChain"
      ],
      "metadata": {
        "id": "5cKto7x-RVn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_openai chromadb langchain_community sentence-transformers langchainhub pypdf"
      ],
      "metadata": {
        "id": "XWo7LO9NaNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Obtaining an OpenAI API Key**\n",
        "\n",
        "In order to delve into the practical exercises and code demonstrations within this section, students will need to obtain an **OpenAI API key**. This key grants access to OpenAI's services, including the ChatGPT functionality we'll be exploring. It's important to note that there is a nominal cost associated with the usage of this key, depending on the volume and intensity of requests made to OpenAI's servers.\n",
        "\n",
        "To obtain an OpenAI API key, access this [site](https://platform.openai.com/apps)."
      ],
      "metadata": {
        "id": "9ftIaZHNBzgw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7l6X9E9U6Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the model you will generally use for this class\n",
        "LLM_MODEL = 'gpt-3.5-turbo-1106'"
      ],
      "metadata": {
        "id": "CtmYoreIC9z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin with a very basic query to LangChain, we ask LangChain what are the 5 largest cities in the USA.\n"
      ],
      "metadata": {
        "id": "-ery1YvsDEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "\n",
        "LLM_MODEL = 'gpt-3.5-turbo-1106'\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key using ChatOpenAI\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0.7)\n",
        "\n",
        "# Define the question\n",
        "question = \"What is the largest university in San Antonio, Texas?\"\n",
        "\n",
        "# Use Langchain to call the OpenAI chat API\n",
        "response = llm.invoke(question)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "kyd7OBeJEgMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmA22qkSTyDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DQnYSgI1TxBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Buffer Window Memory\n",
        "\n",
        "The LangChain library includes a conversation object named **ConversationChain**; this object facilitates an ongoing conversation with an LLM. For any conversation object, you must also specify a memory. For this first example, we will use the **ConversationBufferWindowMemory** object. This object keeps a transcript of the most recent conversation to reference. This memory allows the conversation object to remember what you have asked or told it and its responses to you."
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "VPuxwG6U3mPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "We can now have a conversation with the LLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "conversation.predict(input=\"Hi, my name is David\")"
      ],
      "metadata": {
        "id": "9Eb-aLHn4L8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "id": "QDaSFl2aGPNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "PSn4MbiMGPeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "4McDCV6nWOXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAyqqoy9-Nog"
      },
      "source": [
        "## **Custom Conversation Bots**\n",
        "\n",
        "You can define the prompt template for a conversationbot. This technique allows you to create a bot with a name and perform a specialized task. In this case, we created a bot named \"WashU Assistant\" that we designed to help students."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original code\n",
        "# Now we can override it and set it to \"AI Assistant\"\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = \"\"\"The following is a friendly conversation between a human and an\n",
        "AI to assist UTSA Students. The AI should stick to topics\n",
        "about The University of Texas at San Antonion (UTSA). If the AI does not know the answer to a question,\n",
        "it should suggest the student speak to their advisor.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "UTSA Assistant:\"\"\"\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "conversation = ConversationChain(\n",
        "    prompt=PROMPT,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferWindowMemory(ai_prefix=\"UTSA Assistant\"),\n",
        ")"
      ],
      "metadata": {
        "id": "LNDh2WcQ0AEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now have a conversation with the UTSA assistant bot.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cjZ8AIU0ZrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Orignal code\n",
        "conversation.predict(input=\"Where is the bookstore?\")"
      ],
      "metadata": {
        "id": "5Xyc6Xpv0QcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another question.\n"
      ],
      "metadata": {
        "id": "wl2iFvZ_0hLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is a nice quiet area to study?\")\n"
      ],
      "metadata": {
        "id": "a_sEX1zN19n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often you will have multiple components in langchain that you must call in a \"chain\", to do this you can construct a chain.\n"
      ],
      "metadata": {
        "id": "TysMnwqG7sSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Which of these is closest to the bookstore?\")\n"
      ],
      "metadata": {
        "id": "ED8jTieCMLqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is the meaning of life.\")"
      ],
      "metadata": {
        "id": "hGJOZP2aXh0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "axSFWb7TXmqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "2XUNxg9zXmTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDHTQkREFRSE"
      },
      "source": [
        "## **Conversation Summary Memory**\n",
        "\n",
        "Now, let's look at using a slightly more complex type of memory, the ConversationSummaryMemory object. This type of memory creates a summary of the conversation over time. This memory can be helpful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation and stores the current summary in memory. You can use this memory to inject the conversation summary so far into a prompt/chain. This memory is most useful for more extended conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_mf3A0h-Nox"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I am a computational biologist, what do you do for a living?\")"
      ],
      "metadata": {
        "id": "Kuo4i5V7X8Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "7rEGgchzYJX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What are Embedding Layers in PyTorch**\n",
        "\n",
        "[Embedding Layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.  \n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score."
      ],
      "metadata": {
        "id": "LIE2DEMEbGpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Embedding Layer Example**\n",
        "\n",
        "* **num_embeddings** = How large is the vocabulary?  How many categories are you encoding? This parameter is the number of items in your \"lookup table.\"\n",
        "* **embedding_dim** = How many numbers in the vector you wish to return.\n",
        "\n",
        "Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to 4 number vectors. This neural network does nothing more than passing the embedding on to the output. But it does let us see what the embedding is doing. Each feature vector coming in will have two such features."
      ],
      "metadata": {
        "id": "-79rm2sEbxFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
        "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()"
      ],
      "metadata": {
        "id": "XLdnmnsSb4VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the structure of this neural network to see what is happening inside it.\n"
      ],
      "metadata": {
        "id": "BVn2azpCb_vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer)"
      ],
      "metadata": {
        "id": "XGV_yITPcDhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n",
        "\n",
        "Now, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."
      ],
      "metadata": {
        "id": "ddThHiC0cIYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor([[1, 2]], dtype=torch.long)\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)\n"
      ],
      "metadata": {
        "id": "DXjtJdC9cL-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see two length-4 vectors that PyTorch looked up for each input integer. Recall that Python arrays are zero-based. PyTorch replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, PyTorch returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table.\n"
      ],
      "metadata": {
        "id": "npApVjmHcQM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer.weight.data"
      ],
      "metadata": {
        "id": "vs2C2783cT3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values above are random parameters that PyTorch generated as starting points. Generally, we will transfer an embedding or train these random values into something useful. The following section demonstrates how to embed a hand-coded embedding."
      ],
      "metadata": {
        "id": "vkU-mkWccZ9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transferring An Embedding**\n",
        "\n",
        "Now, we see how to hard-code an embedding lookup that performs a simple one-hot encoding.  One-hot encoding would transform the input integer values of 0, 1, and 2 to the vectors $[1,0,0]$, $[0,1,0]$, and $[0,0,1]$ respectively. The following code replaced the random lookup values in the embedding layer with this one-hot coding-inspired lookup table."
      ],
      "metadata": {
        "id": "rGK8_miFcdXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the embedding lookup matrix\n",
        "embedding_lookup = torch.tensor([\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [0, 0, 1]\n",
        "], dtype=torch.float32)  # Make sure to use float32 for weight matrices\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_layer = nn.Embedding(num_embeddings=3, embedding_dim=3)\n",
        "\n",
        "# Set the weights of the embedding layer\n",
        "embedding_layer.weight.data = embedding_lookup\n"
      ],
      "metadata": {
        "id": "tQtHbJ3ocliJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the following parameters for the Embedding layer:\n",
        "    \n",
        "* input_dim=3 - There are three different integer categorical values allowed.\n",
        "* output_dim=3 - Three columns represent a categorical value with three possible values per one-hot encoding.\n",
        "* input_length=2 - The input vector has two of these categorical values.\n",
        "\n",
        "We query the neural network with two categorical values to see the lookup performed."
      ],
      "metadata": {
        "id": "pv-umuoVcuoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input tensor directly in PyTorch\n",
        "input_tensor = torch.tensor([[0, 1]], dtype=torch.long)\n",
        "\n",
        "# Forward pass to get the predictions\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "4g6yM_4mcbqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given output shows that we provided the program with two rows from the one-hot encoding table. This encoding is a correct one-hot encoding for the values 0 and 1, where there are up to 3 unique values possible.\n",
        "\n",
        "The following section demonstrates how to train this embedding lookup table."
      ],
      "metadata": {
        "id": "eBr0Axfwc1Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training an Embedding**\n",
        "\n",
        "First, we make use of the following imports."
      ],
      "metadata": {
        "id": "-TLuLm2fc7Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "00TcwDWndARg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a neural network that classifies restaurant reviews according to positive or negative. This neural network can accept strings as input, such as given here. This code also includes positive or negative labels for each review."
      ],
      "metadata": {
        "id": "MUkuDZzFdEUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 10 resturant reviews.\n",
        "reviews = [\n",
        "    'Never coming back!',\n",
        "    'Horrible service',\n",
        "    'Rude waitress',\n",
        "    'Cold food.',\n",
        "    'Horrible food!',\n",
        "    'Awesome',\n",
        "    'Awesome service!',\n",
        "    'Rocks!',\n",
        "    'poor work',\n",
        "    'Couldn\\'t have done better']\n",
        "\n",
        "# Define labels (1=negative, 0=positive)\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "u-cTFQzYdHYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the second to the last label is incorrect.  Errors such as this are not too out of the ordinary, as most training data could have some noise.\n",
        "\n",
        "We define a vocabulary size of 50 words.  Though we do not have 50 words, it is okay to use a value larger than needed.  If there are more than 50 words, the least frequently used words in the training set are automatically dropped by the embedding layer during training.  For input, we one-hot encode the strings.  We use the TensorFlow one-hot encoding method here rather than Scikit-Learn. Scikit-learn would expand these strings to the 0's and 1's as we would typically see for dummy variables.  TensorFlow translates all words to index values and replaces each word with that index."
      ],
      "metadata": {
        "id": "D4GIKme4dM_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode reviews\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n",
        "\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "metadata": {
        "id": "FgFABVXCdR5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The program one-hot encodes these reviews to word indexes; however, their lengths are different.  We pad these reviews to 4 words and truncate any words beyond the fourth word."
      ],
      "metadata": {
        "id": "uUm9aZ8NdZw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 4\n",
        "padded_reviews = pad_sequence(encoded_reviews, batch_first=True, padding_value=0).narrow(1, 0, MAX_LENGTH)\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "id": "G0N_9OaEdcue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As specified by the **padding=post** setting, each review is padded by appending zeros at the end, as specified by the **padding=post** setting.\n",
        "\n",
        "Next, we create a neural network to learn to classify these reviews."
      ],
      "metadata": {
        "id": "LPwiCeTLdhD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Embedding(VOCAB_SIZE, 8),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(8 * MAX_LENGTH, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "wEQIfVb6dkpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network accepts four integer inputs that specify the indexes of a padded movie review. The first embedding layer converts these four indexes into four length vectors 8. These vectors come from the lookup table that contains 50 (VOCAB_SIZE) rows of vectors of length 8. This encoding is evident by the 400 (8 times 50) parameters in the embedding layer. The output size from the embedding layer is 32 (4 words expressed as 8-number embedded vectors). A single output neuron is connected to the embedding layer by 33 weights (32 from the embedding layer and a single bias neuron). Because this is a single-class classification network, we use the sigmoid activation function and binary_crossentropy.\n",
        "\n",
        "The program now trains the neural network. The embedding lookup and dense 33 weights are updated to produce a better score."
      ],
      "metadata": {
        "id": "mvxTwu02dsYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(padded_reviews.long())\n",
        "    loss = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "rE-YuaaqdxjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the learned embeddings.  Think of each word's vector as a location in the 8 dimension space where words associated with positive reviews are close to other words.  Similarly, training places negative reviews close to each other.  In addition to the training setting these embeddings, the 33 weights between the embedding layer and output neuron similarly learn to transform these embeddings into an actual prediction.  You can see these embeddings here."
      ],
      "metadata": {
        "id": "28ESN9aOd2G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights = list(model[0].parameters())[0]\n",
        "print(embedding_weights.shape)\n",
        "print(embedding_weights)\n"
      ],
      "metadata": {
        "id": "7xQSupqVd5SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now evaluate this neural network's accuracy, including the embeddings and the learned dense layer.\n"
      ],
      "metadata": {
        "id": "PkGXDSKWd_FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    outputs = model(padded_reviews.long())\n",
        "    predictions = (outputs > 0.5).float().squeeze()\n",
        "    accuracy = (predictions == torch.tensor(labels)).float().mean().item()\n",
        "    loss_value = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float)).item()\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Log-loss: {loss_value}')"
      ],
      "metadata": {
        "id": "hTJZMF_qeB2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is great, but there could be overfitting. It would be good to use early stopping to not overfit for a more complex data set. However, the loss is not perfect. Even though the predicted probabilities indicated a correct prediction in every case, the program did not achieve absolute confidence in each correct answer. The lack of confidence was likely due to the small amount of noise (previously discussed) in the data set. Some words that appeared in both positive and negative reviews contributed to this lack of absolute certainty.\n"
      ],
      "metadata": {
        "id": "qVJnawADeFqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **Attention Is All You Need**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)\n",
        "\n",
        "**\"Attention Is All You Need\"** is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
        "\n",
        "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\n",
        "\n",
        "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
        "\n",
        "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
        "\n",
        "As of 2024, the paper has been cited more than 140,000 times.\n",
        "\n",
        "**Authors**\n",
        "\n",
        "The authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group's diversity:\n",
        "\n",
        "Six of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\n",
        "\n",
        "**Methods discussed & introduced**\n",
        "\n",
        "The paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\n",
        "\n",
        "The following mechanisms were introduced by the paper as part of the development of the transformer architecture.\n",
        "\n",
        "**Scaled dot-product attention & self-attention**\n",
        "\n",
        "The use of the scaled dot-product attention and self-attention mechanism instead of an RNN or LSTM (which rely on recurrence instead) allow for better performance as described in the following paragraph.\n",
        "\n",
        "Since the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors.\n",
        "\n",
        "In the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.\n",
        "\n",
        "**Multi-head attention**\n",
        "\n",
        "In the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.\n",
        "\n",
        "By doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.\n",
        "\n",
        "**Positional encoding**\n",
        "\n",
        "Since the Transformer model is not a seq2seq model and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding.\n",
        "\n",
        "** Historical context**\n",
        "\n",
        "For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\n",
        "\n",
        "A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\n",
        "\n",
        "Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer."
      ],
      "metadata": {
        "id": "1Nj3Qd3KYcFA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUW9ffO9Yl8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9 (torch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}